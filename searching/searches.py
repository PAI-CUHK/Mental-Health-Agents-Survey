import feedparser
from urllib.parse import quote
import json
import csv
import time
import pandas as pd
from springernature_api_client.openaccess import OpenAccessAPI
from datetime import datetime


def search_arxiv():
    import feedparser
    from urllib.parse import quote
    import time

    base_url = "http://export.arxiv.org/api/query?"
    
    # æ›´æ–°å…³é”®è¯æŸ¥è¯¢é€»è¾‘
    query = 'all:((agent OR chatbot) AND (ai OR llm OR "large language model") AND ("mental health" OR psychiatry OR psychology))'
    query_encoded = quote(query)
    
    all_results = []
    start = 0
    max_per_request = 2000
    max_total = 10000  # arXiv ä¸å»ºè®®è¶…è¿‡æ­¤æ•°é‡

    while start < max_total:
        url = (
            f"{base_url}search_query={query_encoded}"
            f"&start={start}&max_results={max_per_request}"
            f"&sortBy=submittedDate&sortOrder=descending"
        )
        print(f"ğŸ” Fetching arXiv records {start} to {start + max_per_request} ...")
        
        feed = feedparser.parse(url)
        entries = feed.entries
        print(f"Fetched {len(entries)} entries")

        if not entries:
            print("No more results.")
            break

        for entry in entries:
            try:
                published_year = int(entry.published[:4])
                published_date = entry.published[:10]
                if "2025-07-01" <= published_date <= "2025-12-18":
                    all_results.append({
                        "title": entry.title,
                        "authors": [author.name for author in entry.authors],
                        "published": entry.published,
                        "summary": entry.summary,
                        "link": entry.link
                    })
            except Exception as e:
                print(f"Error processing entry: {e}")
                continue

        start += max_per_request
        time.sleep(3)

    return all_results


import requests


import requests
import time

def search_medrxiv():
    # URLä¸­å·²ç»åŒ…å«äº†æ—¥æœŸè¿‡æ»¤ï¼Œè¿™æ˜¯æœ€é«˜æ•ˆçš„æ–¹å¼
    url = "https://api.biorxiv.org/details/medrxiv/2025-07-01/2025-12-18"
    
    results = []
    cursor = 0
    page_size = 100  # APIæ¯é¡µè¿”å›çš„æ¡ç›®æ•°

    # ä½¿ç”¨æ— é™å¾ªç¯ï¼Œå®Œå…¨ä¾èµ–APIçš„è¿”å›ç»“æœæ¥å†³å®šä½•æ—¶åœæ­¢
    while True:
        paged_url = f"{url}/{cursor}"
        
        try:
            response = requests.get(paged_url, timeout=10) # å¢åŠ è¶…æ—¶ä»¥é˜²ç½‘ç»œé—®é¢˜
            response.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥ (å¦‚ 404, 500), ä¼šæŠ›å‡ºå¼‚å¸¸
            data = response.json()
        except requests.exceptions.RequestException as e:
            print(f"An error occurred during request: {e}")
            break
        except ValueError: # requests.exceptions.JSONDecodeError ç»§æ‰¿è‡ª ValueError
            print(f"Failed to decode JSON from response.")
            break

        entries = data.get("collection", [])
        print(f"Fetching medRxiv records from index {cursor}... Found {len(entries)} new entries.")

        # å¦‚æœAPIè¿”å›çš„é›†åˆæ˜¯ç©ºçš„ï¼Œè¯´æ˜æ²¡æœ‰æ›´å¤šæ•°æ®äº†ï¼Œè¿™æ˜¯æ­£ç¡®çš„é€€å‡ºç‚¹
        if not entries:
            print("No more results from the API. Stopping.")
            break

        for item in entries:
            title = item.get("title", "").lower()
            abstract = item.get("abstract", "").lower()
            text_to_search = title + " " + abstract # åˆå¹¶æ ‡é¢˜å’Œæ‘˜è¦ä¸€æ¬¡æ€§æœç´¢ï¼Œæ›´é«˜æ•ˆ

            # å°†æ‰€æœ‰æ¡ä»¶ç”¨ 'and' åˆå¹¶åˆ°ä¸€ä¸ªifè¯­å¥ä¸­ï¼Œæ›´æ¸…æ™°
            # è¿™æ˜¯å®¢æˆ·ç«¯è¿‡æ»¤ï¼Œå› ä¸ºAPIä¸æ”¯æŒå…³é”®è¯æœç´¢
            is_agent_related = ("agent" in text_to_search) or ("chatbot" in text_to_search)
            is_mh_related = any(kw in text_to_search for kw in ["mental health", "psychiatry", "psychology"])
            is_ai_related = any(kw in text_to_search for kw in ["ai", "llm", "large language model"])

            if is_agent_related and is_mh_related and is_ai_related:
                results.append({
                    "title": item.get("title"),
                    "authors": item.get("authors"),
                    "published": item.get("date", ""),
                    "summary": item.get("abstract"),
                    "link": f"https://doi.org/{item.get('doi')}" if item.get('doi') else ''
                })

        # æ›´æ–°cursorä»¥è·å–ä¸‹ä¸€é¡µ
        cursor += len(entries) # ä½¿ç”¨å®é™…è¿”å›çš„æ•°é‡æ¥å¢åŠ cursorï¼Œæ¯”å›ºå®šçš„page_sizeæ›´ä¸¥è°¨

        # å‹å¥½çš„APIä½¿ç”¨ä¹ æƒ¯ï¼Œåœ¨ä¸¤æ¬¡è¯·æ±‚ä¹‹é—´ç¨ä½œç­‰å¾…
        time.sleep(0.2)

    print(f"\nSearch complete. Total filtered results found: {len(results)}")
    return results




# def search_medrxiv():
#     import requests
#     import time

#     url = "https://api.biorxiv.org/details/medrxiv/2023-01-01/2025-07-01"
#     results = []
#     cursor = 0
#     page_size = 100
#     max_cursor = 5000

#     while cursor < max_cursor:
#         paged_url = f"{url}/{cursor}"
#         response = requests.get(paged_url)
#         data = response.json()

#         entries = data.get("collection", [])
#         print(f"Fetching medRxiv records {cursor} to {cursor + page_size} ... Got: {len(entries)}")
#         if not entries:
#             print("No more results.")
#             break

#         for item in entries:
#             title = item.get("title", "").lower()
#             abstract = item.get("abstract", "").lower()
#             date = item.get("date", "")
#             # å…³é”®è¯è¿‡æ»¤é€»è¾‘ï¼ˆä¸¥æ ¼åŒ¹é…æ‰€æœ‰ä¸‰ç±»å…³é”®è¯ï¼‰
#             if "agent" in title or "agent" in abstract:
#                 if any(kw in title or kw in abstract for kw in ["mental health", "psychiatry", "psychology"]):
#                     if any(kw in title or kw in abstract for kw in ["ai", "llm", "large language model"]):
#                         if "2023-01-01" <= date <= "2025-07-01":
#                             results.append({
#                                 "title": item.get("title"),
#                                 "authors": item.get("authors"),
#                                 "published": date,
#                                 "summary": item.get("abstract"),
#                                 "link": item.get("doi_url", '')
#                             })
#         print(f"100 filtered results: {len(results)}")
#         cursor += page_size
#         time.sleep(0.2)

#     print(f"Total filtered results: {len(results)}")
#     return results
import requests
import time
from itertools import product

# ä½ çš„å…³é”®è¯ç»„
agent_group = ["agent"]
ai_group = ["ai", "llm", "large language model"]
mh_group = ["mental health", "psychiatry", "psychology"]

def search_osf_preprints_single_query(query, max_results=100):
    url = "https://api.osf.io/v2/preprints/"
    params = {
        "q": query,
        "size": min(max_results, 100)  # OSFæ¯é¡µæœ€å¤§100
    }
    response = requests.get(url, params=params, timeout=20)
    if response.status_code != 200:
        print(f"âŒ Status code: {response.status_code}, message: {response.text[:200]}")
        return []
    try:
        data = response.json()
    except Exception as e:
        print(f"âŒ JSON decode error: {e}")
        return []

    results = []
    for item in data.get("data", []):
        attributes = item.get("attributes", {})
        results.append({
            "title": attributes.get("title", ""),
            "description": attributes.get("description", ""),
            "date_created": attributes.get("date_created", "")[:10],
            "link": item["links"].get("html", ""),
            "id": item.get("id", ""),
        })
    return results

def deduplicate(results):
    seen = set()
    unique = []
    for item in results:
        key = (item["title"], item["link"])
        if key not in seen:
            seen.add(key)
            unique.append(item)
    return unique

def search_osf():
    all_results = []
    for agent_kw, ai_kw, mh_kw in product(agent_group, ai_group, mh_group):
        query = f"{agent_kw} {ai_kw} {mh_kw}"
        print(f"ğŸ” Searching: {query}")
        results = search_osf_preprints_single_query(query)
        all_results.extend(results)
        time.sleep(1)  # é˜²æ­¢è¢«é™æµ
    unique_results = deduplicate(all_results)
    print(f"âœ… Total unique OSF preprints: {len(unique_results)}")
    return unique_results


from Bio import Entrez

def search_pubmed(email="your_email@example.com"):
    from Bio import Entrez
    import time

    Entrez.email = email

    # âœ… æ–°çš„ PubMed æŸ¥è¯¢è¯­æ³•
    query = '((agent[All Fields] OR chatbot[All Fields]) AND (ai[All Fields] OR llm[All Fields] OR "large language model"[All Fields]) ' \
            'AND ("mental health"[All Fields] OR psychiatry[All Fields] OR psychology[All Fields])) ' \
            'AND ("2025/07/01"[Date - Publication] : "2025/12/18"[Date - Publication])'

    print("ğŸ” Searching PubMed...")

    # æ£€ç´¢åŒ¹é…çš„ PubMed ID
    handle = Entrez.esearch(db="pubmed", term=query, retmax=1000, sort="pub+date")
    record = Entrez.read(handle)
    ids = record.get('IdList', [])

    if not ids:
        print("â›” No results found.")
        return []

    print(f"âœ… Retrieved {len(ids)} article IDs")

    # è·å–å…·ä½“æ–‡ç« ä¿¡æ¯
    time.sleep(1)  # ç¨ä½œå»¶è¿Ÿï¼Œé˜²æ­¢è¢«é™
    handle = Entrez.efetch(db="pubmed", id=ids, rettype="medline", retmode="xml")
    records = Entrez.read(handle)

    results = []
    for article in records.get('PubmedArticle', []):
        try:
            title = article['MedlineCitation']['Article']['ArticleTitle']
            abstract = article['MedlineCitation']['Article']['Abstract']['AbstractText'][0]
            pub_date = article['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']
            year = pub_date.get('Year', '')
            month = pub_date.get('Month', '01')
            day = pub_date.get('Day', '01')
            published = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            pmid = article['MedlineCitation']['PMID']

            if "2025-07-01" <= published <= "2025-12-18":
                results.append({
                    "title": title,
                    "summary": abstract,
                    "published": published,
                    "link": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}"
                })
        except Exception as e:
            print(f"âš ï¸ Skipped an entry due to error: {e}")
            continue

    print(f"âœ… Total filtered PubMed results: {len(results)}")
    return results

def search_openalex():
    import requests
    import time

    base_url = "https://api.openalex.org/works"
    query = '(agent OR chatbot) AND (ai OR llm OR "large language model") AND ("mental health" OR psychiatry OR psychology)'
    
    per_page = 200  # æœ€å¤§æ”¯æŒå€¼
    page = 1
    headers = {
        "User-Agent": "YourAppName/0.1 (mailto:your@email.com)"
    }

    def decode_abstract(inv):
        if not inv:
            return ""
        buf = {}
        for w, pos in inv.items():
            for p in pos:
                buf[p] = w
        return " ".join(buf[i] for i in sorted(buf))
    
    results = []
    g1 = '(agent OR chatbot)'
    g2 = '(ai OR llm OR "large language model")'
    g3 = '("mental health" OR psychiatry OR psychology)'
    while True:
        params = {
            # "search": query,
            # "filter": "from_publication_date:2023-01-01,to_publication_date:2025-07-01",
            "filter": (
                f"title_and_abstract.search:{g1},"
                f"title_and_abstract.search:{g2},"
                f"title_and_abstract.search:{g3},"
                "from_publication_date:2025-07-01,"
                "to_publication_date:2025-12-18"
            ),
            "per_page": per_page,
            "page": page,
            "select": "id,display_name,abstract_inverted_index,publication_date",
            "sort": "relevance_score:desc"
        }
        print(f"ğŸ” Fetching OpenAlex records: page {page}")
        response = requests.get(base_url, params=params, headers=headers)
        if response.status_code != 200:
            print(f"âŒ Request failed with status {response.status_code}")
            break
        try:
            data = response.json()
        except Exception as e:
            print(f"âš ï¸ Failed to parse JSON: {e}")
            break
        works = data.get("results", [])
        print(f"âœ… Fetched {len(works)} items")
        for item in works:
            # title = item.get("title", "") or ""
            # abstract = item.get("abstract", "") or ""
            title = item.get("display_name", "")
            abstract_raw = item.get("abstract_inverted_index")
            abstract = decode_abstract(abstract_raw)
            results.append({
                "title": title,
                "summary": abstract,
                "published": item.get("publication_date", ""),
                "link": item.get("id", "")
            })
        if len(works) < per_page:
            print("ğŸ¯ Last page reached.")
            break
        page += 1
        time.sleep(1)  # æ§åˆ¶é€Ÿç‡ï¼Œé˜²æ­¢å°é”
    print(f"ğŸ¯ Total filtered OpenAlex results: {len(results)}")
    return results

from urllib.parse import quote
import requests
import time

def get_abstract_by_eid(eid, api_key):
    url = f"https://api.elsevier.com/content/abstract/eid/{eid}"
    headers = {
        "X-ELS-APIKey": api_key,
        "Accept": "application/json"
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        data = response.json()
        coredata = data.get('abstracts-retrieval-response', {}).get('coredata', {})
        abstract = coredata.get('dc:description', '')
        return abstract
    else:
        print(f"âš ï¸ Failed to fetch abstract for EID {eid}: status {response.status_code}")
        return ""

def search_scopus(api_key):
    headers = {
        'X-ELS-APIKey': api_key,
        'Accept': 'application/json'
    }

    # æ­£ç¡®æ‹†åˆ†æ¡ä»¶ â€”â€” TITLE-ABS-KEY ä¸­ä¸åŒ…å« PUBYEAR
    title_query = 'TITLE-ABS-KEY((agent OR chatbot) AND (ai OR llm OR "large language model") AND ("mental health" OR psychiatry OR psychology))'
    # year_filter = 'PUBYEAR > 2024 AND PUBYEAR < 2026'
    date_filter = 'PUBDATETXT > 2025-06-30 AND PUBDATETXT < 2025-12-19'
    full_query = f"{title_query} AND {date_filter}"

    count = 25
    start = 0
    results = []
    max_requests = 5000
    request_count = 0

    while request_count < max_requests:
        url = f"https://api.elsevier.com/content/search/scopus?query={quote(full_query)}&count={count}&start={start}"
        try:
            print(f"ğŸ” Fetching Scopus records {start} to {start + count} ...")
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            data = response.json()
        except requests.exceptions.RequestException as e:
            print("âŒ API Request Error:", e)
            break

        entries = data.get("search-results", {}).get("entry", [])
        if not entries:
            print("âš ï¸ No more entries found.")
            break

        for entry in entries:
            eid = entry.get("eid")
            summary = get_abstract_by_eid(eid, api_key) if eid else ""
            results.append({
                "title": entry.get("dc:title"),
                "published": entry.get("prism:coverDate", ""),
                "summary": summary,
                "link": entry.get("prism:url")
            })

        if len(entries) < count:
            break
        start += count
        request_count += 1
        time.sleep(1)  # Respectful pause for rate limits

    print(f"âœ… Total filtered Scopus results: {len(results)}")
    return results

import clarivate.wos_starter.client
from clarivate.wos_starter.client.rest import ApiException

def search_wos(api_key, max_pages=100):
    all_results = []
    db = 'WOS'
    limit = 50  # WOS API é™åˆ¶æœ€å¤§ä¸º 50
    page = 1
    sort_field = 'LD+D'  # å‘è¡¨æ—¥æœŸé™åºæ’åˆ—

    # âœ… æ›´æ–°åçš„æŸ¥è¯¢è¯­æ³•ï¼ˆTopic + å…³é”®è¯ + å¹´ä»½èŒƒå›´ï¼‰
    # q = 'TS=(agent AND (ai OR llm OR "large language model") AND (mental health OR psychiatry OR psychology)) AND PY=(2023-2025)'
    # q = 'TS=((agent OR chatbot) AND (ai OR llm OR "large language model") AND ("mental health" OR psychiatry OR psychology)) AND PY=(2023-2025)'
    q = (
        'TS=((agent OR chatbot) AND (ai OR llm OR "large language model") '
        'AND ("mental health" OR psychiatry OR psychology)) '
        'AND PY=2025'
    )
    start_date = datetime(2025, 7, 1)
    end_date = datetime(2025, 12, 18)

    
    
    configuration = clarivate.wos_starter.client.Configuration(
        host="https://api.clarivate.com/apis/wos-starter/v1"
    )
    configuration.api_key['ClarivateApiKeyAuth'] = api_key

    while page <= max_pages:
        with clarivate.wos_starter.client.ApiClient(configuration) as api_client:
            api_instance = clarivate.wos_starter.client.DocumentsApi(api_client)
            try:
                print(f"ğŸ” Fetching WOS page {page} ...")
                api_response = api_instance.documents_get(q, db=db, limit=limit, page=page, sort_field=sort_field)
                data = api_response.model_dump()  # æ¨èç”¨model_dump
                docs = data.get('hits', [])
                print(f"æ–‡çŒ®æ•°é‡: {len(docs)}")
                # for doc in docs:
                #     print(doc)  # è¿™é‡Œçš„docæ˜¯dictï¼Œå¯ä»¥ç›´æ¥è®¿é—®å­—æ®µ

                if not docs:
                    print("â›” No more results.")
                    break

                for doc in docs:
                    title = doc.get('title', '') or ''
                    
                    abstract_field = doc.get("abstract")
                    if isinstance(abstract_field, dict):
                        abstract = abstract_field.get("value", "")
                    elif isinstance(abstract_field, str):
                        abstract = abstract_field
                    else:
                        abstract = ""

                    identifiers = doc.get("identifiers", {})
                    doi = identifiers.get("doi", "")

                    # --- fallback link ---
                    link = f"https://doi.org/{doi}" if doi else ""
                    source_info = doc.get('source', {})
                    # combined_text = (title + abstract).lower()
                    raw_date = (
                        source_info.get("publish_date")  # e.g. "2025-09-15"
                        or source_info.get("cover_date")  # if they use another name
                        or ""
                    )
                    if raw_date:
                        try:
                            pub_date = datetime.strptime(raw_date, "%Y-%m-%d")
                            if not (start_date <= pub_date <= end_date):
                                continue
                        except ValueError:
                            # æ—¥æœŸæ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡è¯¥æ¡è®°å½•
                            continue

                    link = f"https://doi.org/{doi}" if doi else ""

                    all_results.append({
                        "title": title,
                        "summary": abstract,
                        "published": raw_date,
                        "doi": doi,
                        "link": link
                    })

                    # if not publication_year or not publication_year.isdigit():
                    #     continue
                    # if not (2023 <= int(publication_year) <= 2025):
                    #     continue


                    # if "agent" in combined_text and \
                    #    any(kw in combined_text for kw in ["mental health", "psychiatry", "psychology"]) and \
                    #    any(kw in combined_text for kw in ["ai", "llm", "large language model"]):

                if len(docs) < limit:
                    break
                page += 1

            except ApiException as e:
                print(f"âŒ WOS API Error: {e}")
                break

    print(f"âœ… Total filtered WOS results: {len(all_results)}")
    return all_results

def search_springer(api_key):
    """
    Springer Open Access API æŸ¥è¯¢ï¼Œå‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼š
    https://dev.springernature.com/docs/api-endpoints/open-access/
    """
    query = '((agent OR chatbot) AND (ai OR llm OR "large language model") AND ("mental health" OR psychiatry OR psychology))'
    oa_client = OpenAccessAPI(api_key=api_key)
    all_records = []
    s = 1
    p = 10
    page_count = 0

    start_date = datetime(2025, 7, 1)
    end_date   = datetime(2025, 12, 18)

    try:
        while True:
            response = oa_client.search(
                q=query,
                p=p,
                s=s,
                fetch_all=False,
                is_premium=False
            )
            # å®˜æ–¹è¿”å›ç»“æ„åº”æœ‰'records'å­—æ®µ
            records = response.get('records') if isinstance(response, dict) else None
            print(f"[DEBUG] type(records): {type(records)}")
            if records:
                print(f"[DEBUG] records sample: {json.dumps(records[:1], ensure_ascii=False, indent=2)}")
            if not records:
                break
            all_records.extend(records)
            page_count += 1
            if len(records) < p or page_count >= 100:
                break
            s += p
            time.sleep(1)  # æ¯æ¬¡è¯·æ±‚é—´éš”1ç§’ï¼Œé˜²æ­¢è¢«å°ç¦
    except Exception as e:
        print(f"Springer API è¯·æ±‚å¤±è´¥: {e}")
        return None

    if not all_records:
        print("Springer API è¯·æ±‚æ— ç»“æœ")
        return None

    results = []
    for item in all_records:
        # year = item.get("publicationDate", "")[:4]
        pub_date = item.get("publicationDate", "")
        try:
            date_obj = datetime.strptime(pub_date[:10], "%Y-%m-%d")
        except:
            continue

        if not (start_date <= date_obj <= end_date):
            continue

        # if year and 2023 <= int(year) <= 2025:
            # ä¿®æ­£æ‘˜è¦æå–é€»è¾‘
        abstract = item.get("abstract", "")
        if isinstance(abstract, dict):
            summary = abstract.get("p", "")
        elif isinstance(abstract, str):
            summary = abstract
        else:
            summary = ""
        results.append({
            "title": item.get("title"),
            "summary": summary,
            "published": item.get("publicationDate"),
            "link": item.get("url", [{}])[0].get("value", "") if item.get("url") else ""
        })
    if results:
        return pd.DataFrame(results)
    else:
        print("Springer API è¯·æ±‚æ— ç»“æœï¼Œè¿”å›å†…å®¹ï¼š", all_records)
        return None

def save_results(filename, results):
    # å¦‚æœæ˜¯DataFrameï¼Œå…ˆè½¬ä¸ºdict
    if hasattr(results, 'to_dict'):
        results = results.to_dict(orient='records')
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
        

def save_results_csv(filename, results):
    # æ”¯æŒDataFrameç±»å‹
    if hasattr(results, 'empty'):
        if results.empty:
            return
        results = results.to_dict(orient='records')
    if not results:
        return
    keys = results[0].keys()
    with open(filename, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=keys)
        writer.writeheader()
        writer.writerows(results)

if __name__ == "__main__":
    # arxiv_results = search_arxiv()
    # save_results("result_arXiv.json", arxiv_results)
    # save_results_csv("result_arXiv.csv", arxiv_results)

    # medrxiv_results = search_medrxiv()
    # save_results("result_medRxiv.json", medrxiv_results)
    # save_results_csv("result_medRxiv.csv", medrxiv_results)

    # osf_results = search_osf()
    # save_results("result_OSF.json", osf_results)
    # save_results_csv("result_OSF.csv", osf_results)

    # PubMed
    # pubmed_results = search_pubmed(email="your_email@example.com")
    # save_results("result_PubMed.json", pubmed_results)
    # save_results_csv("result_PubMed.csv", pubmed_results)

    # # # OpenAlex
    # openalex_results = search_openalex()
    # save_results("result_OpenAlex.json", openalex_results)
    # save_results_csv("result_OpenAlex.csv", openalex_results)

    # Scopus 
    # scopus_api_key = "a6a4b4a8f0ff49676823b4b795cff8aa"
    # scopus_results = search_scopus(scopus_api_key)
    # save_results("result_Scopus.json", scopus_results)
    # save_results_csv("result_Scopus.csv", scopus_results)

    # WoS (ç­‰ä¸¤å¤©)
    wos_api_key = "296c7877068fd5bba5e70c4dd8540bfbbcf37346"
    wos_results = search_wos(wos_api_key)
    save_results("result_WoS.json", wos_results)
    save_results_csv("result_WoS.csv", wos_results)

    # # Springer Nature 
    # springer_api_key = "9d77ee8d226739c0a15b871ef9333d7d"
    # springer_results = search_springer(springer_api_key)
    # save_results("result_Springer.json", springer_results)
    # save_results_csv("result_Springer.csv", springer_results)