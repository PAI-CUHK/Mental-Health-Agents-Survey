title,absract,date,link,database,Base Model,Interface Type,Workflow Composition,Architecture,Agentic Role - Retrieval,Agentic Role - Personalization,Agentic Role - Risk assessment,Agentic Role - Tool use,Agentic Role - Other,Modality,SoTD - Interactional / Conversational,SoTD - Generated / Synthetic,SoTD - Clinically Collected Data,SoTD - Elicited Evidence,Mental Health Focus,Age Group,Location,Sex,DT - Screening / Triage,DT - Clinical Decision Support,DT - Therapeutic Interventions,DT - Documentation Generation,DT - Ethical / Legal support,DT - Education & Simulation,DT - Evidence & Benchmarking,DT - Other,Automated Evaluation - Applicability,Automated Evaluation - Language,Human Evaluation - Expert,Human Evaluation - User,Unnamed: 35,Unnamed: 36,published
KMI: A Dataset of Korean Motivational Interviewing Dialogues for Psychotherapy,"The increasing demand for mental health services has led to the rise of
AI-driven mental health chatbots, though challenges related to privacy, data
collection, and expertise persist. Motivational Interviewing (MI) is gaining
attention as a theoretical basis for boosting expertise in the development of
these chatbots. However, existing datasets are showing limitations for training
chatbots, leading to a substantial demand for publicly available resources in
the field of MI and psychotherapy. These challenges are even more pronounced in
non-English languages, where they receive less attention. In this paper, we
propose a novel framework that simulates MI sessions enriched with the
expertise of professional therapists. We train an MI forecaster model that
mimics the behavioral choices of professional therapists and employ Large
Language Models (LLMs) to generate utterances through prompt engineering. Then,
we present KMI, the first synthetic dataset theoretically grounded in MI,
containing 1,000 high-quality Korean Motivational Interviewing dialogues.
Through an extensive expert evaluation of the generated dataset and the
dialogue model trained on it, we demonstrate the quality, expertise, and
practicality of KMI. We also introduce novel metrics derived from MI theory in
order to evaluate dialogues from the perspective of MI.",2/8/25,http://arxiv.org/abs/2502.05651v2,arxiv,Foundation model,Chatbot,Multi-Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,TRUE,Text,TRUE,TRUE,FALSE,FALSE,general_psychiatry,age_unspecified,South Korea,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,2/8/25
Towards Understanding Emotions for Engaged Mental Health Conversations,"Providing timely support and intervention is crucial in mental health
settings. As the need to engage youth comfortable with texting increases,
mental health providers are exploring and adopting text-based media such as
chatbots, community-based forums, online therapies with licensed professionals,
and helplines operated by trained responders. To support these text-based media
for mental health--particularly for crisis care--we are developing a system to
perform passive emotion-sensing using a combination of keystroke dynamics and
sentiment analysis. Our early studies of this system posit that the analysis of
short text messages and keyboard typing patterns can provide emotion
information that may be used to support both clients and responders. We use our
preliminary findings to discuss the way forward for applying AI to support
mental health providers in providing better care.",7/1/24,http://arxiv.org/abs/2406.11135v1,arxiv,Foundation model,Non-Chat Agent,Non-Agent,Not Applicable,FALSE,TRUE,TRUE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general_psychiatry,"adolescence, young_adulthood, middle_adulthood",singapore,Both,TRUE,TRUE,,,,,FALSE,,TRUE,,TRUE,,,,7/1/24
A Multi-Agent Dual Dialogue System to Support Mental Health Care Providers,"We introduce a general-purpose, human-in-the-loop dual dialogue system to
support mental health care professionals. The system, co-designed with care
providers, is conceptualized to assist them in interacting with care seekers
rather than functioning as a fully automated dialogue system solution. The AI
assistant within the system reduces the cognitive load of mental health care
providers by proposing responses, analyzing conversations to extract pertinent
themes, summarizing dialogues, and recommending localized relevant content and
internet-based cognitive behavioral therapy exercises. These functionalities
are achieved through a multi-agent system design, where each specialized,
supportive agent is characterized by a large language model. In evaluating the
multi-agent system, we focused specifically on the proposal of responses to
emotionally distressed care seekers. We found that the proposed responses
matched a reasonable human quality in demonstrating empathy, showing its
appropriateness for augmenting the work of mental health care providers.",11/27/24,http://arxiv.org/abs/2411.18429v2,arxiv,Foundation model,Chatbot,Multi-Agent,Not Applicable,TRUE,TRUE,TRUE,TRUE,,Text,TRUE,,,,general_psychiatry,"adolescence, young_adulthood, ",singapore,Unspecified,TRUE,TRUE,TRUE,TRUE,,FALSE,,,TRUE,,TRUE,,,,11/27/24
Large Language Model-Powered Conversational Agent Delivering Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and Therapeutic Alliance Using In-Context Learning,"Family caregivers often face substantial mental health challenges due to
their multifaceted roles and limited resources. This study explored the
potential of a large language model (LLM)-powered conversational agent to
deliver evidence-based mental health support for caregivers, specifically
Problem-Solving Therapy (PST) integrated with Motivational Interviewing (MI)
and Behavioral Chain Analysis (BCA). A within-subject experiment was conducted
with 28 caregivers interacting with four LLM configurations to evaluate empathy
and therapeutic alliance. The best-performing models incorporated Few-Shot and
Retrieval-Augmented Generation (RAG) prompting techniques, alongside
clinician-curated examples. The models showed improved contextual understanding
and personalized support, as reflected by qualitative responses and
quantitative ratings on perceived empathy and therapeutic alliances.
Participants valued the model's ability to validate emotions, explore
unexpressed feelings, and provide actionable strategies. However, balancing
thorough assessment with efficient advice delivery remains a challenge. This
work highlights the potential of LLMs in delivering empathetic and tailored
support for family caregivers.",2025-06-13T00:47:57Z,http://arxiv.org/abs/2506.11376v1,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,,,TRUE,Text,,TRUE,,,general_psychiatry,"adolescence, young_adulthood, middle_adulthood",USA,Both,,,TRUE,,,,,,TRUE,,,TRUE,,,2025-06-13T00:47:57Z
Can an LLM-Powered Socially Assistive Robot Effectively and Safely Deliver Cognitive Behavioral Therapy? A Study With University Students,"Cognitive behavioral therapy (CBT) is a widely used therapeutic method for
guiding individuals toward restructuring their thinking patterns as a means of
addressing anxiety, depression, and other challenges. We developed a large
language model (LLM)-powered prompt-engineered socially assistive robot (SAR)
that guides participants through interactive CBT at-home exercises. We
evaluated the performance of the SAR through a 15-day study with 38 university
students randomly assigned to interact daily with the robot or a chatbot (using
the same LLM), or complete traditional CBT worksheets throughout the duration
of the study. We measured weekly therapeutic outcomes, changes in
pre-/post-session anxiety measures, and adherence to completing CBT exercises.
We found that self-reported measures of general psychological distress
significantly decreased over the study period in the robot and worksheet
conditions but not the chatbot condition. Furthermore, the SAR enabled
significant single-session improvements for more sessions than the other two
conditions combined. Our findings suggest that SAR-guided LLM-powered CBT may
be as effective as traditional worksheet methods in supporting therapeutic
progress from the beginning to the end of the study and superior in decreasing
user anxiety immediately after completing the CBT exercise.",2/27/24,http://arxiv.org/abs/2402.17937v1,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,,TRUE,,,6B0,"adolescence, young_adulthood",USA,Both,FALSE,,TRUE,,,,,,TRUE,,,TRUE,,,2024-02-27T23:30:10Z
Interpersonal Theory of Suicide as a Lens to Examine Suicidal Ideation in Online Spaces,"Suicide is a critical global public health issue, with millions experiencing
suicidal ideation (SI) each year. Online spaces enable individuals to express
SI and seek peer support. While prior research has revealed the potential of
detecting SI using machine learning and natural language analysis, a key
limitation is the lack of a theoretical framework to understand the underlying
factors affecting high-risk suicidal intent. To bridge this gap, we adopted the
Interpersonal Theory of Suicide (IPTS) as an analytic lens to analyze 59,607
posts from Reddit's r/SuicideWatch, categorizing them into SI dimensions
(Loneliness, Lack of Reciprocal Love, Self Hate, and Liability) and risk
factors (Thwarted Belongingness, Perceived Burdensomeness, and Acquired
Capability of Suicide). We found that high-risk SI posts express planning and
attempts, methods and tools, and weaknesses and pain. In addition, we also
examined the language of supportive responses through psycholinguistic and
content analyses to find that individuals respond differently to different
stages of Suicidal Ideation (SI) posts. Finally, we explored the role of AI
chatbots in providing effective supportive responses to suicidal ideation
posts. We found that although AI improved structural coherence, expert
evaluations highlight persistent shortcomings in providing dynamic,
personalized, and deeply empathetic support. These findings underscore the need
for careful reflection and deeper understanding in both the development and
consideration of AI-driven interventions for effective mental health support.",2025-04-17T18:40:55Z,http://arxiv.org/abs/2504.13277v1,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,TRUE,,FALSE,Text,TRUE,,,,suicide,unspecified,unspecified,Unspecified,TRUE,,FALSE,,,TRUE,TRUE,,TRUE,TRUE,,TRUE,,,2025-04-17T18:40:55Z
"Emotion-Aware Embedding Fusion in LLMs (Flan-T5, LLAMA 2, DeepSeek-R1, and ChatGPT 4) for Intelligent Response Generatio","Empathetic and coherent responses are critical in auto-mated
chatbot-facilitated psychotherapy. This study addresses the challenge of
enhancing the emotional and contextual understanding of large language models
(LLMs) in psychiatric applications. We introduce Emotion-Aware Embedding
Fusion, a novel framework integrating hierarchical fusion and attention
mechanisms to prioritize semantic and emotional features in therapy
transcripts. Our approach combines multiple emotion lexicons, including NRC
Emotion Lexicon, VADER, WordNet, and SentiWordNet, with state-of-the-art LLMs
such as Flan-T5, LLAMA 2, DeepSeek-R1, and ChatGPT 4. Therapy session
transcripts, comprising over 2,000 samples are segmented into hierarchical
levels (word, sentence, and session) using neural networks, while hierarchical
fusion combines these features with pooling techniques to refine emotional
representations. Atten-tion mechanisms, including multi-head self-attention and
cross-attention, further prioritize emotional and contextual features, enabling
temporal modeling of emotion-al shifts across sessions. The processed
embeddings, computed using BERT, GPT-3, and RoBERTa are stored in the Facebook
AI similarity search vector database, which enables efficient similarity search
and clustering across dense vector spaces. Upon user queries, relevant segments
are retrieved and provided as context to LLMs, enhancing their ability to
generate empathetic and con-textually relevant responses. The proposed
framework is evaluated across multiple practical use cases to demonstrate
real-world applicability, including AI-driven therapy chatbots. The system can
be integrated into existing mental health platforms to generate personalized
responses based on retrieved therapy session data.",2-Oct-24,http://arxiv.org/abs/2410.01306v2,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,,,TRUE,Text,TRUE,,,,general_psychiatry,age_unspecified,unspecified,Unspecified,FALSE,FALSE,TRUE,,,TRUE,,,TRUE,TRUE,,,,,
Boosting Distress Support Dialogue Responses with Motivational Interviewing Strategy,"AI-driven chatbots have become an emerging solution to address psychological
distress. Due to the lack of psychotherapeutic data, researchers use dialogues
scraped from online peer support forums to train them. But since the responses
in such platforms are not given by professionals, they contain both conforming
and non-conforming responses. In this work, we attempt to recognize these
conforming and non-conforming response types present in online distress-support
dialogues using labels adapted from a well-established behavioral coding scheme
named Motivational Interviewing Treatment Integrity (MITI) code and show how
some response types could be rephrased into a more MI adherent form that can,
in turn, enable chatbot responses to be more compliant with the MI strategy. As
a proof of concept, we build several rephrasers by fine-tuning Blender and GPT3
to rephrase MI non-adherent ""Advise without permission"" responses into ""Advise
with permission"". We show how this can be achieved with the construction of
pseudo-parallel corpora avoiding costs for human labor. Through automatic and
human evaluation we show that in the presence of less training data, techniques
such as prompting and data augmentation can be used to produce substantially
good rephrasings that reflect the intended style and preserve the content of
the original text.",2023-05-17T13:18:28Z,http://arxiv.org/abs/2305.10195v1,arxiv,Foundation model,Non-Chat Agent,Non-Agent,Not Applicable,,,TRUE,,TRUE,Text,TRUE,,,Interview (Questionnaire),general_psychiatry,age_unspecified,unspecified,Unspecified,,,FALSE,FALSE,,,TRUE,,TRUE,,TRUE,TRUE,,,2023-05-17T13:18:28Z
LLM-empowered Chatbots for Psychiatrist and Patient Simulation: Application and Evaluation,"Empowering chatbots in the field of mental health is receiving increasing
amount of attention, while there still lacks exploration in developing and
evaluating chatbots in psychiatric outpatient scenarios. In this work, we focus
on exploring the potential of ChatGPT in powering chatbots for psychiatrist and
patient simulation. We collaborate with psychiatrists to identify objectives
and iteratively develop the dialogue system to closely align with real-world
scenarios. In the evaluation experiments, we recruit real psychiatrists and
patients to engage in diagnostic conversations with the chatbots, collecting
their ratings for assessment. Our findings demonstrate the feasibility of using
ChatGPT-powered chatbots in psychiatric scenarios and explore the impact of
prompt designs on chatbot behavior and user experience.",5/23/23,http://arxiv.org/abs/2305.13614v1,arxiv,Foundation model,Chatbot,Multi-Agent,Flat Architecture,,TRUE,,,TRUE,Text,,TRUE,,,6A7,"adolescence, young_adulthood",China,Both,TRUE,,TRUE,,,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,,,1/1/23
Deconstructing Depression Stigma: Integrating AI-driven Data Collection and Analysis with Causal Knowledge Graphs,"Mental-illness stigma is a persistent social problem, hampering both
treatment-seeking and recovery. Accordingly, there is a pressing need to
understand it more clearly, but analyzing the relevant data is highly
labor-intensive. Therefore, we designed a chatbot to engage participants in
conversations; coded those conversations qualitatively with AI assistance; and,
based on those coding results, built causal knowledge graphs to decode stigma.
The results we obtained from 1,002 participants demonstrate that conversation
with our chatbot can elicit rich information about people's attitudes toward
depression, while our AI-assisted coding was strongly consistent with
human-expert coding. Our novel approach combining large language models (LLMs)
and causal knowledge graphs uncovered patterns in individual responses and
illustrated the interrelationships of psychological constructs in the dataset
as a whole. The paper also discusses these findings' implications for HCI
researchers in developing digital interventions, decomposing human
psychological constructs, and fostering inclusive attitudes.",2025-02-09T23:58:46Z,http://arxiv.org/abs/2502.06075v1,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,,TRUE,,Interview (Questionnaire),6A7,"young_adulthood, middle_adulthood","USA, UK",Both,FALSE,,TRUE,,,TRUE,TRUE,,TRUE,TRUE,TRUE,TRUE,,,2025-02-09T23:58:46Z
VCounselor: A Psychological Intervention Chat Agent Based on a Knowledge-Enhanced Large Language Model,"Conversational artificial intelligence can already independently engage in
brief conversations with clients with psychological problems and provide
evidence-based psychological interventions. The main objective of this study is
to improve the effectiveness and credibility of the large language model in
psychological intervention by creating a specialized agent, the VCounselor, to
address the limitations observed in popular large language models such as
ChatGPT in domain applications. We achieved this goal by proposing a new
affective interaction structure and knowledge-enhancement structure. In order
to evaluate VCounselor, this study compared the general large language model,
the fine-tuned large language model, and VCounselor's knowledge-enhanced large
language model. At the same time, the general large language model and the
fine-tuned large language model will also be provided with an avatar to compare
them as an agent with VCounselor. The comparison results indicated that the
affective interaction structure and knowledge-enhancement structure of
VCounselor significantly improved the effectiveness and credibility of the
psychological intervention, and VCounselor significantly provided positive
tendencies for clients' emotions. The conclusion of this study strongly
supports that VConselor has a significant advantage in providing psychological
support to clients by being able to analyze the patient's problems with
relative accuracy and provide professional-level advice that enhances support
for clients.",3/20/24,http://arxiv.org/abs/2403.13553v1,arxiv,Small pre-trained model,Non-Chat Agent,Non-Agent,Not Applicable,FALSE,TRUE,,,TRUE,"Text, Audio, Video",,,,Interview (Questionnaire),general_psychiatry,"adolescence, young_adulthood",China,Both,TRUE,,TRUE,FALSE,,,,,TRUE,TRUE,TRUE,TRUE,,,2024-03-20T12:46:02Z
MAGI: Multi-Agent Guided Interview for Psychiatric Assessment,"Automating structured clinical interviews could revolutionize mental
healthcare accessibility, yet existing large language models (LLMs) approaches
fail to align with psychiatric diagnostic protocols. We present MAGI, the first
framework that transforms the gold-standard Mini International Neuropsychiatric
Interview (MINI) into automatic computational workflows through coordinated
multi-agent collaboration. MAGI dynamically navigates clinical logic via four
specialized agents: 1) an interview tree guided navigation agent adhering to
the MINI's branching structure, 2) an adaptive question agent blending
diagnostic probing, explaining, and empathy, 3) a judgment agent validating
whether the response from participants meet the node, and 4) a diagnosis Agent
generating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map
symptoms to clinical criteria. Experimental results on 1,002 real-world
participants covering depression, generalized anxiety, social anxiety and
suicide shows that MAGI advances LLM- assisted mental health assessment by
combining clinical rigor, conversational adaptability, and explainable
reasoning.",2025-04-25T11:08:27Z,http://arxiv.org/abs/2504.18260v1,arxiv,Foundation model,Chatbot,Multi-Agent,Team Architecture,FALSE,TRUE,FALSE,,TRUE,Text,,,,Interview (Questionnaire),"6A7, 6B0, 6B4, 6A80","adolescence, young_adulthood",China,Unspecified,TRUE,TRUE,TRUE,,,TRUE,,,TRUE,TRUE,TRUE,,,,2025-04-25T11:08:27Z
Artificial Humans,"This study investigates the development and assessment of an artificial human
designed as a conversational AI chatbot, focusing on its role as a clinical
psychologist. The project involved creating a specialized chatbot using the
Character.ai platform. The chatbot was designed to engage users in
psychological discussions, providing advice and support with a human-like
touch. The study involved participants (N=27) from diverse backgrounds,
including psychologists, AI researchers, and the general public, who interacted
with the chatbot and provided feedback on its human-likeness, empathy, and
engagement levels.
  Results indicate that while many users found the chatbot engaging and
somewhat human-like, limitations were noted in areas such as empathy and
nuanced understanding. The findings suggest that although conversational AI has
made strides, it remains far from achieving the true human-like interaction
necessary for Artificial General Intelligence (AGI). The study highlights the
challenges and potential of AI in human-computer interactions, suggesting
directions for future research and development to bridge the gap between
current capabilities and AGI.
  The project was completed in November of 2022 before the release of chatGPT.",2025-03-12T08:16:07Z,http://arxiv.org/abs/2503.16502v1,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,,FALSE,,,general_psychiatry,"adolescence, young_adulthood",unspecified,Both,FALSE,,TRUE,,,FALSE,,FALSE,,,,TRUE,,,2025-03-12T08:16:07Z
"Using Audio Data to Facilitate Depression Risk Assessment in
Primary Health Care","Telehealth is a valuable tool for primary health care (PHC), where depression
is a common condition. PHC is the first point of contact for most people with
depression, but about 25% of diagnoses made by PHC physicians are inaccurate.
Many other barriers also hinder depression detection and treatment in PHC.
Artificial intelligence (AI) may help reduce depression misdiagnosis in PHC and
improve overall diagnosis and treatment outcomes. Telehealth consultations
often have video issues, such as poor connectivity or dropped calls. Audio-only
telehealth is often more practical for lower-income patients who may lack
stable internet connections. Thus, our study focused on using audio data to
predict depression risk. The objectives were to: 1) Collect audio data from 24
people (12 with depression and 12 without mental health or major health
condition diagnoses); 2) Build a machine learning model to predict depression
risk. TPOT, an autoML tool, was used to select the best machine learning
algorithm, which was the K-nearest neighbors classifier. The selected model had
high performance in classifying depression risk (Precision: 0.98, Recall: 0.93,
F1-Score: 0.96). These findings may lead to a range of tools to help screen for
and treat depression. By developing tools to detect depression risk, patients
can be routed to AI-driven chatbots for initial screenings. Partnerships with a
range of stakeholders are crucial to implementing these solutions. Moreover,
ethical considerations, especially around data privacy and potential biases in
AI models, need to be at the forefront of any AI-driven intervention in mental
health care.",17-Oct-23,http://arxiv.org/abs/2310.10928v1,arxiv,Machine Learning Models,Non-Chat Agent,Non-Agent,Not Applicable,,,TRUE,,,Audio,,,,Interview (Questionnaire),6A7,"adolescence, young_adulthood",unspecified,Both,TRUE,,,,,,,,TRUE,,,,,,
"Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation
Tools","Objective: This study aims to develop and validate an evaluation framework to
ensure the safety and reliability of mental health chatbots, which are
increasingly popular due to their accessibility, human-like interactions, and
context-aware support. Materials and Methods: We created an evaluation
framework with 100 benchmark questions and ideal responses, and five guideline
questions for chatbot responses. This framework, validated by mental health
experts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation
methods explored included large language model (LLM)-based scoring, an agentic
approach using real-time data, and embedding models to compare chatbot
responses against ground truth standards. Results: The results highlight the
importance of guidelines and ground truth for improving LLM evaluation
accuracy. The agentic method, dynamically accessing reliable information,
demonstrated the best alignment with human assessments. Adherence to a
standardized, expert-validated framework significantly enhanced chatbot
response safety and reliability. Discussion: Our findings emphasize the need
for comprehensive, expert-tailored safety evaluation metrics for mental health
chatbots. While LLMs have significant potential, careful implementation is
necessary to mitigate risks. The superior performance of the agentic approach
underscores the importance of real-time data access in enhancing chatbot
reliability. Conclusion: The study validated an evaluation framework for mental
health chatbots, proving its effectiveness in improving safety and reliability.
Future work should extend evaluations to accuracy, bias, empathy, and privacy
to ensure holistic assessment and responsible integration into healthcare.
Standardized evaluations will build trust among users and professionals,
facilitating broader adoption and improved mental health support through
technology.",3-Aug-24,http://arxiv.org/abs/2408.04650v2,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,,TRUE,TRUE,TRUE,Text,,TRUE,,,general_psychiatry,age_unspecified,unspecified,Unspecified,,,,,,,TRUE,,TRUE,TRUE,TRUE,,,,
CA+: Cognition Augmented Counselor Agent Framework for Long-term Dynamic Client Engagement,"Current AI counseling systems struggle with maintaining effective long-term
client engagement. Through formative research with counselors and a systematic
literature review, we identified five key design considerations for AI
counseling interactions. Based on these insights, we propose CA+, a Cognition
Augmented counselor framework enhancing contextual understanding through three
components:
  (1) Therapy Strategies Module: Implements hierarchical Goals-Session-Action
planning with bidirectional adaptation based on client feedback; (2)
Communication Form Module: Orchestrates parallel guidance and empathy pathways
for balanced therapeutic progress and emotional resonance; (3) Information
Management: Utilizes client profile and therapeutic knowledge databases for
dynamic, context-aware interventions.
  A three-day longitudinal study with 24 clients demonstrates CA+'s significant
improvements in client engagement, perceived empathy, and overall satisfaction
compared to a baseline system. Besides, two licensed counselors confirm its
high professionalism. Our research demonstrates the potential for enhancing LLM
engagement in psychological counseling dialogues through cognitive theory,
which may inspire further innovations in computational interaction in the
future.",2025-03-27T10:56:53Z,http://arxiv.org/abs/2503.21365v1,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,TRUE,FALSE,Text,TRUE,FALSE,,Systematic reviews  / Meta analysis,general_psychiatry,"adolescence, young_adulthood",China,Both,TRUE,TRUE,TRUE,TRUE,,FALSE,,,TRUE,TRUE,TRUE,TRUE,,,2025-03-27T10:56:53Z
AutoCBT: An Autonomous Multi-agent Framework for Cognitive Behavioral Therapy in Psychological Counseling,"Traditional in-person psychological counseling remains primarily niche, often
chosen by individuals with psychological issues, while online automated
counseling offers a potential solution for those hesitant to seek help due to
feelings of shame. Cognitive Behavioral Therapy (CBT) is an essential and
widely used approach in psychological counseling. The advent of large language
models (LLMs) and agent technology enables automatic CBT diagnosis and
treatment. However, current LLM-based CBT systems use agents with a fixed
structure, limiting their self-optimization capabilities, or providing hollow,
unhelpful suggestions due to redundant response patterns. In this work, we
utilize Quora-like and YiXinLi single-round consultation models to build a
general agent framework that generates high-quality responses for single-turn
psychological consultation scenarios. We use a bilingual dataset to evaluate
the quality of single-response consultations generated by each framework. Then,
we incorporate dynamic routing and supervisory mechanisms inspired by real
psychological counseling to construct a CBT-oriented autonomous multi-agent
framework, demonstrating its general applicability. Experimental results
indicate that AutoCBT can provide higher-quality automated psychological
counseling services.",2025-01-16T09:57:12Z,http://arxiv.org/abs/2501.09426v1,arxiv,Foundation model,Chatbot,Multi-Agent,Hybrid Architecture,FALSE,FALSE,FALSE,FALSE,TRUE,Text,TRUE,FALSE,Not Applicable,Not Applicable,"6A7, 6B0, 6C4",Unspecified,Unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,2025-01-16T09:57:12Z
PsyDI: Towards a Personalized and Progressively In-depth Chatbot for Psychological Measurements,"In the field of psychology, traditional assessment methods, such as
standardized scales, are frequently critiqued for their static nature, lack of
personalization, and reduced participant engagement, while comprehensive
counseling evaluations are often inaccessible. The complexity of quantifying
psychological traits further limits these methods. Despite advances with large
language models (LLMs), many still depend on single-round Question-and-Answer
interactions. To bridge this gap, we introduce PsyDI, a personalized and
progressively in-depth chatbot designed for psychological measurements,
exemplified by its application in the Myers-Briggs Type Indicator (MBTI)
framework. PsyDI leverages user-related multi-modal information and engages in
customized, multi-turn interactions to provide personalized, easily accessible
measurements, while ensuring precise MBTI type determination. To address the
challenge of unquantifiable psychological traits, we introduce a novel training
paradigm that involves learning the ranking of proxy variables associated with
these traits, culminating in a robust score model for MBTI measurements. The
score model enables PsyDI to conduct comprehensive and precise measurements
through multi-turn interactions within a unified estimation context. Through
various experiments, we validate the efficacy of both the score model and the
PsyDI pipeline, demonstrating its potential to serve as a general framework for
psychological measurements. Furthermore, the online deployment of PsyDI has
garnered substantial user engagement, with over 3,000 visits, resulting in the
collection of numerous multi-turn dialogues annotated with MBTI types, which
facilitates further research. The source code for the training and web service
components is publicly available as a part of OpenDILab at:
https://github.com/opendilab/PsyDI",2024-07-22T07:19:12Z,http://arxiv.org/abs/2408.03337v4,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Interview (Questionnaire),general psychiatry,Unspecified,Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,,,2024-07-22T07:19:12Z
ChatCounselor: A Large Language Models for Mental Health Support,"This paper presents ChatCounselor, a large language model (LLM) solution
designed to provide mental health support. Unlike generic chatbots,
ChatCounselor is distinguished by its foundation in real conversations between
consulting clients and professional psychologists, enabling it to possess
specialized knowledge and counseling skills in the field of psychology. The
training dataset, Psych8k, was constructed from 260 in-depth interviews, each
spanning an hour. To assess the quality of counseling responses, the counseling
Bench was devised. Leveraging GPT-4 and meticulously crafted prompts based on
seven metrics of psychological counseling assessment, the model underwent
evaluation using a set of real-world counseling questions. Impressively,
ChatCounselor surpasses existing open-source models in the counseling Bench and
approaches the performance level of ChatGPT, showcasing the remarkable
enhancement in model capability attained through high-quality domain-specific
data.",2023-09-27T07:57:21Z,http://arxiv.org/abs/2309.15461v1,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general psychiatry,Unspecified,Unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,,,2023-09-27T07:57:21Z
Depression Diagnosis Dialogue Simulation: Self-improving Psychiatrist with Tertiary Memory,"Mental health issues, particularly depressive disorders, present significant
challenges in contemporary society, necessitating the development of effective
automated diagnostic methods. This paper introduces the Agent Mental Clinic
(AMC), a self-improving conversational agent system designed to enhance
depression diagnosis through simulated dialogues between patient and
psychiatrist agents. To enhance the dialogue quality and diagnosis accuracy, we
design a psychiatrist agent consisting of a tertiary memory structure, a
dialogue control and reflect plugin that acts as ``supervisor'' and a memory
sampling module, fully leveraging the skills reflected by the psychiatrist
agent, achieving great accuracy on depression risk and suicide risk diagnosis
via conversation. Experiment results on datasets collected in real-life
scenarios demonstrate that the system, simulating the procedure of training
psychiatrists, can be a promising optimization method for aligning LLMs with
real-life distribution in specific domains without modifying the weights of
LLMs, even when only a few representative labeled cases are available.",2024-09-20T14:25:08Z,http://arxiv.org/abs/2409.15084v2,arxiv,Foundation model,Chatbot,Multi-Agent,Hybrid Architecture,TRUE,TRUE,TRUE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,6A7,"Adolescence, Young adulthood, Middle adulthood, Old",China,Both,TRUE,TRUE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,,,2024-09-20T14:25:08Z
"PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety","Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit
profound capabilities in collective intelligence. However, the potential misuse
of this intelligence for malicious purposes presents significant risks. To
date, comprehensive research on the safety issues associated with multi-agent
systems remains limited. In this paper, we explore these concerns through the
innovative lens of agent psychology, revealing that the dark psychological
states of agents constitute a significant threat to safety. To tackle these
concerns, we propose a comprehensive framework (PsySafe) grounded in agent
psychology, focusing on three key areas: firstly, identifying how dark
personality traits in agents can lead to risky behaviors; secondly, evaluating
the safety of multi-agent systems from the psychological and behavioral
perspectives, and thirdly, devising effective strategies to mitigate these
risks. Our experiments reveal several intriguing phenomena, such as the
collective dangerous behaviors among agents, agents' self-reflection when
engaging in dangerous behavior, and the correlation between agents'
psychological assessments and dangerous behaviors. We anticipate that our
framework and observations will provide valuable insights for further research
into the safety of multi-agent systems. We will make our data and code publicly
accessible at https://github.com/AI4Good24/PsySafe.",1/22/24,http://arxiv.org/abs/2401.11880v3,arxiv,Foundation model,Chatbot,Multi-Agent,,FALSE,TRUE,TRUE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,general psychiatry,Unspecified,Unspecified,Unspecified,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,adversarial defense,TRUE,FALSE,TRUE,FALSE,,,2024-01-22T12:11:55Z
Engagement and Disclosures in LLM-Powered Cognitive Behavioral Therapy Exercises: A Factorial Design Comparing the Influence of a Robot vs. Chatbot Over Time+A26,"Many researchers are working to address the worldwide mental health crisis by
developing therapeutic technologies that increase the accessibility of care,
including leveraging large language model (LLM) capabilities in chatbots and
socially assistive robots (SARs) used for therapeutic applications. Yet, the
effects of these technologies over time remain unexplored. In this study, we
use a factorial design to assess the impact of embodiment and time spent
engaging in therapeutic exercises on participant disclosures. We assessed
transcripts gathered from a two-week study in which 26 university student
participants completed daily interactive Cognitive Behavioral Therapy (CBT)
exercises in their residences using either an LLM-powered SAR or a disembodied
chatbot. We evaluated the levels of active engagement and high intimacy of
their disclosures (opinions, judgments, and emotions) during each session and
over time. Our findings show significant interactions between time and
embodiment for both outcome measures: participant engagement and intimacy
increased over time in the physical robot condition, while both measures
decreased in the chatbot condition.",2025-06-21T21:58:30Z,http://arxiv.org/abs/2506.17831v1,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Interview (Questionnaire),general psychiatry,"Adolescence, Young adulthood",United States,Both,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,Comparison of CBT Exercise Outcomes: Embodied Robot vs. Chatbot,TRUE,FALSE,TRUE,FALSE,,,2025-06-21T21:58:30Z
TestAgent: An Adaptive and Intelligent Expert for Human Assessment,"Accurately assessing internal human states is key to understanding
preferences, offering personalized services, and identifying challenges in
real-world applications. Originating from psychometrics, adaptive testing has
become the mainstream method for human measurement and has now been widely
applied in education, healthcare, sports, and sociology. It customizes
assessments by selecting the fewest test questions . However, current adaptive
testing methods face several challenges. The mechanized nature of most
algorithms leads to guessing behavior and difficulties with open-ended
questions. Additionally, subjective assessments suffer from noisy response data
and coarse-grained test outputs, further limiting their effectiveness. To move
closer to an ideal adaptive testing process, we propose TestAgent, a large
language model (LLM)-powered agent designed to enhance adaptive testing through
interactive engagement. This is the first application of LLMs in adaptive
testing. TestAgent supports personalized question selection, captures
test-takers' responses and anomalies, and provides precise outcomes through
dynamic, conversational interactions. Experiments on psychological,
educational, and lifestyle assessments show our approach achieves more accurate
results with 20% fewer questions than state-of-the-art baselines, and testers
preferred it in speed, smoothness, and other dimensions.",2025-06-03T16:07:54Z,http://arxiv.org/abs/2506.03032v1,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,TRUE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Interview (Questionnaire),6A7,"Young adulthood, Middle adulthood, Old",Unspecified,Both,TRUE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,,,2025-06-03T16:07:54Z
MIND: Towards Immersive Psychological Healing with Multi-agent Inner Dialogue,"Human social interactions depend on the ability to infer others' unspoken
intentions, emotions, and beliefs-a cognitive skill grounded in the
psychological concept of Theory of Mind (ToM). While large language models
(LLMs) excel in semantic understanding tasks, they struggle with the ambiguity
and contextual nuance inherent in human communication. To bridge this gap, we
introduce MetaMind, a multi-agent framework inspired by psychological theories
of metacognition, designed to emulate human-like social reasoning. MetaMind
decomposes social understanding into three collaborative stages: (1) a
Theory-of-Mind Agent generates hypotheses user mental states (e.g., intent,
emotion), (2) a Domain Agent refines these hypotheses using cultural norms and
ethical constraints, and (3) a Response Agent generates contextually
appropriate responses while validating alignment with inferred intent. Our
framework achieves state-of-the-art performance across three challenging
benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain
in ToM reasoning. Notably, it enables LLMs to match human-level performance on
key ToM tasks for the first time. Ablation studies confirm the necessity of all
components, which showcase the framework's ability to balance contextual
plausibility, social appropriateness, and user adaptation. This work advances
AI systems toward human-like social intelligence, with applications in
empathetic dialogue and culturally sensitive interactions. Code is available at
https://github.com/XMZhangAI/MetaMind.",2025-02-27T08:04:27Z,http://arxiv.org/abs/2505.18943v1,arxiv,Foundation model,Chatbot,Multi-Agent,Team Architecture,FALSE,TRUE,FALSE,FALSE,FALSE,Audio,TRUE,,Not Applicable,Not Applicable,general psychiatry,Unspecified,China,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,2025-02-27T08:04:27Z
Harnessing AI Agents to Advance Research on Refugee Child Mental Health,"The international refugee crisis deepens, exposing millions of dis placed
children to extreme psychological trauma. This research suggests a com pact,
AI-based framework for processing unstructured refugee health data and
distilling knowledge on child mental health. We compare two Retrieval-Aug
mented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to
determine how well they process challenging humanitarian datasets while avoid
ing hallucination hazards. By combining cutting-edge AI methods with migration
research and child psychology, this study presents a scalable strategy to
assist policymakers, mental health practitioners, and humanitarian agencies to
better assist displaced children and recognize their mental wellbeing. In
total, both the models worked properly but significantly Deepseek R1 is
superior to Zephyr with an accuracy of answer relevance 0.91",2025-06-30T15:55:41Z,http://arxiv.org/abs/2506.23992v1,arxiv,Foundation model,Non-Chat Agent,Single Agent,Not Applicable,TRUE,FALSE,TRUE,FALSE,FALSE,Text,TRUE,FALSE,EHRs,Not Applicable,6B4,childhood,Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,2025-06-30T15:55:41Z
"Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models with Personality-Indicative Data","Role-playing agents (RPA) have been a popular application area for large
language models (LLMs), attracting significant interest from both industry and
academia.While existing RPAs well portray the characters' knowledge and tones,
they face challenges in capturing their minds, especially for small
role-playing language models (RPLMs). In this paper, we propose to enhance
RPLMs via personality-indicative data. Specifically, we leverage questions from
psychological scales and distill advanced RPAs to generate dialogues that grasp
the minds of characters. Experimental results validate that RPLMs trained with
our dataset exhibit advanced role-playing capabilities for both general and
personality-related evaluations. Code and data are available at
\href{https://github.com/alienet1109/RolePersonality}{this URL}.",2024-06-27T06:24:00Z,http://arxiv.org/abs/2406.18921v3,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,TRUE,Text,FALSE,TRUE,Not Applicable,Interview (Questionnaire),general psychiatry,Unspecified,Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,2024-06-27T06:24:00Z
Supporting Construction Worker Well-Being with a Multi-Agent Conversational AI System,"The construction industry is characterized by both high physical and
psychological risks, yet supports of mental health remain limited. While
advancements in artificial intelligence (AI), particularly large language
models (LLMs), offer promising solutions, their potential in construction
remains largely underexplored. To bridge this gap, we developed a
conversational multi-agent system that addresses industry-specific challenges
through an AI-driven approach integrated with domain knowledge. In parallel, it
fulfills construction workers' basic psychological needs by enabling
interactions with multiple agents, each has a distinct persona. This approach
ensures that workers receive both practical problem-solving support and social
engagement, ultimately contributing to their overall well-being. We evaluate
its usability and effectiveness through a within-subjects user study with 12
participants. The results show that our system significantly outperforms the
single-agent baseline, achieving improvements of 18% in usability, 40% in
self-determination, 60% in social presence, and 60% in trust. These findings
highlight the promise of LLM-driven AI systems in providing domain-specific
support for construction workers.",2025-06-09T17:58:35Z,http://arxiv.org/abs/2506.07997v1,arxiv,Foundation model,Chatbot,Multi-Agent,Hierarchical Architecture,TRUE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Not Applicable,general psychiatry,Unspecified,Unspecified,Both,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,,,2025-06-09T17:58:35Z
Private Yet Social: How LLM Chatbots Support and Challenge Eating Disorder Recovery,"Eating disorders (ED) are complex mental health conditions that require
long-term management and support. Recent advancements in large language model
(LLM)-based chatbots offer the potential to assist individuals in receiving
immediate support. Yet, concerns remain about their reliability and safety in
sensitive contexts such as ED. We explore the opportunities and potential harms
of using LLM-based chatbots for ED recovery. We observe the interactions
between 26 participants with ED and an LLM-based chatbot, WellnessBot, designed
to support ED recovery, over 10 days. We discovered that our participants have
felt empowered in recovery by discussing ED-related stories with the chatbot,
which served as a personal yet social avenue. However, we also identified
harmful chatbot responses, especially concerning individuals with ED, that went
unnoticed partly due to participants' unquestioning trust in the chatbot's
reliability. Based on these findings, we provide design implications for safe
and effective LLM-based interventions in ED management.",2024-12-16T10:59:49Z,http://arxiv.org/abs/2412.11656v1,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Interview (Questionnaire),6B8,"Adolescence, Young adulthood",South Korea,Female only,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,,,2024-12-16T10:59:49Z
Can AI Relate: Testing Large Language Model Response for Mental Health Support,"Large language models (LLMs) are already being piloted for clinical use in
hospital systems like NYU Langone, Dana-Farber and the NHS. A proposed
deployment use case is psychotherapy, where a LLM-powered chatbot can treat a
patient undergoing a mental health crisis. Deployment of LLMs for mental health
response could hypothetically broaden access to psychotherapy and provide new
possibilities for personalizing care. However, recent high-profile failures,
like damaging dieting advice offered by the Tessa chatbot to patients with
eating disorders, have led to doubt about their reliability in high-stakes and
safety-critical settings.
  In this work, we develop an evaluation framework for determining whether LLM
response is a viable and ethical path forward for the automation of mental
health treatment. Our framework measures equity in empathy and adherence of LLM
responses to motivational interviewing theory. Using human evaluation with
trained clinicians and automatic quality-of-care metrics grounded in psychology
research, we compare the responses provided by peer-to-peer responders to those
provided by a state-of-the-art LLM.
  We show that LLMs like GPT-4 use implicit and explicit cues to infer patient
demographics like race. We then show that there are statistically significant
discrepancies between patient subgroups: Responses to Black posters
consistently have lower empathy than for any other demographic group (2%-13%
lower than the control group). Promisingly, we do find that the manner in which
responses are generated significantly impacts the quality of the response. We
conclude by proposing safety guidelines for the potential deployment of LLMs
for mental health response.",2024-05-20T13:42:27Z,http://arxiv.org/abs/2405.12021v2,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,FALSE,TRUE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general psychiatry,Unspecified,Unspecified,Both,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,2024-05-20T13:42:27Z
SocialSim: Towards Socialized Simulation of Emotional Support Conversation,"Emotional support conversation (ESC) helps reduce people's psychological
stress and provide emotional value through interactive dialogues. Due to the
high cost of crowdsourcing a large ESC corpus, recent attempts use large
language models for dialogue augmentation. However, existing approaches largely
overlook the social dynamics inherent in ESC, leading to less effective
simulations. In this paper, we introduce SocialSim, a novel framework that
simulates ESC by integrating key aspects of social interactions: social
disclosure and social awareness. On the seeker side, we facilitate social
disclosure by constructing a comprehensive persona bank that captures diverse
and authentic help-seeking scenarios. On the supporter side, we enhance social
awareness by eliciting cognitive reasoning to generate logical and supportive
responses. Building upon SocialSim, we construct SSConv, a large-scale
synthetic ESC corpus of which quality can even surpass crowdsourced ESC data.
We further train a chatbot on SSConv and demonstrate its state-of-the-art
performance in both automatic and human evaluations. We believe SocialSim
offers a scalable way to synthesize ESC, making emotional care more accessible
and practical.",2025-06-20T05:24:40Z,http://arxiv.org/abs/2506.16756v1,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,general psychiatry,"Adolescence, Young adulthood, Middle adulthood",China,Both,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,,2025-06-20T05:24:40Z
ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents,"Large language models (LLMs) increasingly power mental-health chatbots, yet
the field still lacks a scalable, theory-grounded way to decide which model is
most effective to deploy. We present ESC-Judge, the first end-to-end evaluation
framework that (i) grounds head-to-head comparisons of emotional-support LLMs
in Clara Hill's established Exploration-Insight-Action counseling model,
providing a structured and interpretable view of performance, and (ii) fully
automates the evaluation pipeline at scale. ESC-Judge operates in three stages:
first, it synthesizes realistic help-seeker roles by sampling empirically
salient attributes such as stressors, personality, and life history; second, it
has two candidate support agents conduct separate sessions with the same role,
isolating model-specific strategies; and third, it asks a specialized judge LLM
to express pairwise preferences across rubric-anchored skills that span the
Exploration, Insight, and Action spectrum. In our study, ESC-Judge matched
PhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and
86 percent of Action decisions, demonstrating human-level reliability at a
fraction of the cost. All code, prompts, synthetic roles, transcripts, and
judgment scripts are released to promote transparent progress in emotionally
supportive AI.",2025-05-18T20:04:59Z,http://arxiv.org/abs/2505.12531v1,arxiv,Foundation model,Chatbot,Multi-Agent,Team Architecture,FALSE,TRUE,TRUE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Not Applicable,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,2025-05-18T20:04:59Z
MoodAngels: A Retrieval-augmented Multi-agent Framework for Psychiatry Diagnosis,"The application of AI in psychiatric diagnosis faces significant challenges,
including the subjective nature of mental health assessments, symptom overlap
across disorders, and privacy constraints limiting data availability. To
address these issues, we present MoodAngels, the first specialized multi-agent
framework for mood disorder diagnosis. Our approach combines granular-scale
analysis of clinical assessments with a structured verification process,
enabling more accurate interpretation of complex psychiatric data.
Complementing this framework, we introduce MoodSyn, an open-source dataset of
1,173 synthetic psychiatric cases that preserves clinical validity while
ensuring patient privacy. Experimental results demonstrate that MoodAngels
outperforms conventional methods, with our baseline agent achieving 12.3%
higher accuracy than GPT-4o on real-world cases, and our full multi-agent
system delivering further improvements. Evaluation in the MoodSyn dataset
demonstrates exceptional fidelity, accurately reproducing both the core
statistical patterns and complex relationships present in the original data
while maintaining strong utility for machine learning applications. Together,
these contributions provide both an advanced diagnostic tool and a critical
research resource for computational psychiatry, bridging important gaps in
AI-assisted mental health assessment.",4-Jun-25,http://arxiv.org/abs/2506.03750v1,arxiv,Foundation model,Chatbot,Multi-Agent,Team Architecture,TRUE,TRUE,TRUE,FALSE,FALSE,Text,FALSE,TRUE,EHRs,Interview (Questionnaire),"6A6, 6A7, 6A80",Unspecified,China,Both,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,
Appraisal-Guided Proximal Policy Optimization: Modeling Psychological Disorders in Dynamic Grid World,"The integration of artificial intelligence across multiple domains has
emphasized the importance of replicating human-like cognitive processes in AI.
By incorporating emotional intelligence into AI agents, their emotional
stability can be evaluated to enhance their resilience and dependability in
critical decision-making tasks. In this work, we develop a methodology for
modeling psychological disorders using Reinforcement Learning (RL) agents. We
utilized Appraisal theory to train RL agents in a dynamic grid world
environment with an Appraisal-Guided Proximal Policy Optimization (AG-PPO)
algorithm. Additionally, we investigated numerous reward-shaping strategies to
simulate psychological disorders and regulate the behavior of the agents. A
comparison of various configurations of the modified PPO algorithm identified
variants that simulate Anxiety disorder and Obsessive-Compulsive Disorder
(OCD)-like behavior in agents. Furthermore, we compared standard PPO with
AG-PPO and its configurations, highlighting the performance improvement in
terms of generalization capabilities. Finally, we conducted an analysis of the
agents' behavioral patterns in complex test environments to evaluate the
associated symptoms corresponding to the psychological disorders. Overall, our
work showcases the benefits of the appraisal-guided PPO algorithm over the
standard PPO algorithm and the potential to simulate psychological disorders in
a controlled artificial environment and evaluate them on RL agents.",2024-07-29T19:19:54Z,http://arxiv.org/abs/2407.20383v1,arxiv,Deep Learning Models,Non-Chat Agent,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,TRUE,Text,TRUE,TRUE,Not Applicable,Not Applicable,"6B0, 6B2",Unspecified,Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,2024-07-29T19:19:54Z
"MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders
  Synthesized via Neuro-Symbolic LLM Agents","The clinical diagnosis of most mental disorders primarily relies on the
conversations between psychiatrist and patient. The creation of such diagnostic
conversation datasets is promising to boost the AI mental healthcare community.
However, directly collecting the conversations in real diagnosis scenarios is
near impossible due to stringent privacy and ethical considerations. To address
this issue, we seek to synthesize diagnostic conversation by exploiting
anonymized patient cases that are easier to access. Specifically, we design a
neuro-symbolic multi-agent framework for synthesizing the diagnostic
conversation of mental disorders with large language models. It takes patient
case as input and is capable of generating multiple diverse conversations with
one single patient case. The framework basically involves the interaction
between a doctor agent and a patient agent, and generates conversations under
symbolic control via a dynamic diagnosis tree. By applying the proposed
framework, we develop the largest Chinese mental disorders diagnosis dataset
MDD-5k. This dataset is built upon 1000 real, anonymized patient cases by
cooperating with Shanghai Mental Health Center and comprises 5000 high-quality
long conversations with diagnosis results and treatment opinions as labels. To
the best of our knowledge, it's also the first labeled dataset for Chinese
mental disorders diagnosis. Human evaluation demonstrates the proposed MDD-5k
dataset successfully simulates human-like diagnostic process of mental
disorders.",2024-08-22T05:59:47Z,http://arxiv.org/abs/2408.12142v2,arxiv,Foundation model,Chatbot,Multi-Agent,Hybrid Architecture,TRUE,TRUE,FALSE,TRUE,TRUE,Text,FALSE,TRUE,EHRs,Not Applicable,general_psychiatry,"adolescence, young_adulthood",China,Both,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,,,2024-08-22T05:59:47Z
"DeepPsy-Agent: A Stage-Aware and Deep-Thinking Emotional Support Agent
  System","This paper introduces DeepPsy-Agent, an innovative psychological support
system that combines the three-stage helping theory in psychology with deep
learning techniques. The system consists of two core components: (1) a
multi-stage response-capable dialogue model (\textit{deeppsy-chat}), which
enhances reasoning capabilities through stage-awareness and deep-thinking
analysis to generate high-quality responses; and (2) a real-time stage
transition detection model that identifies contextual shifts to guide the
dialogue towards more effective intervention stages. Based on 30,000 real
psychological hotline conversations, we employ AI-simulated dialogues and
expert re-annotation strategies to construct a high-quality multi-turn dialogue
dataset. Experimental results demonstrate that DeepPsy-Agent outperforms
general-purpose large language models (LLMs) in key metrics such as problem
exposure completeness, cognitive restructuring success rate, and action
adoption rate. Ablation studies further validate the effectiveness of
stage-awareness and deep-thinking modules, showing that stage information
contributes 42.3\% to performance, while the deep-thinking module increases
root-cause identification by 58.3\% and reduces ineffective suggestions by
72.1\%. This system addresses critical challenges in AI-based psychological
support through dynamic dialogue management and deep reasoning, advancing
intelligent mental health services.",2025-03-20T05:59:29Z,http://arxiv.org/abs/2503.15876v1,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,TRUE,Text,FALSE,TRUE,Not Applicable,Not Applicable,general_psychiatry,age_unspecified,Unspecified,Unspecified,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,,2025-03-20T05:59:29Z
"Development and Evaluation of Three Chatbots for Postpartum Mood and
  Anxiety Disorders","In collaboration with Postpartum Support International (PSI), a non-profit
organization dedicated to supporting caregivers with postpartum mood and
anxiety disorders, we developed three chatbots to provide context-specific
empathetic support to postpartum caregivers, leveraging both rule-based and
generative models. We present and evaluate the performance of our chatbots
using both machine-based metrics and human-based questionnaires. Overall, our
rule-based model achieves the best performance, with outputs that are close to
ground truth reference and contain the highest levels of empathy. Human users
prefer the rule-based chatbot over the generative chatbot for its
context-specific and human-like replies. Our generative chatbot also produced
empathetic responses and was described by human users as engaging. However,
limitations in the training dataset often result in confusing or nonsensical
responses. We conclude by discussing practical benefits of rule-based vs.
generative models for supporting individuals with mental health challenges. In
light of the recent surge of ChatGPT and BARD, we also discuss the
possibilities and pitfalls of large language models for digital mental
healthcare.",8/14/23,http://arxiv.org/abs/2308.07407v1,arxiv,Small pre-trained model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,TRUE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,TRUE,"6A7, 6B0",age_unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,,1/1/23
"Sibyl: Empowering Empathetic Dialogue Generation in Large Language
  Models via Sensible and Visionary Commonsense Inference","Recently, there has been a heightened interest in building chatbots based on
Large Language Models (LLMs) to emulate human-like qualities in multi-turn
conversations. Despite having access to commonsense knowledge to better
understand the psychological aspects and causality of dialogue context, even
these powerful LLMs struggle to achieve the goals of empathy and emotional
support. Current commonsense knowledge derived from dialogue contexts is
inherently limited and often fails to adequately anticipate the future course
of a dialogue. This lack of foresight can mislead LLMs and hinder their ability
to provide effective support. In response to this challenge, we present an
innovative framework named Sensible and Visionary Commonsense Knowledge
(Sibyl). Designed to concentrate on the immediately succeeding dialogue, this
paradigm equips LLMs with the capability to uncover the implicit requirements
of the conversation, aiming to elicit more empathetic responses. Experimental
results demonstrate that incorporating our paradigm for acquiring commonsense
knowledge into LLMs comprehensively enhances the quality of their responses.",2023-11-26T14:35:23Z,http://arxiv.org/abs/2311.15316v5,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,TRUE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general_psychiatry,age_unspecified,unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,,,2023-11-26T14:35:23Z
Building Emotional Support Chatbots in the Era of LLMs,"The integration of emotional support into various conversational scenarios
presents profound societal benefits, such as social interactions, mental health
counseling, and customer service. However, there are unsolved challenges that
hinder real-world applications in this field, including limited data
availability and the absence of well-accepted model training paradigms. This
work endeavors to navigate these challenges by harnessing the capabilities of
Large Language Models (LLMs). We introduce an innovative methodology that
synthesizes human insights with the computational prowess of LLMs to curate an
extensive emotional support dialogue dataset. Our approach is initiated with a
meticulously designed set of dialogues spanning diverse scenarios as generative
seeds. By utilizing the in-context learning potential of ChatGPT, we
recursively generate an ExTensible Emotional Support dialogue dataset, named
ExTES. Following this, we deploy advanced tuning techniques on the LLaMA model,
examining the impact of diverse training strategies, ultimately yielding an LLM
meticulously optimized for emotional support interactions. An exhaustive
assessment of the resultant model showcases its proficiency in offering
emotional support, marking a pivotal step in the realm of emotional support
bots and paving the way for subsequent research and implementations.",8/17/23,http://arxiv.org/abs/2308.11584v1,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,general_psychiatry,age_unspecified,unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,,,1/1/23
PsyLite Technical Report,"With the rapid development of digital technology, AI-driven psychological
counseling has gradually become an important research direction in the field of
mental health. However, existing models still have deficiencies in dialogue
safety, detailed scenario handling, and lightweight deployment. To address
these issues, this study proposes PsyLite, a lightweight psychological
counseling large language model agent developed based on the base model
InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation
data fine-tuning and ORPO preference optimization), PsyLite enhances the
model's deep-reasoning ability, psychological counseling ability, and safe
dialogue ability. After deployment using Ollama and Open WebUI, a custom
workflow is created with Pipelines. An innovative conditional RAG is designed
to introduce crosstalk humor elements at appropriate times during psychological
counseling to enhance user experience and decline dangerous requests to
strengthen dialogue safety. Evaluations show that PsyLite outperforms the
baseline models in the Chinese general evaluation (CEval), psychological
counseling professional evaluation (CPsyCounE), and dialogue safety evaluation
(SafeDialBench), particularly in psychological counseling professionalism
(CPsyCounE score improvement of 47.6\%) and dialogue safety (\safe{} score
improvement of 2.4\%). Additionally, the model uses quantization technology
(GGUF q4\_k\_m) to achieve low hardware deployment (5GB memory is sufficient
for operation), providing a feasible solution for psychological counseling
applications in resource-constrained environments.",2025-06-26T17:54:42Z,http://arxiv.org/abs/2506.21536v1,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,TRUE,TRUE,FALSE,Text,TRUE,FALSE,EHRs,Not Applicable,general_psychiatry,age_unspecified,unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,,,2025-06-26T17:54:42Z
"CAMI: A Counselor Agent Supporting Motivational Interviewing through
  State Inference and Topic Exploration","Conversational counselor agents have become essential tools for addressing
the rising demand for scalable and accessible mental health support. This paper
introduces CAMI, a novel automated counselor agent grounded in Motivational
Interviewing (MI) -- a client-centered counseling approach designed to address
ambivalence and facilitate behavior change. CAMI employs a novel STAR
framework, consisting of client's state inference, motivation topic
exploration, and response generation modules, leveraging large language models
(LLMs). These components work together to evoke change talk, aligning with MI
principles and improving counseling outcomes for clients from diverse
backgrounds. We evaluate CAMI's performance through both automated and manual
evaluations, utilizing simulated clients to assess MI skill competency,
client's state inference accuracy, topic exploration proficiency, and overall
counseling success. Results show that CAMI not only outperforms several
state-of-the-art methods but also shows more realistic counselor-like behavior.
Additionally, our ablation study underscores the critical roles of state
inference and topic exploration in achieving this performance.",2/4/25,http://arxiv.org/abs/2502.02807v1,arxiv,Foundation model,Chatbot,Multi-Agent,Flat Architecture,TRUE,TRUE,FALSE,FALSE,TRUE,Text,TRUE,FALSE,Not Applicable,Interview (Questionnaire),general_psychiatry,age_unspecified,unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,,,2/4/25
"""I Like Sunnie More Than I Expected!"": Exploring User Expectation and Perception of an Anthropomorphic LLM-based Conversational Agent for Well-Being Support and Perception of an Anthropomorphic LLM-based Conversational Agent for Well-Being Support","The human-computer interaction (HCI) research community has a longstanding interest in exploring the mismatch between users' actual experiences and expectation toward new technologies, for instance, large language models (LLMs). In this study, we compared users' (N = 38) initial expectations against their post-interaction perceptions of two LLM-powered mental well-being intervention activity recommendation systems. Both systems have a built-in LLM to recommend a personalized well-being intervention activity, but one system (Sunnie) has an anthropomorphic conversational interaction design via elements such as appearance, persona, and natural conversation. Results showed that user engagement was high with both systems, and both systems exceeded users' expectations along the utility dimension, highlighting AI's potential to offer useful intervention activity recommendations. In addition, Sunnie further outperformed the non-anthropomorphic baseline system in relational warmth. These findings suggest that anthropomorphic conversational interaction design may be particularly effective in fostering warmth in mental health support contexts.",22-May-24,https://arxiv.org/abs/2405.13803,arxiv,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general_psychiatry,"adolescence, young_adulthood",united_states,Both,,,TRUE,,,TRUE,TRUE,,,,,TRUE,TRUE,,
AI-enabled conversational agent increases engagement with cognitive-behavioral therapy: A randomized controlled trial,"Timely support after referral to mental healthcare is crucial, yet patients often face prolonged wait times without intervention. Digital mental health interventions offer scalable solutions, but many struggle to achieve acceptable patient engagement. Tailoring and personalizing materials to individual needs is paramount for driving engagement, a task that generative artificial intelligence AI (genAI) is potentially able to achieve. To examine this promise, we conducted a randomized controlled trial using a genAI-enabled therapy app, Limbic Care, which delivers personalized cognitive behavioral therapy (CBT) materials, against PDF workbooks delivering static CBT content, as commonly used in standard care. Adults with elevated symptoms of anxiety or depression (N = 540) were randomly assigned to the app or control group for six weeks. The app group exhibited a threefold increase in engagement (2.4 times higher usage frequency, 3.8 times longer usage durations). While both groups showed similar overall symptom improvement, participants who engaged with the apps clinical personalization capabilities experienced significantly greater reductions in anxiety symptoms and enhanced well-being than those who engaged with the standard CBT materials. Importantly, the app was safe, with no increase in adverse events compared to standard care. Our findings suggest that genAI-enabled therapy apps can safely enhance patient engagement and improve clinical outcomes through clinically personalized interventions.",11/2/24,https://doi.org/10.1101/2024.11.01.24316565,medRxiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Random Controlled Trial (RCT),"6A7, 6B0","adolescence, young_adulthood, middle_adulthood, old",united_states,Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,11/2/24
Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation,"Amidst the growing interest in developing task-autonomous AI for automated mental health care, this paper addresses the ethical and practical challenges associated with the issue and proposes a structured framework that delineates levels of autonomy, outlines ethical requirements, and defines beneficial default behaviors for AI agents in the context of mental health support. We also evaluate ten state-of-the-art language models using 16 mental health-related questions designed to reflect various mental health conditions, such as psychosis, mania, depression, suicidal thoughts, and homicidal tendencies. The question design and response evaluations were conducted by mental health clinicians (M.D.s). We find that existing language models are insufficient to match the standard provided by human professionals who can navigate nuances and appreciate context. This is due to a range of issues, including overly cautious or sycophantic responses and the absence of necessary safeguards. Alarmingly, we find that most of the tested models could cause harm if accessed in mental health emergencies, failing to protect users and potentially exacerbating existing symptoms. We explore solutions to enhance the safety of current models. Before the release of increasingly task-autonomous AI systems in mental health, it is crucial to ensure that these models can reliably detect and manage symptoms of common psychiatric disorders to prevent harm to users. This involves aligning with the ethical framework and default behaviors outlined in our study. We contend that model developers are responsible for refining their systems per these guidelines to safeguard against the risks posed by current AI technologies to user mental health and safety.

Trigger warningContains and discusses examples of sensitive mental health topics, including suicide and self-harm.",4/2/24,https://doi.org/10.1101/2024.04.07.24305462,medRxiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,TRUE,,TRUE,Text,,,EHRs,,general_psychiatry,age_unspecified,unspecified,Unspecified,TRUE,TRUE,TRUE,,,,TRUE,,,,TRUE,,,,4/2/24
Evaluating Chatbots in Psychiatry: Rasch-Based Insights into Clinical Knowledge and Reasoning,"Chatbots are increasingly being recognized as valuable tools for clinical support in psychiatry. This study systematically evaluates their strengths and limitations in psychiatric clinical knowledge and reasoning. A total of 27 chatbots, including ChatGPT-o1-preview, were assessed using 160 multiple-choice questions derived from the 2023 and 2024 Taiwan Psychiatry Licensing Examinations. The Rasch model was employed to analyze chatbot performance, supplemented by dimensionality analysis and qualitative assessments of reasoning processes. Among the models, ChatGPT-o1-preview achieved the highest performance, with a JMLE ability score of 2.23, significantly exceeding the passing threshold (p < 0.001). It excelled in diagnostic and treatment reasoning and demonstrated a strong grasp of psychopharmacology concepts. However, limitations were identified in its factual recall, handling of niche topics, and occasional reasoning biases. Building on these findings, we have highlighted key aspects of a potential clinical workflow to guide the practical integration of chatbots into psychiatric practice. While ChatGPT-o1-preview holds significant potential as a clinical decision-support tool, its limitations underscore the necessity of human oversight. Continuous evaluation and domain-specific training are crucial to maximize its utility and ensure safe clinical implementation.",12/31/24,https://doi.org/10.1101/2024.12.30.24319383,medRxiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Interview (Questionnaire),general psychiatry,Unspecified,Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,12/31/24
Evaluating Risk Progression in Mental Health Chatbots Using Escalating Prompts,"The safety of large language models (LLMs) as mental health chatbots is not fully established. This study evaluated the risk escalation responses of publicly available ChatGPT conversational agents when presented with prompts of increasing depression severity and suicidality. The average referral point to a human was at the midpoint of escalating prompts. However, most agents only definitively recommended professional help at the highest level of risk. Few agents included crisis resources like suicide hotlines. The results suggest current LLMs may fail to escalate mental health risk scenarios appropriately. More rigorous testing and oversight are needed before deployment in mental healthcare settings.",9/12/23,https://doi.org/10.1101/2023.09.10.23295321,medRxiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,6A7,Unspecified,Unspecified,Unspecified,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,,,9/12/23
A Context-Aware Mental Health LLM Chatbot with Enhanced Security,"In an age where conversations with machines are becoming as common as chats with friends, mental health chatbots stand at the forefront of todays technology-driven world for improving patient care and optimizing medical procedures. The deployment of mental health chatbots presents significant challenges and a critical responsibility to uphold strict data privacy and security standards. A breach of data privacy can trigger severe implications such as monetary loss, legal repercussions, and even the closure of a business. On the other hand, security breaches often extend beyond immediate monetary damage, potentially leading to identity theft, operational disruptions, and lasting economic setbacks. To overcome these challenges and enhance data privacy and security, we introduce an innovative approach that integrates: 1) Parameter-efficient fine-tuning of a pre-trained Large Language Model (LLM) on a curated mental health dataset, 2) End-to-End Privacy mechanisms to protect sensitive patient data, and 3) Retrieval Augmented Generation (RAG) to enhance contextual awareness and improve the quality of LLM-generated responses. We conduct a thorough evaluation of our developed chatbots performance using a comprehensive set of metrics. Through the integration of these advancements, our goal is to build a secure and efficient LLM-based chatbot that enhances the accessibility and quality of mental healthcare while proactively addressing key privacy and security challenges.",21-Jun-25,https://openalex.org/W4411522440,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,FALSE,TRUE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general psychiatry,Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,,,
"A Machine Learning Enabled Approach for Mental and Physical Health Management Using OpenCV, NLP and IOT","Traditional healthcare systems have historically emphasized physical well-being, sidelining mental health concerns. The proposed machine learning and artificial intelligence system, with its all-encompassing strategy takes into account many physical health indicators, including oxygen saturation, heart rate, and body temperature. It introduces a groundbreaking dimension to mental health assessment through an emotion recognition model, utilizing Convolutional Neural Networks (CNNs) with Keras and TensorFlow. This innovative system interprets diverse mental states like happiness, disgust, anger, fear, happiness, sadness, and surprise, neutral using a trained CNN. The results are visualized through the laptop's built-in camera via OpenCV and deployed seamlessly through a Flask server. The web application, powered by the ESP8266WebServer, exhibits sensor data and network architecture. Evaluation of this integrated health management system is facilitated by a virtual doctor through a medical chatbot, leveraging a manually trained NLP dataset. The incorporation of large language models, Langchain techniques, and Hugging Face embeddings in NLP opens avenues for progressive healthcare solutions. The system's potential is further underscored by the creation of an interactive graphical user interface for the AI-enabled chatbot, employing Streamlit, Python packages, HTML5, and CSS3. This dynamic machine learning-based system not only positions itself as a virtual healthcare assistant but also contributes to the ongoing evolution of advanced technical solutions in healthcare. Its continuous learning mechanisms ensure adaptability and sustained enhancement, marking it as a forefront player in cutting-edge healthcare technologies.",17-Apr-24,https://openalex.org/W4394895215,OpenAlex,"Deep Learning Models, Foundation model",Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,general psychiatry,Unspecified,Unspecified,Unspecified,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,
The Effectiveness of LLMs in Mental Health,"It has an impact on everyone's health, which is why mental illness should be prioritized in the healthcare industry.However, it appears that this field is developing at a somewhat slow pace.AI (Artificial-Intelligence) technologies have recently received a lot of attention in a variety of fields, including mental health.Advanced AI approaches and machine learning algorithms have made it possible to provide personalized care that primarily focuses on providing emotional support tailored to a specific individual.We explore the possibility of using large language models like OpenAI's GPT3 and Facebook's Llama and Stanford's Alpaca to provide an effective conversational partner to people suffering with such mental health conditions where it may be helpful, such as depression and anxiety disorders.We compare the performance of the chatbots based on their responses to questions from counselchat.comdataset of therapist responses, and use the GPT4-davinci, the largest GPT4 model, as a judge to evaluate the quality of responses.",23-Apr,https://openalex.org/W4376134863,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,"6A7, 6B0",Unspecified,Unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,
Development of a Mental Health Chatbot Using Large Language Models for Indonesian Undergraduates,"Mental health issues among university students are an escalating global problem, especially in Indonesia. Young adults, especially college students, are a vulnerable demographic due to academic pressures, social transitions, and the challenges of independent living. This paper describes the development of a mental health chatbot specifically for Indonesian university students, utilizing large language models (LLMs). The development encompassed three essential phases: establishing a knowledge base, refining the LLM with culturally pertinent Indonesian data, and assessing the system. The findings demonstrated notable enhancements in the chatbot's precision, with training accuracy rising from 0.6 to 0.85 and validation accuracy from 0.5 to 0.79 due to fine-tuning and regularization. User acceptability testing including 58 students resulted in an acceptance percentage of 87.38%, indicating substantial satisfaction with the chatbot's response and usefulness. This research highlights the efficacy of AI-driven interventions in addressing the mental health requirements of Indonesian university students.",30-Oct-24,https://openalex.org/W4403919822,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general psychiatry,"adolescence, young adulthood",Indonesia,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,
MentalAgora: A Gateway to Advanced Personalized Care in Mental Health through Multi-Agent Debating and Attribute Control,"As mental health issues globally escalate, there is a tremendous need for advanced digital support systems. We introduce MentalAgora, a novel framework employing large language models enhanced by interaction between multiple agents for tailored mental health support. This framework operates through three stages: strategic debating, tailored counselor creation, and response generation, enabling the dynamic customization of responses based on individual user preferences and therapeutic needs. We conduct experiments utilizing a high-quality evaluation dataset TherapyTalk crafted with mental health professionals, shwoing that MentalAgora generates expert-aligned and user preference-enhanced responses. Our evaluations, including experiments and user studies, demonstrate that MentalAgora aligns with professional standards and effectively meets user preferences, setting a new benchmark for digital mental health interventions.",7/2/24,https://openalex.org/W4400375799,OpenAlex,Foundation model,Chatbot,Multi-Agent,Team Architecture,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Interview (Questionnaire),general psychiatry,Unspecified,Unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,TRUE,,,7/2/24
The Role of AI-Powered Chatbots in Reducing Student Anxiety in Online Learning Environments,"This dissertation examines the efficacy of AI-powered chatbots in alleviating student anxiety within online learning environments, addressing a pressing concern as mental health challenges among digitally engaged learners continue to rise. Employing a mixed-methods research design, the study integrates qualitative and quantitative data to evaluate the impact of AI chatbots on student well-being. Data collection includes student feedback on chatbot interactions, anxiety levels measured through validated psychological scales (e.g., GAD-7, STAI), and usage analytics derived from chatbot platforms across diverse educational contexts.The findings demonstrate that students who interacted with AI-powered chatbots experienced statistically significant reductions in anxiety levels, attributed to the chatbots' ability to provide real-time emotional support, personalized guidance, and immediate access to resources. These results underscore the potential of AI chatbots as scalable, cost-effective interventions for addressing psychological distress in online learning environments. Moreover, the study highlights the role of natural language processing (NLP) and machine learning (ML) algorithms in enabling chatbots to deliver context-aware, empathetic responses tailored to individual student needs. The implications of this research are twofold: (1) it contributes to the EdTech literature by demonstrating how AI-driven tools can enhance mental health support in digital education, and (2) it provides a framework for integrating AI chatbots into pedagogical strategies to promote student well-being and academic success. Importantly, the study identifies the potential of AI chatbots to bridge mental health resource gaps, particularly in under-resourced educational settings, where access to traditional support systems is often limited. By bridging the intersection of educational technology, mental health, and AI innovation, this research not only advances the field of EdTech but also paves the way for future interventions leveraging digital solutions to improve student mental health and learning outcomes.",25-Mar,https://openalex.org/W4408173928,OpenAlex,Foundation model,Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Not Applicable,6B0,Unspecified,Unspecified,Both,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,TRUE,,,
"GymBuddy and Elomia, AI-integrated applications, effects on the mental health of the students with psychological disorders","Digital mental health interventions, including AI-integrated applications, are increasingly utilized to support individuals with elevated symptoms of psychological distress. However, a gap exists in understanding their efficacy specifically for student populations. This study aimed to investigate the effects of GymBuddy, an AI-powered fitness and accountability app, and Elomia, an AI-based mental health chatbot, on the mental health of students at risk for psychological distress. A quasi-experimental study was conducted involving 65 participants who exhibited heightened psychological distress but did not have a formal diagnosis of a psychological disorder. Participants were randomly assigned to either the intervention group, which utilized GymBuddy and Elomia for structured mental health support, or the control group. Mental health outcomes such as anxiety, depression, and stress levels were assessed using standardized baseline, midpoint, and endpoint measures. Data were analyzed using Mixed ANOVA. The mixed ANOVA analysis revealed significant improvements across all measured mental health outcomes, including somatic symptoms, anxiety and insomnia, social dysfunction, and severe depression. Significant main effects of time and group membership were observed for all variables, indicating overall symptom reduction and baseline differences between groups. Moreover, significant interaction effects for somatic symptoms (F(2, 70) = 59.96, p < 0.0001,  = 0.63), anxiety and insomnia (F(2, 70) = 32.05, p < 0.0001,  = 0.48), social dysfunction (F(2, 70) = 59.96, p < 0.0001,  = 0.63), and severe depression (F(2, 70) = 32.05, p < 0.0001,  = 0.48) indicated that participants in the intervention group experienced significantly greater reductions in psychological distress compared to the control group. Our findings suggest that AI-integrated interventions like GymBuddy and Elomia may serve as effective tools for reducing psychological distress in student populations. Integrating AI technology into mental health interventions offers personalized support and guidance, addressing a crucial need in student populations. Further research is warranted to explore long-term outcomes and optimize the implementation of these interventions in educational settings.",8-Apr-25,https://openalex.org/W4409245232,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Not Applicable,"6A7, 6B0",adolescence,CHINA,Female only,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,,,
Real-Time Mental Health Prediction Using An AI Chatbot Powered By SVM,"The AI chatbot, a mental wellness tool, also provides an instant-self-help portal of non-judgmental advice to people experiencing depressive states. The chatbot utilizes ML and NLP techniques in generating responses to user inquiries and comments while keeping the conversation as engaging as possible and allowing the user to open up about their feelings and other abstractions in a safe manner. Through its features, users can analyze their emotional states and have a depression diagnosis while also allowing appropriate interventions and emotional support, and cognitive behavioral therapies can be performed in conversation.",24-Jun-25,https://openalex.org/W4411600265,OpenAlex,Machine Learning Models,Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Interview (Questionnaire),6A7,adolescence,Unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,
From Interaction to Attitude: Exploring the Impact of Human-AI Cooperation on Mental Illness Stigma,"AI conversational agents have demonstrated efficacy in social contact interventions for stigma reduction at a low cost. However, the underlying mechanisms of how interaction designs contribute to these effects remain unclear. This study investigates how participating in three human-chatbot interactions affects attitudes toward mental illness. We developed three chatbots capable of engaging in either one-way information dissemination from chatbot to a human or two-way cooperation where the chatbot and a human exchange thoughts and work together on a cooperation task. We then conducted a two-week mixed-methods study to investigate variations over time and across different group memberships. The results indicate that human-AI cooperation can effectively reduce stigma toward individuals with mental illness by fostering relationships between humans and AI through social contact. Additionally, compared to a one-way chatbot, interacting with a cooperative chatbot led participants to perceive it as more competent and likable, promoting greater empathy during the conversation. However, despite the success in reducing stigma, inconsistencies between the chatbot's role and the mental health context raised concerns. We discuss the implications of our findings for human-chatbot interaction designs aimed at changing human attitudes.",2-May-25,https://openalex.org/W4410049736,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,6A7,adolescence,Singapore,Both,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,,,
Engagement of Sri Lankan Medical Practitioners with AI and Mental Health Decision Support AI Chatbot,"This study addresses the critical challenges in mental healthcare delivery in Sri Lanka, where limited resources hinder accurate and efficient diagnoses, particularly in low-resource settings. Mental health disorders affect millions globally, highlighting the need for innovative solutions. This research investigates factors influencing medical practitioners engagement with AI technologies and evaluates a Mental Health Decision Support AI Chatbot designed to improve mental healthcare outcomes. Using a mixed-methods approach that includes a systematic literature review, survey analysis, and chatbot evaluation, the study identifies key technological, organizational, and environmental factors affecting engagement. The chatbot, developed and tested on the Microsoft Azure platform, shows significant potential in enhancing diagnostic efficiency and improving patient management. The findings contribute to the growing body of knowledge on AI applications in healthcare and provide practical insights for implementing AI-driven solutions in resource-constrained settings.",16-Apr-25,https://openalex.org/W4409495001,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general psychiatry,"young adulthood, middle adulthood",Sri Lanka,Both,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,,,
"AI-Powered Holistic Mental Health Monitoring: Integrating Facial Emotion Recognition, Chatbot, and Voicebot for Personalized Support","The proposed system will be enabled with various AI technologies of face emotion recognition, an emotional chatbot, and voice bot. Increasing awareness about the problems of mental health creates a demand for innovative solutions capable of filling the gaps in conventional mental health care. Our proposed system integrates pioneering support with the help of advanced AI techniques that offer personalized and accessible support to persons suffering from psychological distress. Face emotion recognition systems use advanced algorithms to detect, in real time, facial expressions, and therefore the emotional states or conditions of the users. The emotional chatbot can further include the user in empathetic conversations by offering customized emotional support and resources with natural language processing and sentiment analysis. This additional voice bot feature is expanding accessibility by enabling users to engage in therapeutic dialogues and get customized recommendations for self-care. With that, this AI-enabled mental health monitoring system integrates all those components specified herein in tandem, so as to bring a revolution in mental health care that nurtures resilience and well-being among individuals globally.",29-May-25,https://openalex.org/W4410853392,OpenAlex,"Foundation model, Deep Learning Models",Chatbot,Single Agent,Not Applicable,TRUE,TRUE,TRUE,FALSE,FALSE,"Text, Audio, Image",FALSE,TRUE,Not Applicable,Not Applicable,general psychiatry,Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,
A Comprehensive RAG-Based LLM for AI-Driven Mental Health Chatbot,"Mental health challenges affect millions worldwide, with approximately 970 million people experiencing conditions such as anxiety and depression. In the United States alone, 57.8 million adults struggle with mental illness. Despite the growing need for support, many face barriers such as stigma, limited access to care, and long waiting times, especially in underserved areas with few mental health resources. AI-powered chatbots are an emerging intervention option that offers available, accessible, and confidential services. The study below describes the development of an AI chatbot to support people in distress, using a fine-tuned version of the Llama model, which was trained on actual mental health counseling conversations. The chatbot uses Retrieval-Augmented Generation (RAG) to improve the relevance of its responses and make interactions more personalized. It also leverages LangChains ConversationBufferMemory to recall past conversations, allowing for more natural and meaningful dialogue. Facebook AI Similarity Search (FAISS) and Sentence Transformer embeddings also allow the retrieval of relevant materials to provide practical, real-time coping strategies. This chatbot uses natural language processing (NLP) and machine learning (ML) to bridge the gaps in mental health care, making it more affordable, accessible, and free of stigma. The current paper discusses the effectiveness of chatbots, the challenges in AI-driven mental health support, and future improvements in deeper personalization, improved response refinement, and integration with multimedia resources for a more holistic user experience.",2-Jun-25,https://openalex.org/W4411205260,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general psychiatry,Unspecified,Unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,
Designing Conversational Agents for Emotional Self-Awareness,"Emotional self-awareness is a crucial social-emotional skill for positive psychology and mental well-being. While prior works have used AI applications to foster emotion awareness, it is unclear what specific design features users prefer in the context of self-awareness skill practice. To better understand these features, we design, deploy, and evaluate Teddy, a mobile conversational agent that helps users practice verbalizing and reflecting on their emotions through experience sampling, affect labeling, and personal storytelling interactions. We conduct a user study with 18 days of interaction data from six participants to evaluate what aspects of Teddy contribute to emotion awareness. We find that users felt comfortable self-disclosing to the agent and found it helpful to verbalize their emotions, but desired more characteristics like social responsiveness and personalization. Based on our pilot study, we outline design insights for future work on AI agents that support practice of social-emotional skills such as emotional self-awareness.",16-Jan-24,https://openalex.org/W4390905735,OpenAlex,unspecified,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Not Applicable,general psychiatry,"adolescence, young adulthood",USA,Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,,,
We Care: Multimodal Depression Detection and Knowledge Infused Mental Health Therapeutic Response Generation,"The detection of depression through non-verbal cues has gained significant attention. Previous research predominantly centred on identifying depression within the confines of controlled laboratory environments, often with the supervision of psychologists or counsellors. Unfortunately, datasets generated in such controlled settings may struggle to account for individual behaviours in real-life situations. In response to this limitation, we present the Extended D-vlog dataset, encompassing a collection of 1, 261 YouTube vlogs. Additionally, the emergence of large language models (LLMs) like GPT3.5, and GPT4 has sparked interest in their potential they can act like mental health professionals. Yet, the readiness of these LLM models to be used in real-life settings is still a concern as they can give wrong responses that can harm the users. We introduce a virtual agent serving as an initial contact for mental health patients, offering Cognitive Behavioral Therapy (CBT)-based responses. It comprises two core functions: 1. Identifying depression in individuals, and 2. Delivering CBT-based therapeutic responses. Our Mistral model achieved impressive scores of 70.1% and 30.9% for distortion assessment and classification, along with a Bert score of 88.7%. Moreover, utilizing the TVLT model on our Multimodal Extended D-vlog Dataset yielded outstanding results, with an impressive F1-score of 67.8%",15-Jun-24,https://openalex.org/W4399794357,OpenAlex,Foundation model,Non-Chat Agent,Single Agent,Not Applicable,TRUE,TRUE,TRUE,FALSE,FALSE,"Text, Video, Images, Audio",TRUE,TRUE,Not Applicable,Interview (Questionnaire),6A7,Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,,,
An AI based mental health support chat-bot for cyber bullied victims,"In recent days, an unprecedented increase in cyberattacks has been seen globally. There are several cyberbullied victims who commit suicide every year as a consequence of their sensitive information being publicized. The victims who are being bullied or blackmailed need mental support and guidance on how to react. And some of the victims, due to defamation, may go as far as taking their lives. So, we've built two AI chatbots named Guidance Bot and Friendly Bot using Botpress and BrainShop API specifically for cyberbullying victims, as they may not directly plead for human help. Conversing with bots might seem more secure and reassuring than having a conversation with humans, as bots maintain secrecy. The bot comprehends the user's query and responds accordingly using natural language processing (NLP) and natural language understanding (NLU). We integrated our trained and developed chatbots into a website, which mainly includes three features, such as providing guidance on how to approach concerned authorities, providing information regarding cyberbullying, answering queries related to mental health, and finally assisting the victim's mental health. Moreover, we have implemented a WhatsApp redirection feature, enabling victims to effortlessly connect with mental health specialists by adding their contact numbers.",7-Dec-23,https://openalex.org/W4389428805,OpenAlex,Rule-based models,Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,"6A7, 6B0",Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,
Lumina: Bridging Healthcare Knowledge Gaps Through AI-Powered Assistance,"Healthcare systems are increasingly incorporating artificial intelligence to address diverse medical needs, including improving access to healthcare resources and aiding non-medical individuals in understanding complex medical information. One such innovation is Lumina, a healthcare chatbot designed to provide personalised medical assistance, leveraging advanced AI technologies. Lumina aims to simplify medical processes such as wound detection, prescription analysis, and health insurance advisory through an integrated and user-friendly interface.Additionally, AI-powered psychometric analysis systems can assess an individual's stress levels and predict mental states by analyzing responses to dynamically generated questions, categorizing them into states like stable, depressive, impulsive, or anxious. These systems utilize sentiment analysis and logistic regression to provide personalized recommendations, enhancing mental health support and coping strategies.",4/17/25,https://openalex.org/W4409617143,OpenAlex,Foundation model,Chatbot,Non-Agent,Not Applicable,TRUE,TRUE,TRUE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Not Applicable,"6A7, 6B0, 6B4",Unspecified,Unspecified,Unspecified,TRUE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,,,
"Advancing Conversational Psychotherapy: Integrating Privacy, Dual-Memory, and Domain Expertise with Large Language Models","Mental health has increasingly become a global issue that reveals the limitations of traditional conversational psychotherapy, constrained by location, time, expense, and privacy concerns. In response to these challenges, we introduce SoulSpeak, a Large Language Model (LLM)-enabled chatbot designed to democratize access to psychotherapy. SoulSpeak improves upon the capabilities of standard LLM-enabled chatbots by incorporating a novel dual-memory component that combines short-term and long-term context via Retrieval Augmented Generation (RAG) to offer personalized responses while ensuring the preservation of user privacy and intimacy through a dedicated privacy module. In addition, it leverages a counseling chat dataset of therapist-client interactions and various prompting techniques to align the generated responses with psychotherapeutic methods. We introduce two fine-tuned BERT models to evaluate the system against existing LLMs and human therapists: the Conversational Psychotherapy Preference Model (CPPM) to simulate human preference among responses and another to assess response relevance to user input. CPPM is useful for training and evaluating psychotherapy-focused language models independent from SoulSpeak, helping with the constrained resources available for psychotherapy. Furthermore, the effectiveness of the dual-memory component and the robustness of the privacy module are also examined. Our findings highlight the potential and challenge of enhancing mental health care by offering an alternative that combines the expertise of traditional therapy with the advantages of LLMs, providing a promising way to address the accessibility and personalization gap in current mental health services.",4-Dec-24,https://openalex.org/W4405088996,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general psychiatry,Unspecified,Unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,,,
"""You tell me"": A Dataset of GPT-4-Based Behaviour Change Support Conversations","Conversational agents are increasingly used to address emotional needs on top of information needs. One use case of increasing interest are counselling-style mental health and behaviour change interventions, with large language model (LLM)-based approaches becoming more popular. Research in this context so far has been largely system-focused, foregoing the aspect of user behaviour and the impact this can have on LLM-generated texts. To address this issue, we share a dataset containing text-based user interactions related to behaviour change with two GPT-4-based conversational agents collected in a preregistered user study. This dataset includes conversation data, user language analysis, perception measures, and user feedback for LLM-generated turns, and can offer valuable insights to inform the design of such systems based on real interactions.",29-Jan-24,https://openalex.org/W4391376818,OpenAlex,Foundation model,Chatbot,Non-Agent,Not Applicable,,TRUE,,,TRUE,Text,TRUE,,,,general_psychiatry,"adolescence, young_adulthood, middle_adulthood, old",unspecified,Both,TRUE,TRUE,,,,TRUE,,,,,,TRUE,,,
Detecting anxiety and depression in dialogues: a multi-label and explainable approach,"Anxiety and depression are the most common mental health issues worldwide, affecting a non-negligible part of the population. Accordingly, stakeholders, including governments' health systems, are developing new strategies to promote early detection and prevention from a holistic perspective (i.e., addressing several disorders simultaneously). In this work, an entirely novel system for the multi-label classification of anxiety and depression is proposed. The input data consists of dialogues from user interactions with an assistant chatbot. Another relevant contribution lies in using Large Language Models (LLMs) for feature extraction, provided the complexity and variability of language. The combination of LLMs, given their high capability for language understanding, and Machine Learning (ML) models, provided their contextual knowledge about the classification problem thanks to the labeled data, constitute a promising approach towards mental health assessment. To promote the solution's trustworthiness, reliability, and accountability, explainability descriptions of the model's decision are provided in a graphical dashboard. Experimental results on a real dataset attain 90 % accuracy, improving those in the prior literature. The ultimate objective is to contribute in an accessible and scalable way before formal treatment occurs in the healthcare systems.",23-Dec-24,https://openalex.org/W4405768286,OpenAlex,Foundation model,Chatbot,Non-Agent,Not Applicable,TRUE,,,,TRUE,Text,TRUE,,,,"6A7, 6B0",age_unspecified,,Unspecified,,TRUE,TRUE,,,,,TRUE,TRUE,,,TRUE,,,
Adaptive Mental Health And Mood Uplifting Chatbot,"Mental health concerns are on the rise worldwide, and the stigma and lack of confidentiality surrounding therapy make it difficult for many people to seek the help they need. In response to this problem, my research proposes the development of an AI chatbot that acts as a virtual psychologist, providing accessible and confidential mental health support. Using NLP technology, the chatbot will be able to understand and respond to user queries, offering personalized therapy sessions based on individual input. My paper explores various NLP models and techniques that can be used for the chatbot's development, as well as ethical and privacy considerations. We are truly passionate about the potential of this chatbot to revolutionize the mental health industry, providing affordable and accessible therapy while breaking down the stigma associated with seeking help. While there is still much work to be done to ensure the chatbot's reliability and effectiveness, We are excited to be at the forefront of this innovative new field, working towards a brighter future for mental health support.",23-Nov-23,https://openalex.org/W4388937838,OpenAlex,Deep Learning Models,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,TRUE,,,,"6A7, 6B0, 6B4, 7A0",age_unspecified,unspecified,Unspecified,,,TRUE,,,,,,TRUE,,,TRUE,,,
On the Reliability of Large Language Models to Misinformed and Demographically-Informed Prompts,"Abstract We investigate and observe the behavior and performance of Large Language Model (LLM)backed chatbots in addressing misinformed prompts and questions with demographic information within the domains of Climate Change and Mental Health. Through a combination of quantitative and qualitative methods, we assess the chatbots' ability to discern the veracity of statements, their adherence to facts, and the presence of bias or misinformation in their responses. Our quantitative analysis using True/False questions reveals that these chatbots can be relied on to give the right answers to these closeended questions. However, the qualitative insights, gathered from domain experts, shows that there are still concerns regarding privacy, ethical implications, and the necessity for chatbots to direct users to professional services. We conclude that while these chatbots hold significant promise, their deployment in sensitive areas necessitates careful consideration, ethical oversight, and rigorous refinement to ensure they serve as a beneficial augmentation to human expertise rather than an autonomous solution. Dataset and assessment information can be found at https://github.com/tolusophy/EdgeofTomorrow .",8-Jan-25,https://openalex.org/W4406184544,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,,TRUE,,TRUE,Text,,TRUE,,,general_psychiatry,age_unspecified,unspecified,Unspecified,,,,,,,TRUE,,TRUE,TRUE,,TRUE,,,
PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents,"Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ human evaluators to examine the generated content across various psychological constructs, including depression, cognitive distortions, and personality traits. Results demonstrate that PsychoGAT serves as an effective assessment tool, achieving statistically significant excellence in psychometric metrics such as reliability, convergent validity, and discriminant validity. Moreover, human evaluations confirm PsychoGAT's enhancements in content coherence, interactivity, interest, immersion, and satisfaction.",2/19/24,https://openalex.org/works/w4402671526,OpenAlex,Foundation model,Chatbot,Multi-Agent,Team Architecture,,TRUE,TRUE,,TRUE,Text,TRUE,,,,general_psychiatry,"adolescence, young_adulthood",unspecified,Unspecified,FALSE,,,,,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,,,2024-02-19T18:00:30Z
Synergistic Precision: Integrating Artificial Intelligence and Bioactive Natural Products for Advanced Prediction of Maternal Mental Health During Pregnancy,"Background: Present-day advancements in Artificial Intelligence (AI) and Machine Learning offer promising avenues for addressing psychological health challenges in pregnant women. Despite significant strides, there remains a gap in effective, accessible and personalised interventions for managing mental health risks during pregnancy, which can have profound implications on both maternal and fetal well-being. Aim: This study aims to develop a predictive model for monitoring and assessing the psychological health of pregnant women. The goal is to create an accessible tool using machine learning algorithms and sustainable technologies to provide early warnings and support interventions for mental health issues. Methods: The research utilised a dataset comprising psychological health indicators of pregnant women, including symptoms such as anxiety, depression and sleep disturbances. Machine Learning models, including Random Forest, Decision Tree, Support Vector Machine (SVM), Logistic Regression and Gaussian Naive Bayes, were employed to classify mental health status. The models were evaluated using metrics like accuracy, precision, recall and F1-score. A web-based chatbot was developed to integrate the predictive model, providing real-time mental health assessments and personalised recommendations. Results: The Random Forest model demonstrated superior performance, achieving an accuracy of 92%, outperforming other models like SVM and Decision Tree, which achieved accuracies of 88% and 85%, respectively. Integrating the model into a web-based chatbot provided users with an interactive and user-friendly platform for mental health monitoring. Initial feedback from users indicated a 70% satisfaction rate with the tools ease of use and perceived accuracy. Conclusion: The study successfully developed a machine learning-based predictive model for assessing the psychological health of pregnant women, integrated into a web-based chatbot. This approach offers a promising, scalable solution for early detection and management of mental health challenges during pregnancy, potentially enhancing maternal and fetal outcomes through timely interventions.",24-Nov,https://openalex.org/W4405211202,OpenAlex,Machine Learning Models,Chatbot,Non-Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,,,EHRs,Interview (Questionnaire),6.00E+02,unspecified,India,Female only,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,
Mental Illness Aiding through Machine Learning and Soft-Computing,"This research presents an innovative chatbot system aimed at addressing mental health challenges through a fusion of Natural Language Processing (NLP) and Artificial Intelligence (AI). The chatbot stands out by offering personalized therapeutic conversations and tailored activity recommendations, enhancing the well-being of users. By integrating web scraping, the system enriches user experiences with relevant online stories. The study introduces a hybrid approach for response accuracy and includes multilingual support to cater to a diverse user base. The results of this study demonstrate substantial enhancements in both response accuracy and sentiment analysis. Overall, this chatbot system has the potential to make a significant impact on mental health support, showcasing the advancements in technology for the betterment of user well-being in an ever-evolving digital landscape.",12-Feb-24,https://openalex.org/W4391743076,OpenAlex,Deep Learning Models,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,TRUE,Text,TRUE,TRUE,,,6.00E+02,age_unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,
"USE OF AI IN MHEALTH MOBILE APPLICATIONS: THE INFLUENCE OF VIRTUAL THERAPISTS' VISUAL CHARACTERISTICS ON USER PERCEPTION""","Implementing artificial intelligence within the applications for mental health has improved their services. As the most common application, Chatbots that simulate communication with a therapist stand out. With this feature, individuals who shy away from accessing therapy due to the stigma associated with mental health have the opportunity to access therapy without fear of negative feedback. This research aimed to investigate the impact of virtual therapists' visual characteristics on users' reactions. Six characters, created using MetaHuman Creator software and augmented with audio recordings, were evaluated by respondents on parameters such as likability, naturalness of appearance, excitement, positivity, and sense of trustworthiness. Video stimuli were employed to encourage these assessments. Results indicated that the third female character received the highest likability rating, while the first male character was considered the most natural and reliable. The first female character aroused the most incredible excitement and positive feelings. In contrast, the second male character received the lowest ratings across multiple parameters, and the second female character scored poorly overall. Statistical analyses, including T-tests and one-factor ANOVA, revealed no significant variations across characters for most parameters except for excitement. Post hoc tests highlighted notable differences, especially between the second female character, other female characters, and the first male character. Correlation analysis revealed a strong positive relationship between likability and the naturalness of appearance with user trust. These findings underscore the importance of planning virtual therapists' visual characteristics to suit users, revealing interesting observations such as respondents' perceived differences in character voices and the greater attention to detail exhibited by male subjects towards male characters.",16-Jul-05,https://openalex.org/W4403643983,OpenAlex,Rule-based models,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,TRUE,"Audio,Video",FALSE,TRUE,Not Applicable,Not Applicable,general_psychiatry,"young_adulthood, middle_adulthood",sebria,Both,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,
Rule-Based Chatbot for Early Self-Depression Indication: A Promising Approach,"Depression is a prevalent mental health condition worldwide, often characterized by persistent sadness, loss of interest or pleasure, and feelings of worthlessness. Depression is the leading cause of mental health issues worldwide, and it is becoming more severe without self-awareness, early screening, and further medication. Early detection and intervention are critical in mitigating its adverse effects. Leveraging advancements in Artificial Intelligence (AI), particularly in Natural Language Processing (NLP), chatbots have emerged as potential tools for early depression indication. Chatbots are beneficial tools in the mental health domain, such as in assisting mental health risk users. This paper presents the development of a rule-based chatbot aimed at detecting early signs of depression through conversational interactions by screening symptoms of depression. Predefined rules are developed to ensure the assessment can generate reliable results. The rule-based chatbot is developed to assist in depression indication assessment for mental health-risk individuals at an early stage and provide the risky patient with appropriate support and resources. The chatbot assessment has adopted the Depression Anxiety and Stress Scale 21 (DASS21) instrument. Based on the System Usability Scale (SUS) results, the rule-based chatbot has been accepted by all 30 respondents with good acceptance of an average SUS score of 77.2. Thus, the outcome of this chatbot can be utilized as a professional platform to encourage self-disclosure of mental depression indications for users, and it can be beneficial as the initial reference before recommending further action before the earlier help-seeking.",9/18/24,https://openalex.org/W4404889071,OpenAlex,Rule-based models,Chatbot,Single Agent,Not Applicable,FALSE,FALSE,TRUE,FALSE,FALSE,Text,FALSE,FALSE,FALSE,Interview (Questionnaire),6A7,age_unspecified,malaysia,Unspecified,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,,,
AI-driven digital twin framework for personalized mental health monitoring and intervention,A growing global mental health crisis encounters ongoing obstacles due to discriminatory attitudes and spatial needs and rising treatment expenses. This study develops an innovative dialogue platform that offers personalized mental health assessments alongside prescribing specific virtual care recommendations according to real-time identified severity levels. Through Digital Twin technology a virtual mental state model updates and analyses patient data to generate tailored care experiences. Through a precise AI chatbot developed in collaboration with clinical psychopathologists our system operates as an efficient mental health symptom measurement tool. The BERT-based approach trained specifically on E-DAIC data delivers depression and other mental distress level identification features and classification functionality. The system employed NLP technology to provide feedback about individual psychological state during user dialogues which generated directed guidance. Our system underwent extensive testing that demonstrated 85% classification accuracy surpassing conventional methods. User tests validated the system interface model through a satisfaction score of 90% from satisfied participants. Research results validate that AI-driven mental health assessments assess psychological states accurately while delivering accessible reliable results as part of emotional support while eliminating conventional barriers to treatment. Digital twins revolutionize mental healthcare through their ability to develop stigma-free services in a new digital age where scalability and affordable treatment become possible.,2/15/25,https://openalex.org/W4407573342,OpenAlex,Small pre-trained model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,TRUE,"Text, Audio",TRUE,FALSE,Not Applicable,Interview (Questionnaire),6A7,age_unspecified,unspecified,Unspecified,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,,,
Integrating AI chatbots into mental health strategies: Pre-post intervention study on support for unemployed graduates in China,"Artificial intelligence (AI) plays a crucial role in tertiary education; reshaping teaching and learning experiences, streamlining administrative tasks, and driving research innovation. This study aims to assess the effectiveness of AI chatbot interventions on unemployed university graduates in China, in reducing anxiety and enhancing career readiness. A mixed-methods research design was employed, incorporating a quantitative approach using a randomized pretest-posttest control group design and a qualitative approach involving focus group discussion. Sixty unemployed graduates in China were randomly assigned to an experimental group (receiving eight structured AI chatbot counseling via Chat GPT); and control group (receiving human counseling and supplementary activities). Anxiety and career readiness were assessed using the Generalized Anxiety Disorder-7 scale (GAD-7) and Career Adapt-Abilities Scale (CAAS). The findings indicated that, following the intervention, the experimental group demonstrated significant improvements in both anxiety and career readiness, and greater than those measured in the control group. In terms of user experience, AI chatbots in counseling offer several advantages, including enhanced accessibility, constant availability, and anonymity. According to university policymakers, counselors, and technology specialists, AI chatbots can be effectively utilized by a large number of students to address diverse mental health concerns, ensuring cost efficiency compared to human counseling. Therefore, AI chatbot interventions represent as a feasible tool within tertiary education for delivering mental health and career development services under the guidance and oversight of counselors or educational advisors. Successful integration requires preparation in terms of leadership support, institutional policies, resource allocation, and capacity building for both students and staff.",1/1/25,https://openalex.org/W4411573981,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,TRUE,Text,FALSE,FALSE,Not Applicable,Interview (Questionnaire),6B0,adolescence,china,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,
Emotion_aware psychological first aid: Integrating BERT_based emotional distress detection with Psychological First Aid_Generative Pre_Trained Transformer chatbot for mental health support,"Abstract Mental health disorders have a global prevalence of 25%, according to the WHO, and this is exacerbated by factors such as stigma, geographical location, and a worldwide shortage of practitioners. Mental health chatbots have been developed to address these barriers, but these systems lack key features such as emotion recognition, personalisation, multilingual support, and ethical appropriateness. This paper introduces an innovative mental health support system that integrates BERTbased emotional distress detection with a psychological first aid (PFA)generative pretrained transformer (PFAGPT) model, providing an emotionaware PFA chatbot. The methodology leverages deep learning models, utilising bidirectional encoder representations from transformers (BERT) for emotional distress detection and finetuning GPT3.5 on therapy transcripts for PFA chatbot development. The findings demonstrate BERT's superior accuracy (93%) for emotional distress detection compared to bidirectional long shortterm memory. The multilingual PFA chatbot developed using the PFAGPT model demonstrated superior BERT scores (exceeding 83%) and proficiently provided ethical PFA. A proof of concept has been developed to illustrate the integration of the emotional distress detection model with the novel generative conversational agent for PFA. This integrated approach holds significant potential in overcoming existing barriers to mental health support and has the potential to transform mental health support, offering timely and accessible care through AIpowered psychological interventions.",25-Jan-25,https://openalex.org/W4406820817,OpenAlex,Foundation model,Chatbot,Multi-Agent,Flat Architecture,TRUE,TRUE,FALSE,FALSE,TRUE,Text,TRUE,FALSE,Not Applicable,Interview (Questionnaire),general_psychiatry,age_unspecified,unspecified,Unspecified,TRUE,FALSE,TRUE,TRUE,TRUE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,,,
On the reliability of Large Language Models to misinformed and demographically informed prompts,"Abstract We investigate and observe the behavior and performance of Large Language Model (LLM)backed chatbots in addressing misinformed prompts and questions with demographic information within the domains of Climate Change and Mental Health. Through a combination of quantitative and qualitative methods, we assess the chatbots' ability to discern the veracity of statements, their adherence to facts, and the presence of bias or misinformation in their responses. Our quantitative analysis using True/False questions reveals that these chatbots can be relied on to give the right answers to these closeended questions. However, the qualitative insights, gathered from domain experts, shows that there are still concerns regarding privacy, ethical implications, and the necessity for chatbots to direct users to professional services. We conclude that while these chatbots hold significant promise, their deployment in sensitive areas necessitates careful consideration, ethical oversight, and rigorous refinement to ensure they serve as a beneficial augmentation to human expertise rather than an autonomous solution. Dataset and assessment information can be found at https://github.com/tolusophy/EdgeofTomorrow .",1/8/25,https://openalex.org/W4406184544,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,TRUE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,general_psychiatry,age_unspecified,unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,,,
MADP: Multi-Agent Deductive Planning for Enhanced Cognitive-Behavioral Mental Health Question Answer,"The Mental Health Question Answer (MHQA) task requires the seeker and supporter to complete the support process in one-turn dialogue. Given the richness of help-seeker posts, supporters must thoroughly understand the content and provide logical, comprehensive, and well-structured responses. Previous works in MHQA mostly focus on single-agent approaches based on the cognitive element of Cognitive Behavioral Therapy (CBT), but they overlook the interactions among various CBT elements, such as emotion and cognition. This limitation hinders the models' ability to thoroughly understand the distress of help-seekers. To address this, we propose a framework named Multi-Agent Deductive Planning (MADP), which is based on the interactions between the various psychological elements of CBT. This method guides Large Language Models (LLMs) to achieve a deeper understanding of the seeker's context and provide more personalized assistance based on individual circumstances. Furthermore, we construct a new dataset based on the MADP framework and use it to fine-tune LLMs, resulting in a specialized model named MADP-LLM. We conduct extensive experiments, including comparisons with multiple LLMs, human evaluations, and automatic evaluations, to validate the effectiveness of the MADP framework and MADP-LLM.",1/27/25,https://openalex.org/W4406881038,OpenAlex,Foundation model,Chatbot,Multi-Agent,Hybrid Architecture,FALSE,TRUE,FALSE,FALSE,TRUE,Text,TRUE,TRUE,Not Applicable,Not Applicable,general_psychiatry,age_unspecified,unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,,,
LLM-empowered Chatbots for Psychiatrist and Patient Simulation: Application and Evaluation,"Empowering chatbots in the field of mental health is receiving increasing amount of attention, while there still lacks exploration in developing and evaluating chatbots in psychiatric outpatient scenarios. In this work, we focus on exploring the potential of ChatGPT in powering chatbots for psychiatrist and patient simulation. We collaborate with psychiatrists to identify objectives and iteratively develop the dialogue system to closely align with real-world scenarios. In the evaluation experiments, we recruit real psychiatrists and patients to engage in diagnostic conversations with the chatbots, collecting their ratings for assessment. Our findings demonstrate the feasibility of using ChatGPT-powered chatbots in psychiatric scenarios and explore the impact of prompt designs on chatbot behavior and user experience.",5/23/23,https://openalex.org/W4378464713,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,TRUE,Text,FALSE,TRUE,Not Applicable,Not Applicable,general_psychiatry,"adolescence, young_adulthood",China,Both,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,,,
"NeuroSafe-Advancing Mental Health using AI, ML and Blockchain","This study offers a novel platform for mental health support that incorporates state-of-the-art technology to improve user experience. The platform analyzes user journals using BERT-based emotion analysis, and it generates weekly reports that show emotional trends. By utilizing blockchain technology, individuals can grant and withdraw therapist access as needed, while also ensuring safe record-keeping. Therapists can promote cooperative and private treatment by safely uploading session recordings to the blockchain. Real-time help is provided via a generative AI-based chatbot that makes use of LSTM and Seq2Seq architectures, and peer interactions are facilitated by a community chat app. The platform's usefulness is demonstrated by the results, and user feedback validates its influence on mental health assistance. This project creates a user-centric framework that integrates blockchain security, emotion analysis, and AI-driven interactions to provide a holistic approach to mental health.",5/6/24,https://openalex.org/W4396666866,OpenAlex,Small pre-trained model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,TRUE,Text,FALSE,TRUE,Not Applicable,Systematic reviews / Meta analysis,general_psychiatry,age_unspecified,unspecified,Unspecified,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,,,
Human guided empathetic AI agent for mental health support leveraging reinforcement learning-enhanced retrieval-augmented generation,"Global mental health issues is increasing due to problems such as the social stigma around treatment, a long-neglected burdens of insufficient resources, and the rising tide of mental issues. Large language models (LLMs) can accelerate the development of comprehensive, extensive solutions that support mental health. However, the LLMs capability to generate and comprehend human-like conversations is one of the main challenges faced by psychiatric counselling. This work proposes a mental health counselling LLM-based conversational agent that relies on the integration of Retrieval Augmented Generation (RAG) and Reinforcement learning. RAG provides the proposed LLM-based conversational agent with contextually relevant and accurate responses through useful information extracted from a curated dataset of psychological questions and answers pooled from mental health forums. Reinforcement Learning Integrated reward Model trained with Human feedback has also been used in the proposed framework to ensure contractually of the responses generated with moral and human values. By setting up a reward mechanism that considers variables like user feedback and empathetic scores of responses, the proposed Conversational Agent learns to prioritize empathetic answers and the ones that are user preferable. With the utilization of reward-based training, the agent was able to show substantial improvements in response quality. Improved emotional alignment, steady training dynamics, decreased hallucination rates with responses having less distress and increased empathy values were the significant outcomes. The proposed methodology ensures that the conversational agent remains attentive to the emotional requirements of people seeking for mental health care and provide improved relevance and accuracy in its responses.",2/1/25,https://openalex.org/W4407852580,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,TRUE,FALSE,TRUE,Text,TRUE,FALSE,Not Applicable,Systematic reviews / Meta analysis,general_psychiatry,age_unspecified,unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,,,
Prompt Engineering an Informational Chatbot for Educating about Mental Health: Utilizing a Multi-Agent Approach for Enhanced Compliance with Prompt Instruction,"People with schizophrenia often present with cognitive impairments that may hinder their ability to learn about their condition. Education platforms powered by Large Language Models (LLMs) have the potential to improve accessibility of mental health information. However, the black-box nature of LLMs raises ethical and safety concerns regarding the controllability over chatbots. In particular, prompt-engineered chatbots may drift from their intended role as the conversation progresses and become more prone to hallucinations. To develop and evaluate a Critical Analysis Filter (CAF) system that ensures that an LLM-powered prompt-engineered chatbot reliably complies with predefined its instructions and scope while delivering validated mental health information. For a proof-of-concept, we prompt-engineered an educational schizophrenia chatbot powered by GPT-4 that can dynamically access information from a schizophrenia manual written for people with schizophrenia and caregivers. In the CAF, a team of prompt-engineered LLM agents are used to critically analyze and refine the chatbot's responses and deliver real-time feedback to the chatbot. To assess the ability of the CAF to re-establish the chatbot's adherence to its instructions, we generate three conversations (by conversing with the chatbot with the CAF disabled) wherein the chatbot starts to drift from its instructions towards various unintended roles. We use these checkpoint conversations to initialize automated conversations between the chatbot and adversarial chatbots designed to entice it towards unintended roles. Conversations were repeatedly sampled with the CAF enabled and disabled respectively. Three human raters independently rated each chatbot response according to criteria developed to measure the chatbot's integrity; specifically, its transparency (such as admitting when a statement lacks explicit support from its scripted sources) and its tendency to faithfully convey the scripted information in the schizophrenia manual. In total, 36 responses (3 different checkpoint conversations, 3 conversations per checkpoint, 4 adversarial queries per conversation) were rated for compliance with the CAF enabled and disabled respectively. Activating the CAF resulted in a compliance score that was considered acceptable (2) in 67.0% of responses, compared to only 8.7% when the CAF was deactivated. Although more rigorous testing in realistic scenarios is needed, our results suggest self-reflection mechanisms could enable LLMs to be used effectively and safely in educational mental health platforms. This approach harnesses the flexibility of LLMs while reliably constraining their scope to appropriate and accurate interactions.",26.Mar.2025,https://openalex.org/W4407901897,OpenAlex,Foundation model,Chatbot,Multi-Agent,Team Architecture,TRUE,FALSE,FALSE,FALSE,TRUE,Text,FALSE,FALSE,Not Applicable,Systematic reviews / Meta analysis,6A2,age_unspecified,Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,,,
ComPeer: A Generative Conversational Agent for Proactive Peer Support,"Conversational Agents (CAs) acting as peer supporters have been widely studied and demonstrated beneficial for people's mental health. However, previous peer support CAs either are user-initiated or follow predefined rules to initiate the conversations, which may discourage users to engage and build relationships with the CAs for long-term benefits. In this paper, we develop ComPeer, a generative CA that can proactively offer adaptive peer support to users. ComPeer leverages large language models to detect and reflect significant events in the dialogue, enabling it to strategically plan the timing and content of proactive care. In addition, ComPeer incorporates peer support strategies, conversation history, and its persona into the generative messages. Our one-week between-subjects study (N=24) demonstrates ComPeer's strength in providing peer support over time and boosting users' engagement compared to a baseline user-initiated CA.",2024-07-25T14:19:35Z,https://openalex.org/W4403661795,OpenAlex,Foundation model,Chatbot,Multi-Agent,Hierarchical Architecture,FALSE,TRUE,FALSE,FALSE,TRUE,Text,FALSE,TRUE,Not Applicable,Not Applicable,general_psychiatry,"adolescence, young_adulthood",china,Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,,,
Improving the Well-Being of Financially Stressed University Students: A CBT-Based AI Chat-bot Intervention,"<sec> <title>BACKGROUND</title> Mental health concerns, particularly depression, anxiety, and loneliness, are prevalent among university students, with financial stress further compounding these issues. This study investigates the efficacy of a CBT-based AI chatbot in improving the well-being of financially stressed university students. </sec> <sec> <title>OBJECTIVE</title> This study investigates the efficacy of a CBT-based AI chatbot in improving the well-being of financially stressed university students. </sec> <sec> <title>METHODS</title> In this randomized controlled trial, 100 university students aged 18-24 years were randomly allocated to an intervention (n=50) or waitlist control group (n=50). The intervention group interacted with a CBT-based AI chatbot for seven consecutive days, receiving personalized support and coping strategies to alleviate psychological distress. Anxiety, depression, and loneliness were assessed at baseline, Day 3, and Day 7 using the Generalized Anxiety Disorder 7-item (GAD-7), Centre for Epidemiologic Studies Depression (CES-D), and UCLA Loneliness Scale. </sec> <sec> <title>RESULTS</title> Compared to controls, the intervention group showed significant reductions in depression (F = 8.48, p &lt; .001) and loneliness (F = 4.69, p = .011) at Day 7 relative to baseline. GAD-7 scores did not show significant changes. The chatbot's effectiveness was moderated by financial stress, with participants with high baseline financial stress (PIFS scores above the median) demonstrating greater improvements in CES-D (F = 11.56, p &lt; .001) and UCLA Loneliness scores (F = 11.18, p &lt; .001) compared to those with low financial stress. </sec> <sec> <title>CONCLUSIONS</title> The CBT-based AI chatbot effectively reduced depression and loneliness in financially stressed university students. AI-driven interventions show promise in improving student mental health, especially for those facing financial hardship. </sec>",21-Jul-25,https://openalex.org/W4400577738,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,TRUE,Text,FALSE,FALSE,Not Applicable,Interview (Questionnaire),"6A7, 6B0, 6B4",adolescence,china,Both,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,
Evaluating an LLM-Powered Chatbot for Cognitive Restructuring: Insights from Mental Health Professionals,"Recent advancements in large language models (LLMs) promise to expand mental health interventions by emulating therapeutic techniques, potentially easing barriers to care. Yet there is a lack of real-world empirical evidence evaluating the strengths and limitations of LLM-enabled psychotherapy interventions. In this work, we evaluate an LLM-powered chatbot, designed via prompt engineering to deliver cognitive restructuring (CR), with 19 users. Mental health professionals then examined the resulting conversation logs to uncover potential benefits and pitfalls. Our findings indicate that an LLM-based CR approach has the capability to adhere to core CR protocols, prompt Socratic questioning, and provide empathetic validation. However, issues of power imbalances, advice-giving, misunderstood cues, and excessive positivity reveal deeper challenges, including the potential to erode therapeutic rapport and ethical concerns. We also discuss design implications for leveraging LLMs in psychotherapy and underscore the importance of expert oversight to mitigate these concerns, which are critical steps toward safer, more effective AI-assisted interventions.",2025-01-26T16:53:27Z,https://openalex.org/W4406880802,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,FALSE,TRUE,FALSE,TRUE,Text,FALSE,TRUE,Not Applicable,Not Applicable,"6A7, 6B0","adolescence, young_adulthood",united_states,Both,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,,,
Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation Tools,"Objective: This study aims to develop and validate an evaluation framework to ensure the safety and reliability of mental health chatbots, which are increasingly popular due to their accessibility, human-like interactions, and context-aware support. Materials and Methods: We created an evaluation framework with 100 benchmark questions and ideal responses, and five guideline questions for chatbot responses. This framework, validated by mental health experts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation methods explored included large language model (LLM)-based scoring, an agentic approach using real-time data, and embedding models to compare chatbot responses against ground truth standards. Results: The results highlight the importance of guidelines and ground truth for improving LLM evaluation accuracy. The agentic method, dynamically accessing reliable information, demonstrated the best alignment with human assessments. Adherence to a standardized, expert-validated framework significantly enhanced chatbot response safety and reliability. Discussion: Our findings emphasize the need for comprehensive, expert-tailored safety evaluation metrics for mental health chatbots. While LLMs have significant potential, careful implementation is necessary to mitigate risks. The superior performance of the agentic approach underscores the importance of real-time data access in enhancing chatbot reliability. Conclusion: The study validated an evaluation framework for mental health chatbots, proving its effectiveness in improving safety and reliability. Future work should extend evaluations to accuracy, bias, empathy, and privacy to ensure holistic assessment and responsible integration into healthcare. Standardized evaluations will build trust among users and professionals, facilitating broader adoption and improved mental health support through technology.",8/3/24,https://openalex.org/W4402385952,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,TRUE,Text,FALSE,TRUE,Not Applicable,Not Applicable,general_psychiatry,age_unspecified,unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,,,
Healthify: A Conversational AI for Mental Health Support Using Groq and LangChain Frameworks,"Mental health issues are a growing global concern, with many individuals facing barriers to accessing timely and effective care. Recent advancements in artificial intelligence (AI) have opened new possibilities for providing continuous, personalized emotional support. This study presents Healthify, an AI-powered mental health support chatbot designed to provide empathetic, context-aware conversations for individuals seeking emotional guidance. By leveraging LangChain and the Groq API, Healthify combines state-of-the-art natural language processing (NLP) models with a robust memory system, ensuring consistent and personalized interactions tailored to the users needs. The chatbots ability to engage users in continuous dialogues aims to bridge gaps in mental health care, offering an accessible and non-judgmental platform for support. This research evaluates the effectiveness of Healthify in providing emotional support by analysing its ability to simulate empathetic conversation and address various mental health concerns. Through user interaction data and qualitative analysis, we assess the chatbots responsiveness, emotional intelligence, and potential impact on mental well- being. The findings demonstrate that AI-driven platforms can provide a valuable supplement to traditional mental health resources, particularly for individuals who may be reluctant or unable to seek professional help. We conclude that Healthify offers a promising direction for integrating AI in mental health interventions, with implications for expanding access to mental health support on a global scale.",5/2/25,https://openalex.org/W4410036624,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,TRUE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general_psychiatry,"adolescence, young_adulthood, middle_adulthood, old",unspecified,Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,
"Deploying a Mental Health Chatbot in Higher Education: The Development and Evaluation of Luna, an AI-Based Mental Health Support System","Rising mental health challenges among postsecondary students have increased the demand for scalable, ethical solutions. This paper presents the design, development, and safety evaluation of Luna, a GPT-4-based mental health chatbot. Built using a modular PHP architecture, Luna integrates multi-layered prompt engineering, safety guardrails, and referral logic. The Institutional Review Board (IRB) at the University of Detroit Mercy (Protocol #23-24-38) reviewed the proposed study and deferred full human subject approval, requesting technical validation prior to deployment. In response, we conducted a pilot test with a variety of usersincluding clinicians and students who simulated at-risk student scenarios. Results indicated that 96% of expert interactions were deemed safe, and 90.4% of prompts were considered useful. This paper describes Lunas architecture, prompt strategy, and expert feedback, concluding with recommendations for future human research trials.",6/1/25,https://openalex.org/W4411162000,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,TRUE,,TRUE,Text,FALSE,TRUE,Not Applicable,Not Applicable,general_psychiatry,"adolescence, young_adulthood",united_states,Unspecified,,,TRUE,,,,,TRUE,TRUE,,TRUE,TRUE,,,
"Deep Learning Integration and AI-Driven Support: A Comprehensive Student Platform for Emotion Detection, Psychological Assessment, and Career Guidance","This research paper introduces a meticulously crafted mental health and well-being platform tailored specifically to address the unique needs of 11th and 12th-grade students during critical academic junctures. Leveraging advanced machine learning techniques, the application employs emotion detection for insightful mood analysis, providing users with real-time insights into their emotional states to facilitate proactive mental health management. Integrating established assessment scales such as GAD7, SRQ20, and PHQ9 enhances diagnostic precision, enabling personalized stress relief activities. Seamlessly integrating psychiatric consultations through contact details, chat, and video calling capabilities ensures prompt professional support. An emotionally intelligent chatbot serves as a knowledgeable companion, offering guidance on mental health challenges and crucial academic and career decisions. Additionally, the application provides a vibrant community platform for users to engage, share insights, and access information on mental health and career guidance. In essence, this application bridges the gap between mental health support and academic pursuits, empowering students to navigate their educational journey while prioritizing their mental well-being.",5/6/24,https://openalex.org/W4396666961,OpenAlex,Deep Learning Models,Chatbot,Single Agent,Not Applicable,,TRUE,TRUE,,TRUE,"Text, Image",FALSE,TRUE,Not Applicable,Interview (Questionnaire),"6A7, 6B0",adolescence,India,Unspecified,TRUE,,TRUE,,,TRUE,,TRUE,TRUE,,,,,,
A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to Assist Adolescent Cyberbullying Education,"Cyberbullying harms teenagers' mental health, and teaching them upstanding intervention is crucial. Wizard-of-Oz studies show chatbots can scale up personalized and interactive cyberbullying education, but implementing such chatbots is a challenging and delicate task. We created a no-code chatbot design tool for K-12 teachers. Using large language models and prompt chaining, our tool allows teachers to prototype bespoke dialogue flows and chatbot utterances. In offering this tool, we explore teachers' distinctive needs when designing chatbots to assist their teaching, and how chatbot design tools might better support them. Our findings reveal that teachers welcome the tool enthusiastically. Moreover, they see themselves as playwrights guiding both the students' and the chatbot's behaviors, while allowing for some improvisation. Their goal is to enable students to rehearse both desirable and undesirable reactions to cyberbullying in a safe environment. We discuss the design opportunities LLM-Chains offer for empowering teachers and the research opportunities this work opens up.",2/27/24,https://openalex.org/W4392271267,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,FALSE,TRUE,Not Applicable,Not Applicable,6B4,adolescence,united_states,Unspecified,,,TRUE,,,TRUE,,,,,,TRUE,,,
ESHRO: An Innovative Evaluation Framework for AI-Driven Mental Health Chatbots,"Mental health is a growing concern across demographics, with one in five adults (National Institute of Mental Health, 2022) and one in seven children aged three to seventeen (Centers for Disease Control and Prevention, 2023) in the United States diagnosed with a mental health condition. Despite it being a prevalent issue, access to mental health support remains limited for many people, a challenge exacerbated by the pandemic (Lattie, 2022). In recent years, AI chatbots have emerged as a potential avenue to overcome these obstacles. With the rise of the development and use of such mental health support chatbots, it has been integral to have evaluation frameworks that ensure that these chatbots consistently provide empathetic, safe, and effective responses to the users. For this purpose, this paper introduces ESHRO, an innovative evaluation framework to analyze the LLM-generated responses on five critical metrics: Empathy, Safety, Helpfulness, Relevance, and Overall Quality. By incorporating multidimensional metrics and integrating both automated and human evaluation, ESHRO overcomes many limitations of existing frameworks. Moreover, to showcase its application, we developed ELY Chatbot, an AI-driven mental health chatbot developed to deliver emotional support and motivation. We utilized the ESHRO framework to evaluate it. The ESHRO framework demonstrates the potential to improve evaluations of mental health chatbots. The paper concludes by discussing limitations and highlighting opportunities for future research, ultimately paving the way for safer, more empathetic, and more impactful mental health solutions.",6/19/25,https://openalex.org/W4411438638,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,,,TRUE,Text,,TRUE,Not Applicable,Not Applicable,general_psychiatry,age_unspecified,unspecified,Unspecified,,,TRUE,,,,TRUE,,TRUE,TRUE,TRUE,,,,
Assessing the performance of ChatGPT in psychiatry: A study using clinical cases from foreign medical graduate examination (FMGE),"Dear Editor, ChatGPT, developed by OpenAI, is an advanced language model designed for natural language understanding and generation.[1] ChatGPT's role in healthcare involves acting as a conversational agent that can provide information, answer queries, and even assist in preliminary diagnostics. Its ability to process and generate human-like text makes it a valuable tool in the healthcare ecosystem, particularly in fields where effective communication is crucial.[2] In psychiatry, personalized care is essential for understanding the unique needs and conditions of individual patients.[3] ChatGPT can contribute to personalized care by engaging in conversations that help gather patient information, assess mental health symptoms, and provide relevant information about various psychiatric conditions.[4] This study aims to evaluate the performance of ChatGPT in psychiatry using clinical cases sourced from the Foreign Medical Graduate Examination (FMGE). ChatGPT 3.5  a freely accessible, pre-trained AI model was used for this evaluation. Ten clinical cases were selected from a freely available online source with previous year FMGE questions.[5] All of the cases were multiple-choice questions (MCQs) with four options. The psychiatric conditions covered in the study included a range of disorders to ensure a comprehensive evaluation. Each clinical case, representing a spectrum of mental health disorders such as bulimia nervosa, bipolar disorder, delusions, attention deficit hyperactivity disorder, Fregoli delusion, Othello syndrome, schizophrenia, obsessive-compulsive disorder, and depression, was presented to ChatGPT in the form of MCQs. These cases were given as inputs into ChatGPT, and the responses generated by the model were then cross-checked against the correct diagnoses provided in the FMGE online resource. All responses were generated twice to confirm the answer of ChatGPT. ChatGPT has nailed the evaluation with an overall score of 90% [Supplementary Materials 1-10]. In the majority of the cases (Cases 18 and Case 10), ChatGPT provided accurate diagnoses, aligning with the correct answers from the FMGE key. This suggests that ChatGPT has the potential to effectively analyze and comprehend various psychiatric scenarios, providing correct assessments and demonstrating its utility as a tool for preliminary diagnostics in psychiatry. In Case 9, ChatGPT diagnosed the psychiatric condition as ""delusion of reference"" instead of the correct answer, which was ""delusion of control."" The findings of this study hold significant implications for the integration of ChatGPT and similar language models in psychiatric diagnostics. While the majority of cases demonstrated ChatGPT's competence in providing accurate diagnoses, the misclassification in Case 9 underscores the need for language models to possess an enhanced nuanced understanding of psychiatric disorders, especially those with subtle distinctions. Table 1 shows the evaluation report of ChatGPT responses.Table 1: Evaluation reportThis study emphasizes the necessity for continuous refinement of language models, incorporating feedback from psychiatric professionals and integrating evolving knowledge in the field. Regular updates can address identified limitations, enhancing the model's accuracy and reliability over time. ChatGPT's demonstrated accuracy in the majority of cases suggests its potential as a valuable tool for preliminary diagnostics in psychiatry. To mitigate bias, all selected MCQs are sourced exclusively from the National Board of Examinations  FMGE test series, validated for diverse case scenarios and questioning patterns. Additionally, for this evaluation, all MCQs on multiple cases available in the test series have been included. Future assessments will broaden their scope by incorporating MCQs from all psychiatry-related test series asked in the FMGE. Future directions could explore the integration of language models like ChatGPT into clinical workflows, serving as an aid to healthcare professionals in decision-making processes. As language models become more integrated into healthcare, ethical considerations surrounding patient privacy and data security must be prioritized. To enhance ChatGPT, continued training on diverse datasets and rigorous testing with user feedback is crucial. Integration into clinical practice should involve ongoing collaboration with healthcare professionals, ensuring adherence to ethical guidelines, and addressing privacy concerns. Regular updates and refinements based on real-world usage can optimize its utility in supporting clinical tasks. Collaboration between computer scientists, mental health professionals, and policymakers is crucial. A multidisciplinary approach will contribute to the development of guidelines and standards for the ethical and effective use of language models in mental healthcare. Financial support and sponsorship Nil. Conflicts of interest There are no conflicts of interest.",4/1/24,https://openalex.org/W4395014102,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,TRUE,,,Text,,,,,general_psychiatry,age_unspecified,unspecified,Unspecified,TRUE,TRUE,TRUE,,,,,,TRUE,,,TRUE,,,
Mis-forecasting Relationship Formation with AI Agents: Implications for Moral Judgment,"People are increasingly using AI agents for companionship. What are the socio-moral consequences of these interactions, and how accurately can people predict them? Three studies (N = 895) examined (1) how close participants felt to an AI (versus human) partner after repeated, meaningful conversations; (2) whether this closeness predicted more lenient moral judgment of the AI following a transgression; and (3) how accurate third-party observers' predictions were about others' (Study 2) and their own (Study 3) closeness to and moral judgment of the AI partner in identical, but hypothetical, scenarios. Results indicate that participants felt, and foresaw feeling, equivalently closer to both human and AI partners after engaging in conversations characterized by emotional self-disclosure sustained over several sessions (the Fast Friends procedure). However, while in-lab participants felt closer to their human partner (Study 1), this human bias in closeness was not foreseen (Studies 2-3). In all cases, closeness to AI did not influence participants' moral judgments following their partners transgression, though predictions about others' (Study 2) and one's own (Study 3) moral judgments were significantly harsher than in-lab participants' reports. We situate these social and moral forecasting errors within the moral psychology literature and discuss their implications for policy, design, and the future of human-AI interaction.",7-Mar-25,https://openalex.org/W4408305272,OpenAlex,Rule-based models,Chatbot,Non-Agent,Not Applicable,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The Use of AI Chatbot Therapy as a Stress Management Intervention In Non-Clinical Populations: An Experimental Study,"BACKGROUND Mental health concerns are rising in the Philippines, yet access to psychological services remains limited due to a shortage of mental health professionals and financial barriers. AI chatbot therapy presents a potential solution by providing accessible and scalable mental health support. While previous research has explored AI chatbots in clinical populations, their effectiveness in stress management among non-clinical populations remains underexplored. OBJECTIVE This study aims to evaluate the effectiveness of AI chatbot therapy in reducing perceived stress among non-clinical populations. Specifically, it examines whether AI chatbot therapy significantly decreases stress levels compared to a control group. METHODS An experimental pretest-posttest design was utilized. A total of 100 undergraduate students from Metro Manila (aged 18-25) were randomly assigned to either the intervention group (n=50), which used AI chatbot therapy for 40 minutes to 1 hour daily over seven days, or the control group (n=50), which did not receive the intervention. Stress levels were measured using the Perceived Stress Scale (PSS) before and after the intervention. Statistical analyses, including paired and independent t-tests, were conducted to compare within-group and between-group differences. RESULTS The intervention group demonstrated a significant reduction in stress levels from pre-test (M = 31.36, SD = 4.25) to post-test (M = 20.86, SD = 5.10), with a large effect size (Cohens d = 2.43, p < .001). The control group also showed a reduction in stress (M = 32.00, SD = 4.17 to M = 24.20, SD = 8.65), but with a smaller effect size (Cohens d = 0.96, p < .001). A comparison of post-test scores between groups revealed a statistically significant difference (p = .021), favoring the intervention group. CONCLUSIONS The findings suggest that AI chatbot therapy is an effective stress management tool for non-clinical populations, demonstrating significant reductions in perceived stress. Given the accessibility and scalability of AI chatbots, they hold promise as a supplementary mental health intervention, particularly in resource-limited settings. Further research should explore long-term effects, usability across diverse populations, and integration with traditional mental health care.",8-Feb-25,https://openalex.org/works/w4407287594,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,,,,,Text,,,,Interview (Questionnaire),6B4,"adolescence, young_adulthood",philippines,Unspecified,,,TRUE,,,,,,TRUE,,,TRUE,,,
TherapyBot: a chatbot for mental well-being using transformers,"The field of natural language processing (NLP) and conversational artificial intelligence (AI) has one ingenious application in the psychological space. Depression and anxiety are two major issues that the world is facing, with close to 41% of adults reporting these symptoms in the United States alone, as of December 2020. It has also been observed that most of the people are not open about it. As a result, it is critical to address this issue on a global scale. Developed countries reportedly have 9 psychiatrists per 100,000 people. One way to mitigate this is the use of chatbots. We propose a transformer-based methodology to build a therapy bot that has been trained on a combination of open-domain conversations from a publicly available dataset and therapist-client conversations from a self-constructed dataset. This end-to-end data-driven model shows quality performance in conversations and adds value by aiding in the case of mental health issues. The proposed architecture is proven to be effective in its usability in the psychological space for both single-turn and multi-turn dialogue. The performance of the proposed system shows loss is 0.29 and perplexity is 1.34, both metrics keeps gradually decreasing and it means an improvement in performance of chatbots system.",1/31/24,https://openalex.org/W4391417799,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,,,,TRUE,Text,TRUE,,,,6A7,unspecified,unspecified,Unspecified,TRUE,,,,TRUE,,,,TRUE,,TRUE,,,,
Large language models outperform mental and medical health care professionals in identifying obsessive-compulsive disorder,"Despite the promising capacity of large language model (LLM)-powered chatbots to diagnose diseases, they have not been tested for obsessive-compulsive disorder (OCD). We assessed the diagnostic accuracy of LLMs in OCD using vignettes and found that LLMs outperformed medical and mental health professionals. This highlights the potential benefit of LLMs in assisting in the timely and accurate diagnosis of OCD, which usually entails a long delay in diagnosis and treatment.",7/19/24,https://openalex.org/W4400837480,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,,TRUE,,TRUE,Text,,TRUE,,Interview (Questionnaire),6B2,"young_adulthood, middle_adulthood",unspecified,Both,TRUE,TRUE,,,,,,TRUE,,,TRUE,,,,
"MOODIFY: Tailored, Personal and Multifaceted AI Assistant for Young Adult Mental Health Issues","Mental health issues are prevalent among young adults aged 20-35, who face stresses related to studies, work, finances, and personal relationships. We present Moodify, an Android application to identify, analyze, and alleviate mental health problems at an early stage. The app has three key features: 1) An emotion detection module that divides the face into lines to extract and predict emotions, then recommends uplifting songs and podcasts based on the user's current state. 2) An autonomous chatbot for users to privately discuss their feelings and receive suggested coping strategies without revealing their identity. 3) Anonymous group chat rooms where users can socialize and discuss common problems anonymously. Moodify aims to improve mental wellbeing by allowing users to monitor their emotional state through facial analysis, vent feelings safely to an AI, and connect with peers undergoing similar issues while maintaining privacy. Evaluations demonstrate the emotion detection module can identify facial expressions indicative of mood with 82% accuracy. The chatbot is effectively being designed for reducing negative sentiment through supportive conversations. Moodify represents a novel mobile approach to enhancing mental health through emotionally intelligent systems and secure social features tailored to young adults' needs. The app empowers users to privately manage mental health issues by increasing self-awareness, providing personalized support, and facilitating anonymous peer discussions.",12/8/23,https://openalex.org/W4392408883,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,"Text, Image",,,,Interview (Questionnaire),general_psychiatry,age_unspecified,India,Unspecified,TRUE,,TRUE,,,TRUE,,TRUE,TRUE,TRUE,,TRUE,,,
AI Chatbots for Mental Health Self-Management: A Lived ExperienceCentered Qualitative Study on Values and Harms,"<sec> <title>BACKGROUND</title> Large language models (LLMs) now enable chatbots to engage in sensitive mental health conversations, including depression self-management. Yet their rapid deployment often overlooks how well these tools align with the values of people with lived experiences. Without such alignment, chatbots risk causing harm through misinformation, lack of empathy, or inadequate crisis support. </sec> <sec> <title>OBJECTIVE</title> This study aims to explore how the values of individuals with lived experiences of depression relate to the potential harms and design implications of LLM-based mental health chatbots. </sec> <sec> <title>METHODS</title> We developed a technology probea GPT-4obased chatbot named Zennydesigned to simulate depression self-management scenarios grounded in prior research. We then conducted interviews with 17 individuals with lived experiences of depression, who interacted with Zenny during the session. Thematic analysis was applied to the interview data to identify key concerns and value-driven insights. </sec> <sec> <title>RESULTS</title> Our analysis identified five core values that participants prioritized when engaging with the chatbot: informational support, emotional support, personalization, privacy, and crisis management. These values shaped how participants perceived both the benefits and limitations of using LLMs for mental health support. </sec> <sec> <title>CONCLUSIONS</title> This study highlights the importance of aligning AI chatbot design with the values of people with lived experiences to mitigate potential harms. We offer design recommendations that aim to enhance the safety and usefulness of LLM-based tools for depression self-management. </sec>",29-May-25,https://openalex.org/W4411030366,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,,TRUE,,,general psychiatry,"adolescence, young_adulthood, middle_adulthood, old",united_states,Both,,,TRUE,,,TRUE,,,,,,TRUE,,,
AI-Counsellor Using Emerging Technique,"Mental health is an integral part of overall well-being, encompassing emotional, psychological, and social well-being. It affects how we think, feel, and act. Yet, mental health concerns are on the rise globally. According to the World Health Organization (WHO), an estimated one in four people will experience a mental health condition in their lifetime. Depression, anxiety, and stress are amongst the most prevalent mental health issues. This paper aims to develop an AI Counsellor - a chatbot designed to use AI and ML capabilities to offer initial mental health support. Design the chatbot to understand user input effectively through sentiment analysis, intent recognition, and entity recognition. Finally develop a user-friendly interface for text-based interaction with the AI-Counsellor.",6/24/24,https://openalex.org/W4400018739,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,TRUE,,TRUE,Text,TRUE,,EHRs,Interview (Questionnaire),general_psychiatry,age_unspecified,unspecified,Unspecified,TRUE,,TRUE,,TRUE,TRUE,,,TRUE,,,,,,
Development and Evaluation of an AI-Based Chatbot for Preventing Social Media Addiction: A Waterfall Model Approach,"The rapid growth of social media has transformed interaction and communication patterns, but it has also led to the rise of social media addiction, particularly among teenagers and young adults. This addiction, marked by compulsive usage and negative impacts on mental health and daily life, necessitates effective interventions. This research explores the development and evaluation of an AI-based chatbot designed to mitigate social media addiction by employing cognitive and behavioural strategies. The study utilizes the Waterfall modela structured, sequential approachin the chatbots development, encompassing stages from needs analysis to maintenance. The chatbots effectiveness was assessed through rigorous testing and user feedback. The methodology included problem analysis, system design, implementation, testing, and iterative improvements. A comprehensive needs analysis identified the psychological and behavioural factors contributing to social media addiction, leading to the design of a prototype chatbot integrated with AI for dynamic content adaptation and real-time feedback. The implementation phase focused on coding and system integration, followed by rigorous testing using Black Box Testing and the System Usability Scale (SUS) to ensure functionality and user-friendliness. Results indicate that the chatbot significantly reduced social media addiction scores, with a mean decrease from 55.21 to 50.17, supported by a highly significant p-value of &lt;0.0001. User satisfaction was high, particularly regarding ease of use and information quality. However, user engagement declined over time, highlighting the need for ongoing content updates and feature enhancements. This study contributes to the field by providing insights into the application of the Waterfall model in AI chatbot development and offers a scalable solution for addressing social media addiction, with implications for future digital interventions in mental health.",10/7/24,https://openalex.org/works/w4414340090,OpenAlex,Rule-based models,Chatbot,Single Agent,Not Applicable,,TRUE,,,,Text,,,,,6C5,unspecified,unspecified,Unspecified,,,TRUE,,,TRUE,,,TRUE,,,TRUE,,,
Exploring Large-Scale Language Models to Evaluate EEG-Based Multimodal Data for Mental Health,"Integrating physiological signals such as electroencephalogram (EEG), with other data such as interview audio, may offer valuable multimodal insights into psychological states or neurological disorders.Recent advancements with Large Language Models (LLMs) position them as prospective ""health agents"" for mental health assessment.However, current research predominantly focus on single data modalities, presenting an opportunity to advance understanding through multimodal data.Our study aims to advance this approach by investigating multimodal data using LLMs for mental health assessment, specifically through zero-shot and few-shot prompting.Three datasets are adopted for depression and emotion classifications incorporating EEG, facial expressions, and audio (text).The results indicate that multimodal information confers substantial advantages over single modality approaches in mental health assessment.Notably, integrating EEG alongside commonly used LLM modalities such as audio and images demonstrates promising potential.Moreover, our findings reveal that 1-shot learning offers greater benefits compared to zero-shot learning methods.",2024-08-14T06:14:02Z,https://openalex.org/W4402427685,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,,TRUE,,,"Text, Audio, Image",TRUE,TRUE,Lab Test,,6A7,unspecified,unspecified,Unspecified,TRUE,,,,,,TRUE,TRUE,TRUE,,,,,,
Real-Time Mental Health Support with AI and ML,"Mental health issues such as stress, anxiety, and depression are increasing worldwide, yet access to timely professional mental health support remains limited due to stigma, cost, and geographic constraints. This paper presents an AI-powered mental health chatbot that utilizes Natural Language Processing (NLP) and sentiment analysis to provide real-time emotional support, personalized coping strategies, and mental health resources. The chatbot ensures accessibility, scalability, and confidentiality, helping bridge the gap in mental health services by offering immediate assistance anytime, anywhere.",3/31/25,https://openalex.org/W4409063179,OpenAlex,Small pre-trained model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,,TRUE,,,general_psychiatry,unspecified,unspecified,Unspecified,,,TRUE,,,,,TRUE,TRUE,,,TRUE,,,
Manasvita : AI-Powered Multimodal Mental Wellness Platform,"Abstract: Mental health issues such as stress, anxiety, and depression have become increasingly prevalent in todays fast-paced world. Early detection and timely intervention can significantly improve mental well-being. This project presents a Manasvita : AI-Powered Multimodal Mental Wellness Platformintegrating advanced machine learning techniques and AI-driven solutions to assist users in understanding and managing their mental health. The system incorporates user authentication, enabling secure access to personalized assessments and services. A doctor appointment module allows users to schedule consultations with mental health professionals. The platform utilizes Convolutional Neural Networks (CNNs) to analyze facial expressions and predict mental health conditions based on the FER2013 dataset. Additionally, a Random Forest classifier assesses stress levels using a structured dataset. An AI-powered chatbot, leveraging the Gemini AI API, provides users with immediate mental health-related support and guidance. To encourage positive behavioral changes, the platform includes a Task and Reward system, where doctors assign therapeutic tasks, and users earn incentives such as discounts or coupons upon completion. By integrating machine learning, artificial intelligence, and user engagement strategies, this project aims to provide an accessible, technology-driven solution for mental health monitoring and support. The proposed system enhances self-awareness, promotes timely intervention, and bridges the gap between users and professional healthcare services",5/29/25,https://openalex.org/W4410855717,OpenAlex,Deep Learning Models,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,"Text, Image",,TRUE,,Interview (Questionnaire),general_psychiatry,age_unspecified,unspecified,Unspecified,TRUE,TRUE,TRUE,,,,,TRUE,TRUE,,,,,,
"The role of AI Chatbots in providing mental health support: emotional perception, human-AI interaction, and ethical implications","This study investigates the role of AI chatbots in providing mental health support, focusing on emotional perception, human-AI interaction, and ethical implications. The research aimed to evaluate the effectiveness of AI chatbots in reducing symptoms of depression and anxiety, enhance emotional intelligence, and explore ethical concerns related to data security and emotional dependency. A mixed-methods approach was employed, combining quantitative measures (PHQ-9 for depression, GAD-7 for anxiety, and emotional intelligence assessments) with qualitative data from participant interviews. Results show a significant reduction in depression and anxiety symptoms after a 4-week AI chatbot intervention",4/20/25,https://openalex.org/W4409646228,OpenAlex,Rule-based models,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,TRUE,,,,"6A7, 6B0","adolescence, young_adulthood",unspecified,Unspecified,,,TRUE,,,TRUE,,,,,,TRUE,,,
"Talk, Listen, Connect: Navigating Empathy in Human-AI Interactions","Social interactions promote well-being, yet challenges like geographic distance and mental health conditions can limit in-person engagement. Advances in AI agents are transferring communication, particularly in mental health, where AI chatbots provide accessible, non-judgmental support. However, a key challenge is how effectively these systems can express empathy, which is crucial in human-centered design. Current research highlights a gap in understanding how AI can authentically convey empathy, particularly as issues like anxiety, depression, and loneliness increase. Our research focuses on this gap by comparing empathy expression in human-human versus human-AI interactions. Using personal narratives and statistical analysis, we examine empathy levels elicited by humans and AI, including GPT-4o and fine-tuned versions of the model. This work aims to enhance the authenticity of AI-driven empathy, contributing to the future design of more reliable and effective mental health support systems that foster meaningful social interactions.",9/23/24,https://openalex.org/W4403786359,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,TRUE,TRUE,,,general_psychiatry,"adolescence, young_adulthood",united_states,Unspecified,,,,,,,,TRUE,TRUE,,,TRUE,,,
Social Media App with time Capsule Messaging & Chatbot,"Abstract- This article presents a novel social networking site, integrating time capsule messaging with an artificial intelligence-driven chatbot to provide a more thoughtful and interactive online experience. Users may write messages, share images, or record videos, scheduling them to be received on a designated date in the future, promoting purposeful communication and longer-term personal narrative. In addition to this, a smart chatbot helps users by taking care of their time capsules and providing conversational assistance, making the experience more interactive and emotionally engaging. By combining delayed messaging with real-time AI interaction, the platform enables deeper digital connections and assists users in saving memories in a meaningful manner. Early user feedback points towards its potential for improving emotional well-being, aiding mental health, and creating lasting digital legacies. This article discusses the system's design, implementation, and general implications for future social technologies. Key words: social media, time capsule messaging, AI chatbot, digital memory, emotional well-being, delayed communication, user experience",6-May-25,https://openalex.org/W4410088612,OpenAlex,Small pre-trained model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,"Text, Video, Image",TRUE,,,,general_psychiatry,age_unspecified,unspecified,Unspecified,,,TRUE,,,TRUE,,,TRUE,,TRUE,,,,
A Novel Cognitive Behavioral TherapyBased Generative AI Tool,"<sec> <title>BACKGROUND</title> Digital mental health tools, designed to augment traditional mental health treatments, are becoming increasingly important due to a wide range of barriers to accessing mental health care, including a growing shortage of clinicians. Most existing tools use rule-based algorithms, often leading to interactions that feel unnatural compared with human therapists. Large language models (LLMs) offer a solution for the development of more natural, engaging digital tools. In this paper, we detail the development of Socrates 2.0, which was designed to engage users in Socratic dialogue surrounding unrealistic or unhelpful beliefs, a core technique in cognitive behavioral therapies. The multiagent LLM-based tool features an artificial intelligence (AI) therapist, Socrates, which receives automated feedback from an AI supervisor and an AI rater. The combination of multiple agents appeared to help address common LLM issues such as looping, and it improved the overall dialogue experience. Initial user feedback from individuals with lived experiences of mental health problems as well as cognitive behavioral therapists has been positive. Moreover, tests in approximately 500 scenarios showed that Socrates 2.0 engaged in harmful responses in under 1% of cases, with the AI supervisor promptly correcting the dialogue each time. However, formal feasibility studies with potential end users are needed. </sec> <sec> <title>OBJECTIVE</title> This mixed methods study examines the feasibility of Socrates 2.0. </sec> <sec> <title>METHODS</title> On the basis of the initial data, we devised a formal feasibility study of Socrates 2.0 to gather qualitative and quantitative data about users and clinicians experience of interacting with the tool. Using a mixed method approach, the goal is to gather feasibility and acceptability data from 100 users and 50 clinicians to inform the eventual implementation of generative AI tools, such as Socrates 2.0, in mental health treatment. We designed this study to better understand how users and clinicians interact with the tool, including the frequency, length, and time of interactions, users satisfaction with the tool overall, quality of each dialogue and individual responses, as well as ways in which the tool should be improved before it is used in efficacy trials. Descriptive and inferential analyses will be performed on data from validated usability measures. Thematic analysis will be performed on the qualitative data. </sec> <sec> <title>RESULTS</title> Recruitment will begin in February 2024 and is expected to conclude by February 2025. As of September 25, 2024, overall, 55 participants have been recruited. </sec> <sec> <title>CONCLUSIONS</title> The development of Socrates 2.0 and the outlined feasibility study are important first steps in applying generative AI to mental health treatment delivery and lay the foundation for formal feasibility studies. </sec> <sec> <title>INTERNATIONAL REGISTERED REPORT</title> DERR1-10.2196/58195 </sec>",8-Mar-24,https://openalex.org/W4392656842,OpenAlex,Foundation model,Chatbot,Multi-Agent,Hierarchical Architecture,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,general_psychiatry,"adolescence, young_adulthood, middle_adulthood, old",unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,,,
Performance comparison ensemble classifiers performance,"In todays era of digital healthcare transformation, there is a growing demand for swift responses to mental health queries. To meet this need, we introduce an AI-driven chatbot system designed to automatically address frequently asked questions in psychology. Leveraging a range of classifiers including Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Nave Bayes, our system extracts insights from expert data sources and employs natural language processing techniques like LDA Topic Modeling and Cosine similarity to generate contextually relevant responses. Through rigorous experimentation, we find that SVM surpasses Nave Bayes and KNN in accuracy, precision, recall, and F1-score, making it our top choice for constructing the final response system. This research underscores the effectiveness of ensemble classifiers, particularly SVM, in providing accurate and valuable information to enhance mental health support in response to common psychological inquiries.",5/3/24,https://openalex.org/W4393097653,OpenAlex,Machine Learning Models,Chatbot,Non-Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,general_psychiatry,age_unspecified,unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,
Design and Evaluation of an AI-Powered Conversational Agent,"This research study describes the design and evaluation of an interactive MindBot to help a person deal with depressive Sentiments and to support individuals experiencing depressive symptoms by providing timely, interactive, and empathetic engagement. With the growing trend of mental health issues and a lack of adequate resources to cope, the need for innovative solutions is of great importance. In this MindBot, users will be connected to each other to communicate their Sentiments and seek support. The integration of NLP and LLMs by the chatbot fosters meaningful conversations and ensures timely support to users. Computational architecture optimizes response generation to control the processing cost related to LLMs while striking a balance between conversational richness and efficiency. This method allows for real-time communication without sacrificing the caliber of involvement. The chatbot's integration of NLP and LLMs promotes meaningful discussions and guarantees that consumers receive prompt service. The utilization of predefined response templates combined with dynamic responses generated by GPT -3.5-turbo further improved the implementation's performance in conversational capability. The paper covered the system architecture, methodology, and results achieved, with an emphasis on the contribution of sentiment analysis and the contributions of the chatbot towards mental health support. User happiness, emotional correctness, referral appropriateness, and the model's capacity to sustain coherent conversation even in the face of small user input errors are all used to gauge how effective the model is. The results indicate that the effectiveness of mental health interventions and user engagement are increased when sentiment analysis and sophisticated language models are combined. It was demonstrated that the impact of integrating advanced language models along with sentiment analysis can have a huge bearing on enhancing the user engagement factor as well as the ef...",10-Feb-25,https://openalex.org/W4407317165,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,TRUE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,6A7,age_unspecified,unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,
Artificial Intelligence Enabled Mobile Chatbot,"In recent years, the demand for mental health services has increased exponentially, prompting the need for accessible, cost-effective, and efficient solutions. This paper introduces an Artificial Intelligence (AI) enabled mobile chatbot psychologist that leverages AIML (Artificial Intelligence Markup Language) and Cognitive Behavioral Therapy (CBT) to provide psychological support. The chatbot is designed to facilitate mental health care by offering personalized CBT interventions to individuals experiencing psychological distress. The proposed mobile chatbot psychologist employs AIML, a language created to facilitate human-computer interactions, to understand user inputs and generate contextually appropriate responses. To ensure the efficacy of the chatbot, it is equipped with a knowledge base comprising CBT principles and techniques, enabling it to provide targeted psychological interventions. The integration of CBT allows the chatbot to address a wide range of mental health issues, including anxiety, depression, stress, and phobias, by helping users identify and challenge cognitive distortions. The paper discusses the development and implementation of the mobile chatbot psychologist, detailing the AIML-based conversational engine and the incorporation of CBT techniques. The chatbot's effectiveness is evaluated through a series of user studies involving participants with varying levels of psychological distress. Results demonstrate the chatbot's ability to deliver personalized interventions, with users reporting significant improvements in their mental well-being. The AI-enabled mobile chatbot psychologist offers a promising solution to bridge the gap in mental health care, providing an easily accessible, cost-effective, and scalable platform for psychological support. This innovative approach can serve as a valuable adjunct to traditional therapy and help reduce the burden on mental health professionals, while empowering individuals to take charge of their mental well-being.",15-Jul-05,https://openalex.org/W4383426667,OpenAlex,Rule-based models,Chatbot,Non-Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Systematic reviews  / Meta analysis,general_psychiatry,age_unspecified,unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,,,
Development and Evaluation of a Mental Health Chatbot Using ChatGPT 4.0: Pilot Study of Dr. CareSam with Korean Users (Preprint),"Background Mental health chatbots have emerged as a promising tool for providing accessible and convenient support to individuals in need. Building on our previous research on digital interventions for loneliness and depression among Korean college students, this study addresses the limitations identified and explores more advanced artificial intelligencedriven solutions. Objective This study aimed to develop and evaluate the performance of HoMemeTown Dr. CareSam, an advanced cross-lingual chatbot using ChatGPT 4.0 (OpenAI) to provide seamless support in both English and Korean contexts. The chatbot was designed to address the need for more personalized and culturally sensitive mental health support identified in our previous work while providing an accessible and user-friendly interface for Korean young adults. Methods We conducted a mixed methods pilot study with 20 Korean young adults aged 18 to 27 (mean 23.3, SD 1.96) years. The HoMemeTown Dr CareSam chatbot was developed using the GPT application programming interface, incorporating features such as a gratitude journal and risk detection. User satisfaction and chatbot performance were evaluated using quantitative surveys and qualitative feedback, with triangulation used to ensure the validity and robustness of findings through cross-verification of data sources. Comparative analyses were conducted with other large language models chatbots and existing digital therapy tools (Woebot [Woebot Health Inc] and Happify [Twill Inc]). Results Users generally expressed positive views towards the chatbot, with positivity and support receiving the highest score on a 10-point scale (mean 9.0, SD 1.2), followed by empathy (mean 8.7, SD 1.6) and active listening (mean 8.0, SD 1.8). However, areas for improvement were noted in professionalism (mean 7.0, SD 2.0), complexity of content (mean 7.4, SD 2.0), and personalization (mean 7.4, SD 2.4). The chatbot demonstrated statistically significant performance differences compared with other large language models chatbots (F=3.27; P=.047), with more pronounced differences compared with Woebot and Happify (F=12.94; P&lt;.001). Qualitative feedback highlighted the chatbots strengths in providing empathetic responses and a user-friendly interface, while areas for improvement included response speed and the naturalness of Korean language responses. Conclusions The HoMemeTown Dr CareSam chatbot shows potential as a cross-lingual mental health support tool, achieving high user satisfaction and demonstrating comparative advantages over existing digital interventions. However, the studys limited sample size and short-term nature necessitate further research. Future studies should include larger-scale clinical trials, enhanced risk detection features, and integration with existing health care systems to fully realize its potential in supporting mental well-being across different linguistic and cultural contexts.",11/16/24,https://openalex.org/W4404433953,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,TRUE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Interview (Questionnaire),"6A7, 6B0","adolescence, young_adulthood",Korea,Both,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,11/16/24
Effectiveness of Topic-Based Chatbots on Mental Health Self-Care,"<sec> <title>BACKGROUND</title> The global surge in mental health challenges has placed unprecedented strain on healthcare systems, highlighting the need for scalable interventions to promote mental health self-care. Chatbots have emerged as promising tools to address these gaps by providing accessible, evidence-based support. However, most existing studies focus on the clinical population and symptom reduction, with limited exploration of preventive strategies targeting self-care and mental health literacy among people in the community. </sec> <sec> <title>OBJECTIVE</title> This study evaluates the effectiveness of a rule-based, topic-specific chatbot intervention in improving self-care efficacy, literacy, self-care intention and self-care behaviors as well as mental well-being immediately after 10 days of usage and after 1 month. </sec> <sec> <title>METHODS</title> A two-arm, assessor-blinded randomized controlled trial was conducted. 285 participants were randomly assigned to either the chatbot intervention group (n=140) or a waitlist control group (n=145). The chatbot intervention consisted of ten topic-specific sessions targeting stress management, emotion regulation, and value clarification, delivered over 10 days with a 7-day free-access period. Primary outcomes included self-care self-efficacy, behavioral intentions, self-care behaviors, and mental health literacy. Secondary outcomes included depressive symptoms, anxiety symptoms, and mental well-being. Assessments were conducted at baseline, post-intervention at 10 days, and 1-month follow-up. Primary data analysis was performed using linear mixed models with intention-to-treat approach. </sec> <sec> <title>RESULTS</title> Participants in the chatbot group demonstrated greater improvements in behavioral intentions (d=0.31) and mental health literacy (d=0.45) compared to the control group as revealed by the significant interaction effects (mental health literacy: F(2, 423.57)=4.27, P=.015; behavioral intentions: F(2, 379.74)=15.02, P&lt;.001). The chatbots were also able to bring immediate improvement on self-care behaviors (d=0.36), mindfulness (d=0.37), mental health outcomes of depressive symptoms (d=-0.26) and overall well-being (d=0.22) and positive emotions (d=0.28). Yet, these improvements did not differ significantly at 1 month when compared to the waitlist control. Adherence was higher among participants who received push notifications. </sec> <sec> <title>CONCLUSIONS</title> This study highlights the potential of rule-based chatbots in promoting mental health literacy and fostering the intention to take care of own mental health. Despite recent advancements in generative AI, rule-based systems remain valuable due to their content control and safety assurances. However, improvements in chatbot design, including greater personalization and interactive features, may be necessary to enhance self-efficacy and long-term mental health outcomes. These findings underscore the utility of chatbots as scalable interventions to bridge gaps in mental health service delivery. Future research should explore hybrid approaches that combine rule-based and generative AI systems to optimize intervention effectiveness. </sec> <sec> <title>CLINICALTRIAL</title> ClinicalTrials.gov NCT05694507 </sec>",27-Mar-25,https://openalex.org/W4405677212,OpenAlex,Rule-based models,Chatbot,Non-Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Not Applicable,general_psychiatry,age_unspecified,unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,
Apprentice bot model design and implementation for psychological clients therapy,"Abstract Conversational agents play a crucial role in advising clients and providing treatment for mental health issues. In Ethiopia, a developing country with a high prevalence of mental health issues, an AI text-based chatbot has been developed to assist users in addressing these cases. The bot recognizes features such as general knowledge queries, potential mental disorders, auto-generated advice, and client stories. The syntactic and semantic structure of data and user chat's memory network were investigated. Four neural networks (LSTM, single GRU, transposed GRU and double GRU) were experimented with to find the best-fitting deep learning model. The approach uses Amharic word2vec word embedding techniques (skip gram and CBOW) for semantic extraction of Amharic words and a seq2seq model for response generation. An ensemble architecture of generative-based and retrieval-based approaches was employed to handle user dialogue. The scruffy technique was used to generate word embedding, which was then used to extract semantically comparable terms. The model also embedded custom rules for smoothness and error handling. The performance and outcomes of the chatbot were evaluated using cross-validation, data organization, perplexity, accuracy measurement, f-1 score (precision and recall), and human-evaluation methods. The model's accuracy was 79.62%, with a 79.62% likelihood of delivering relevant responses. The results show that the ensemble architecture based on seq2seq modeling with embedded custom rules and Amharic word vector implementation provides pertinent responses for user utterances.",24-Feb-25,https://openalex.org/W4407870757,OpenAlex,Deep Learning Models,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,EHRs,Interview (Questionnaire),general_psychiatry,age_unspecified,ethiopia,Unspecified,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,,,
Building Emotional Support Chatbots in the Era of LLMs,"The integration of emotional support into various conversational scenarios presents profound societal benefits, such as social interactions, mental health counseling, and customer service. However, there are unsolved challenges that hinder real-world applications in this field, including limited data availability and the absence of well-accepted model training paradigms. This work endeavors to navigate these challenges by harnessing the capabilities of Large Language Models (LLMs). We introduce an innovative methodology that synthesizes human insights with the computational prowess of LLMs to curate an extensive emotional support dialogue dataset. Our approach is initiated with a meticulously designed set of dialogues spanning diverse scenarios as generative seeds. By utilizing the in-context learning potential of ChatGPT, we recursively generate an ExTensible Emotional Support dialogue dataset, named ExTES. Following this, we deploy advanced tuning techniques on the LLaMA model, examining the impact of diverse training strategies, ultimately yielding an LLM meticulously optimized for emotional support interactions. An exhaustive assessment of the resultant model showcases its proficiency in offering emotional support, marking a pivotal step in the realm of emotional support bots and paving the way for subsequent research and implementations.",17-Aug-23,https://openalex.org/W4386114394,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,general_psychiatry,age_unspecified,unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,,,
Development of A Comprehensive Evaluation Scale for LLM-Powered Counseling Chatbots (CES-LCC) Using the eDelphi Method,"Background/Objectives: With advancements in Large Language Models (LLMs), counseling chatbots are becoming vital tools for delivering scalable and accessible mental health support. Traditional evaluation scales, however, fail to adequately capture the sophisticated capabilities of these systems, such as personalized interactions, empathetic responses, and memory retention. This study aims to design a robust and comprehensive evaluation scale, the Comprehensive Evaluation Scale for LLM-Powered Counseling Chatbots (CES-LCC), using the eDelphi method to address this gap. Methods: A panel of 16 experts in psychology, artificial intelligence, human-computer interaction, and digital therapeutics participated in two iterative eDelphi rounds. The process focused on refining dimensions and items based on qualitative and quantitative feedback. Initial validation, conducted after assembling the final version of the scale, involved 49 participants using the CES-LCC to evaluate an LLM-powered chatbot delivering Self-Help Plus (SH+), an Acceptance and Commitment Therapy-based intervention for stress management.Results: The final version of the CES-LCC features 27 items grouped into nine dimensions: Understanding Requests, Providing Helpful Information, Clarity and Relevance of Responses, Language Quality, Trust, Emotional Support, Guidance and Direction, Memory, and Overall Satisfaction. Initial real-world validation revealed high internal consistency (Cronbach&amp;rsquo;s alpha = 0.94), although minor adjustments are required for specific dimensions, such as Clarity and Relevance of Responses.",1/22/25,https://openalex.org/W4406744646,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,"6A7, 6B4","adolescence, young_adulthood, middle_adulthood",Italy,Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,,,1/22/25
CAMI: A Counselor Agent Supporting Motivational Interviewing through State Inference,"Conversational counselor agents have become essential tools for addressing the rising demand for scalable and accessible mental health support. This paper introduces CAMI, a novel automated counselor agent grounded in Motivational Interviewing (MI) -- a client-centered counseling approach designed to address ambivalence and facilitate behavior change. CAMI employs a novel STAR framework, consisting of client's state inference, motivation topic exploration, and response generation modules, leveraging large language models (LLMs). These components work together to evoke change talk, aligning with MI principles and improving counseling outcomes for clients from diverse backgrounds. We evaluate CAMI's performance through both automated and manual evaluations, utilizing simulated clients to assess MI skill competency, client's state inference accuracy, topic exploration proficiency, and overall counseling success. Results show that CAMI not only outperforms several state-of-the-art methods but also shows more realistic counselor-like behavior. Additionally, our ablation study underscores the critical roles of state inference and topic exploration in achieving this performance.",5-Feb-25,https://openalex.org/W4407212402,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,EHRs,Systematic reviews  / Meta analysis,general_psychiatry,age_unspecified,unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,,
"AI-based chatbot micro-intervention for parents: Meaningful engagement, learning, and efficacy","Mental health issues have been on the rise among children and adolescents, and digital parenting programs have shown promising outcomes. However, there is limited research on the potential efficacy of utilizing chatbots to promote parental skills. This study aimed to understand whether parents learn from a parenting chatbot micro intervention, to assess the overall efficacy of the intervention, and to explore the user characteristics of the participants, including parental busyness, assumptions about parenting, and qualitative engagement with the chatbot.A sample of 170 parents with at least one child between 2-11 years old were recruited. A randomized control trial was conducted. Participants in the experimental group accessed a 15-min intervention that taught how to utilize positive attention and praise to promote positive behaviors in their children, while the control group remained on a waiting list.Results showed that participants engaged with a brief AI-based chatbot intervention and were able to learn effective praising skills. Although scores moved in the expected direction, there were no significant differences by condition in the praising knowledge reported by parents, perceived changes in disruptive behaviors, or parenting self-efficacy, from pre-intervention to 24-hour follow-up.The results provided insight to understand how parents engaged with the chatbot and suggests that, in general, brief, self-guided, digital interventions can promote learning in parents. It is possible that a higher dose of intervention may be needed to obtain a therapeutic change in parents. Further research implications on chatbots for parenting skills are discussed.",1/20/23,https://openalex.org/W4317519912,OpenAlex,Rule-based models,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Random Controlled Trial (RCT),general_psychiatry,"young_adulthood, middle_adulthood",Argentina,Both,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,1/20/23
EEG Signals Based Emotion Prediction,"Emotion recognition using Electroencephalogram (EEG) signals has become an essential tool in mental health monitoring, human-computer interaction, and affective computing. EEG provides a non-invasive method for capturing electrical activity in the brain with high temporal resolution, making it highly suitable for real-time emotion analysis. Traditional emotion recognition methods relying on facial expressions or voice can be biased or manipulated, whereas EEG-based analysis offers a more objective and direct understanding of emotional states. Existing systems typically implement EEG-based emotion classification pipelines used Multi-Scale Principal Component Analysis (MSPCA) for denoising. Feature extraction methods like Second-Order Difference Plot (SODP) and Summation of Distance to Coordinate (SDC) are commonly used, followed by spatial transformations like Equidistant Azimuthal Projection (EAP). Advanced models integrate Convolutional Block Attention Module (CBAM) and Generative Adversarial Networks (GANs) for refinement and data augmentation, respectively. This system, often limited by fixed emotion categories, insufficient temporal modelling, and lack of real-time interaction. To overcome these limitations, the proposed system introduces an advanced end-to-end EEG-based emotion recognition framework. It enhances preprocessing using MSPCA and performs Welchs Power Spectral Density (PSD) estimation and Differential Entropy (DE) calculation across five EEG bands. A Linear Dynamic System (DE_LDS) with Kalman filtering effectively models temporal feature dynamics. The system integrates GANs for data augmentation and utilizes a Bi-Directional Long Short-Term Memory (Bi-LSTM) network for capturing complex temporal dependencies in EEG signals. The model is further integrated to a Flask-based AI therapeutic chatbot, which receives real-time emotion predictions via an external API and uses win32com.client for speech synthesis, enabling empathetic, voice-enabled interactions.",6/11/25,https://openalex.org/W4411215856,OpenAlex,Deep Learning Models,Chatbot,Non-Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,"Text, Audio, Image",FALSE,FALSE,Lab Test,Not Applicable,general_psychiatry,"adolescence, young_adulthood",China,Both,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,
Design and Implementation of an AI-Driven Mental Health Chatbot,"This paper describes the development, design, and maturation of an AI-based mental health chatbot that allows a user to engage with more accessible, higher forms of mental health assistance. As developed as a retrieval-based system which leveraged predefined responses contained in a database and based upon input received from a user, it matured into a far more scalable system which included the generative AI capabilities that allowed users to achieve far more personalized, contextually relevant interactions. This deployment variant robustly leverages on the chaining framework provided by LangChain to enable chaining of prompt templates, utilizes the OllamaLLM model that uses the Llama3 language model, and fully exploits other NLP methods. Therefore, the rule-based system is migrated to a generative model that integrates both algorithms of machine learning for effective enhancement of both understanding and generating. The primary issues with this development were about the conversational context, empathetic and coherent response, and the scalability of the system. All these were achieved through advanced prompt engineering, a set of rigorous evaluation metrics like BLEU score, accuracy, precision, recall, and F1 score besides continuous user feedback for refining the performance of this model. This paper discusses methodologies to overcome the challenges and implications for the future of AI and mental health care with an emphasis on embedding technological advancements within empathetic human-computer interaction. The findings point out this chatbot is effective in providing emotional support and how AI is changing its role to improve the services of mental health.",12-Mar-25,https://openalex.org/W4408358782,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general_psychiatry,age_unspecified,unspecified,Unspecified,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,,,
Mind-Balance: AI-Powered Mental Health Assistant,"This document presents the development of an AI driven chatbot designed to provide immediate mental health sup port through natural language processing. The chatbot demon strated effective user engagement and support, indicating its potential to enhance accessibility and provide valuable assistance for mental health concerns.",3/28/25,https://openalex.org/W4409237922,OpenAlex,Deep Learning Models,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,EHRs,Not Applicable,general_psychiatry,age_unspecified,unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,3/28/25
Analyze and Implement a Reinforced AI Chatbot in Guayaquil to Improve Mental Health in Adolescents,"Mental health is vital to the development of young adolescents to create strong relationships and resilience, keeping a positive influence on society. The impact of COVID-19 on mental health is anticipated to be significant to the population, especially adolescents by depriving social contact and creating mental disorders. Currently, there are countless Chatbots that give dynamic chatbot services in health. In recent years, neural networks for natural language processing (NLP) have shown to generate more responses learned by the machine. This study aims to implement and compare two Chatbot mobile applications using Neural generative with sentiment models: OpenAI GPT-3 model, and personalized chatbot based on deep Learning, Transformer BERT, and TextBlob model. The dataset for this model was generated from a database of the flow conversation from GPT-3 and the trained bot, frequently asked questions collected at the beginning and end of the term of academic. To analyze, the sentiment it trains the dataset used in the conversation which validates the prediction through a confusion matrix which resulted in test accuracy frequently correct is 70% for Transformer Bert and 68% in TextBlob. To test the usability of the chatbot and application, a survey was conducted on a group of 30 participants, using Chatbot Usability Questionnaire (CUQ) and Linkert scale, it showed that GPT-3 CUQ Mean is 77,71 higher than Deep Learning. As for the Linkert scale, it was verified that 68% of participants perceived that the chatbot was adequate to their concerns, as well that the acceptance rate was 90%.",29-May-24,https://openalex.org/W4399096418,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,EHRs,Not Applicable,general_psychiatry,adolescence,Ecuador,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,
An experimental study of integrating fine-tuned large language models,"Background: Conversational mental healthcare support plays a crucial role in aiding individuals with mental health concerns. Large language models (LLMs) like GPT and BERT show potential in enhancing chatbot-based therapy responses. Despite their potential, there are recognised limitations in directly deploying these LLMs for therapeutic interactions as they are trained in general context and knowledge data. The overarching aim of this study is to integrate the capabilities of both GPT and BERT with the use of specialised mental health dataset methodologies. Its goal is to enhance mental health conversations, limiting the risk and increasing quality.",4-Jun-24,https://openalex.org/W4400089271,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general_psychiatry,age_unspecified,unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,,
Emotion-Aware Embedding Fusion in Large Language Models,"Empathetic and coherent responses are critical in automated chatbot-facilitated psychotherapy. This study addresses the challenge of enhancing the emotional and contextual understanding of large language models (LLMs) in psychiatric applications. We introduce Emotion-Aware Embedding Fusion, a novel framework integrating hierarchical fusion and attention mechanisms to prioritize semantic and emotional features in therapy transcripts. Our approach combines multiple emotion lexicons, including NRC Emotion Lexicon, VADER, WordNet, and SentiWordNet, with state-of-the-art LLMs such as Flan-T5, Llama 2, DeepSeek-R1, and ChatGPT 4. Therapy session transcripts, comprising over 2000 samples, are segmented into hierarchical levels (word, sentence, and session) using neural networks, while hierarchical fusion combines these features with pooling techniques to refine emotional representations. Attention mechanisms, including multi-head self-attention and cross-attention, further prioritize emotional and contextual features, enabling the temporal modeling of emotional shifts across sessions. The processed embeddings, computed using BERT, GPT-3, and RoBERTa, are stored in the Facebook AI similarity search vector database, which enables efficient similarity search and clustering across dense vector spaces. Upon user queries, relevant segments are retrieved and provided as context to LLMs, enhancing their ability to generate empathetic and contextually relevant responses. The proposed framework is evaluated across multiple practical use cases to demonstrate real-world applicability, including AI-driven therapy chatbots. The system can be integrated into existing mental health platforms to generate personalized responses based on retrieved therapy session data. The experimental results show that our framework enhances empathy, coherence, informativeness, and fluency, surpassing baseline models while improving LLMs emotional intelligence and contextual adaptability for psychotherapy.",12-Mar-25,https://openalex.org/W4408413603,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general_psychiatry,age_unspecified,unspecified,Unspecified,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,
"Structured Dialogue System for Mental Health: An LLM Chatbot Leveraging
  the PM+ Guidelines""","The Structured Dialogue System, referred to as SuDoSys, is an innovative Large Language Model (LLM)-based chatbot designed to provide psychological counseling. SuDoSys leverages the World Health Organization (WHO)'s Problem Management Plus (PM+) guidelines to deliver stage-aware multi-turn dialogues. Existing methods for employing an LLM in multi-turn psychological counseling typically involve direct fine-tuning using generated dialogues, often neglecting the dynamic stage shifts of counseling sessions. Unlike previous approaches, SuDoSys considers the different stages of counseling and stores essential information throughout the counseling process, ensuring coherent and directed conversations. The system employs an LLM, a stage-aware instruction generator, a response unpacker, a topic database, and a stage controller to maintain dialogue flow. In addition, we propose a novel technique that simulates counseling clients to interact with the evaluated system and evaluate its performance automatically. When assessed using both objective and subjective evaluations, SuDoSys demonstrates its effectiveness in generating logically coherent responses. The system's code and program scripts for evaluation are open-sourced.",16-Nov-24,https://openalex.org/W4404570057,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general_psychiatry,age_unspecified,China,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,,,
The Synergy of Dialogue and Art,"The rapid advancements in generative AI have spurred the development of AI chatbots for emotional support. In this work, we designed ArtTheraCat, a novel multimodal chatbot that promotes mental health by integrating supportive dialogue with artistic image generation. Utilizing images as an auxiliary medium, ArtTheraCat guides users through a process of emotion externalization in multi-turn dialogues, which can facilitates self-awareness, reflection, and positive emotional encouragement. Results from a user study involving 21 participants indicate a significant improvement in their emotional state post-interaction with ArtTheraCat, especially in reducing negative affects. Qualitative analysis highlighted the role of generated images as a non-verbal medium in emotional release and self-insight. The findings of this paper provide practical guidance for the design of multimodal mental health chatbots.",13-Nov-24,https://openalex.org/W4404331706,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,general_psychiatry,"adolescence, young_adulthood","China, Malaysia, Germany, Canada",Both,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,
Enhancing Individual Self-Efficacy Through a Self-Growing Memory Artificial Intelligence Agent Integrated with a Diary Application,"This paper introduces an artificial intelligence (AI) interactive system featuring a self-growing memory network designed to enhance self-efficacy, reduce loneliness, and maintain social interaction among the elderly. The system dynamically analyzes and processes user-written diaries, generating empathic and personalized responses tailored to each individual. The system architecture includes an experience extraction model, a self-growing memory network that provides a contextual understanding of the users daily life, a chat agent, and a feedback loop that adaptively learns the users behavioral patterns and emotional states. By drawing on both successful and challenging experiences, the system crafts responses that reinforce the self-efficacy of the user, fostering a sense of accomplishment and engagement. This approach improves the psychological well-being of elderly users and promotes their mental health and overall quality of life through consistent interaction. To validate our proposed method, we developed a diary application to facilitate user interaction and collect diary entries. Over time, the systems capacity to learn and adapt further refines the user experience, suggesting that AI-driven solutions hold significant potential for mitigating the effects of declining self-efficacy on mental health and social interactions. With the proposed system, we achieve an average system usability scale score of 77.3 (SD = 5.4) and a general self-efficacy scale score of 34.2 (SD = 3.5).",20-Jan-25,https://openalex.org/W4406588643,OpenAlex,Small pre-trained model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Not Applicable,general_psychiatry,old,Unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,,,
Enhancing Mental Health Support through Human-AI Collaboration,"The increasing demand for mental health support necessitates innovative solutions to augment traditional therapeutic approaches. This paper explores thepotential of human-AI collaboration in enhancing mental health support, focusing on the development of secure and empathetic AI-enabled chatbots. We examinethe current landscape of mental health chatbots, highlighting their limitations and ethical considerations. Furthermore, we propose a framework for designingAI chatbots that prioritize user privacy, data security, and empathetic communication. By integrating natural language processing, machine learning, and humanoversight, we envision a collaborative model where AI chatbots serve as accessible and preliminary support systems, complementing the expertise and nuancedunderstanding of human mental health professionals. This paper discusses the key challenges and future directions in realizing the potential of secure andempathetic AI in transforming mental health support",25-May,https://openalex.org/W4410939264,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,EHRs,Not Applicable,general_psychiatry,age_unspecified,unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,,,
Yesil o1 Pro,"<sec> <title>BACKGROUND</title> Integrating evidence-based approaches in healthcare and artificial intelligence (AI) is crucial for enhancing clinical decision-making and patient safety. Yesil o1 Pro is a specialized large language model (LLM) designed to transform medical knowledge synthesis by leveraging a comprehensive, curated database of scientific literature, clinical guidelines, and medical textbooks. </sec> <sec> <title>OBJECTIVE</title> The system's innovative ""AI Hospital"" framework employs domain-specific expert agents coordinated by a central Master Agent, enabling tailored and precise medical responses across multiple disciplines. </sec> <sec> <title>METHODS</title> The model's advanced methodology incorporates sophisticated techniques including GraphRAG-based retrieval, extensive fine-tuning with 1.5 million question-answer pairs, and Chain of Thought (CoT) reasoning. Its robust training dataset comprises 100.5M words from high-impact journals, 96.8M words from core medical texts, and 74.4M words from international standards, ensuring a comprehensive and authoritative knowledge base. </sec> <sec> <title>RESULTS</title> Benchmark evaluations demonstrate Yesil o1 Pro's exceptional performance, achieving an overall accuracy of 89.1% and surpassing leading models like GPT-4o (83.9%) and Claude 3.5 Sonnet (83.0%). Domain-specific accuracies are particularly impressive, with 96.1% in Mental Health, 94.6% in Epidemiology, and 94.6% in Dentistry, highlighting the model's proficiency in handling complex, reasoning-intensive medical queries. </sec> <sec> <title>CONCLUSIONS</title> The model shows promising applications in clinical decision support, interdisciplinary collaboration, professional education, and medical research. While challenges remain in real-world integration and maintaining alignment with evolving medical knowledge, Yesil o1 Pro represents a significant advancement in AI-driven healthcare support. Future research will focus on validating the model's utility in clinical environments and developing strategies for seamless healthcare system integration. </sec>",14-Feb-25,https://openalex.org/W4408099196,OpenAlex,Foundation model,Non-Chat Agent,Multi-Agent,Hierarchical Architecture,TRUE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,EHRs,Systematic reviews  / Meta analysis,general_psychiatry,age_unspecified,Unspecified,Unspecified,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,,,
PsyDraw: A Multi-Agent Multimodal System for Mental Health Screening in Left-Behind Children,"Left-behind children (LBCs), numbering over 66 million in China, face severe mental health challenges due to parental migration for work. Early screening and identification of at-risk LBCs is crucial, yet challenging due to the severe shortage of mental health professionals, especially in rural areas. While the House-Tree-Person (HTP) test shows higher child participation rates, its requirement for expert interpretation limits its application in resource-scarce regions. To address this challenge, we propose PsyDraw, a multi-agent system based on Multimodal Large Language Models that assists mental health professionals in analyzing HTP drawings. The system employs specialized agents for feature extraction and psychological interpretation, operating in two stages: comprehensive feature analysis and professional report generation. Evaluation of HTP drawings from 290 primary school students reveals that 71.03% of the analyzes achieved High Consistency with professional evaluations, 26.21% Moderate Consistency and only 2.41% Low Consistency. The system identified 31.03% of cases requiring professional attention, demonstrating its effectiveness as a preliminary screening tool. Currently deployed in pilot schools, \method shows promise in supporting mental health professionals, particularly in resource-limited areas, while maintaining high professional standards in psychological assessment.",19-Dec-24,https://openalex.org/W4405631011,OpenAlex,Foundation model,Non-Chat Agent,Multi-Agent,Flat Architecture,FALSE,TRUE,TRUE,FALSE,FALSE,Images,FALSE,FALSE,Not Applicable,Interview (Questionnaire),general_psychiatry,childhood,china,Unspecified,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,,,
 HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered Therapy Using LLM Agents,"This paper presents HamRaz, a novel Persian-language mental health dataset designed for Person-Centered Therapy (PCT) using Large Language Models (LLMs). Despite the growing application of LLMs in AI-driven psychological counseling, existing datasets predominantly focus on Western and East Asian contexts, overlooking cultural and linguistic nuances essential for effective Persian-language therapy. To address this gap, HamRaz combines script-based dialogues with adaptive LLM role-playing, ensuring coherent and dynamic therapy interactions. We also introduce HamRazEval, a dual evaluation framework that measures conversational quality and therapeutic effectiveness using General Dialogue Metrics and the Barrett-Lennard Relationship Inventory (BLRI). Experimental results show HamRaz outperforms conventional Script Mode and Two-Agent Mode, producing more empathetic, context-aware, and realistic therapy sessions. By releasing HamRaz, we contribute a culturally adapted, LLM-driven resource to advance AI-powered psychotherapy research in diverse communities.",9-Feb-25,https://openalex.org/W4407386105,OpenAlex,Foundation model,Chatbot,Multi-Agent,Team Architecture,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Interview (Questionnaire),general_psychiatry,age_unspecified,Iran,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,,,
Combining Artificial Users and Psychotherapist Assessment to Evaluate Large Language Model-based Mental Health Chatbots,"Large Language Models (LLMs) promise to overcome limitations of rule-based mental health chatbots through more natural conversations. However, evaluating LLM-based mental health chatbots presents a significant challenge: Their probabilistic nature requires comprehensive testing to ensure therapeutic quality, yet conducting such evaluations with people with depression would impose an additional burden on vulnerable people and risk exposing them to potentially harmful content. Our paper presents an evaluation approach for LLM-based mental health chatbots that combines dialogue generation with artificial users and dialogue evaluation by psychotherapists. We developed artificial users based on patient vignettes, systematically varying characteristics such as depression severity, personality traits, and attitudes toward chatbots, and let them interact with a LLM-based behavioral activation chatbot. Ten psychotherapists evaluated 48 randomly selected dialogues using standardized rating scales to assess the quality of behavioral activation and its therapeutic capabilities. We found that while artificial users showed moderate authenticity, they enabled comprehensive testing across different users. In addition, the chatbot demonstrated promising capabilities in delivering behavioral activation and maintaining safety. Furthermore, we identified deficits, such as ensuring the appropriateness of the activity plan, which reveals necessary improvements for the chatbot. Our framework provides an effective method for evaluating LLM-based mental health chatbots while protecting vulnerable people during the evaluation process. Future research should improve the authenticity of artificial users and develop LLM-augmented evaluation tools to make psychotherapist evaluation more efficient, and thus further advance the evaluation of LLM-based mental health chatbots.",27-Mar-25,,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,6A7,"adolescence, young_adulthood",unspecified,Both,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,,,
Aiding LLMs with Clinical Scoresheets,"<sec> <title>BACKGROUND</title> Large language models (LLMs) have demonstrated the ability to perform complex tasks traditionally requiring human intelligence. However, their use in automated diagnostics for psychiatry and behavioral sciences remains understudied. </sec> <sec> <title>OBJECTIVE</title> To evaluate whether simple prompting of LLM-based chatbots can support diagnostic classification for neuropsychiatric conditions, and whether providing clinical assessment scales improves performance. </sec> <sec> <title>METHODS</title> We tested two approaches using ChatGPT and Claude: (1) direct diagnostic querying and (2) execution of chatbot-generated code for classification. Three diagnostic datasets were used: ASDBank (autism), AphasiaBank (aphasia), and DAIC-WOZ (depression and related conditions). Each approach was evaluated with and without the aid of clinical assessment scales. Performance was compared to existing machine learning benchmarks on these datasets. </sec> <sec> <title>RESULTS</title> Across all three datasets, incorporating clinical assessment scales led to modest improvements in performance, but results remained inconsistent and below the performances in prior studies. On the AphasiaBank dataset, the Direct Diagnosis approach with ChatGPT 4 yielded a relatively low F1 score (0.655) and very low specificity (0.33). Using the Code Generation method, performance improved substantially, with ChatGPT 4o achieving an F1 score of 0.814, specificity of 0.786, and sensitivity of 0.843. On the ASDBank dataset, Direct Diagnosis approaches yielded lower F1 scores (0.598 for ChatGPT 4 and 0.514 for ChatGPT 4o), while the Code Generation approach with Claude 3.5 improved performance to an F1 score of 0.6, specificity of 0.67, and sensitivity of 0.69. In the Daic-woz dataset, the Direct Diagnosis method produced high sensitivity (0.939) but very low specificity (0.08) and an F1 score of 0.452 with ChatGPT 4. Code Generation improved specificity (up to 0.886 with ChatGPT 4o), but F1 scores remained low overall, ranging from 0.203 to 0.33. These findings indicate that while clinical scales can help structure outputs in both approaches they do not consistently enable LLMs to reach clinically high diagnostic performance when using simple prompts. </sec> <sec> <title>CONCLUSIONS</title> Current LLM-based chatbots, when prompted navely, underperform on psychiatric and behavioral diagnostic tasks compared to specialized machine learning models. Clinical assessment scales might modestly aid chatbot performance, but more sophisticated prompt engineering and domain integration are likely required to reach clinically actionable standards. </sec> <sec> <title>CLINICALTRIAL</title> Not applicable. </sec>",,https://openalex.org/W4408927647,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,TRUE,Text,TRUE,FALSE,EHRs,Not Applicable,"6A0, 6A7",age_unspecified,unspecified,Unspecified,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,
"""Prompt Engineering a Schizophrenia Chatbot: Utilizing a Multi-Agent
  Approach for Enhanced Compliance with Prompt Instructions""","Patients with schizophrenia often present with cognitive impairments that may hinder their ability to learn about their condition. These individuals could benefit greatly from education platforms that leverage the adaptability of Large Language Models (LLMs) such as GPT-4. While LLMs have the potential to make topical mental health information more accessible and engaging, their black-box nature raises concerns about ethics and safety. Prompting offers a way to produce semi-scripted chatbots with responses anchored in instructions and validated information, but prompt-engineered chatbots may drift from their intended identity as the conversation progresses. We propose a Critical Analysis Filter for achieving better control over chatbot behavior. In this system, a team of prompted LLM agents are prompt-engineered to critically analyze and refine the chatbot's response and deliver real-time feedback to the chatbot. To test this approach, we develop an informational schizophrenia chatbot and converse with it (with the filter deactivated) until it oversteps its scope. Once drift has been observed, AI-agents are used to automatically generate sample conversations in which the chatbot is being enticed to talk about out-of-bounds topics. We manually assign to each response a compliance score that quantifies the chatbot's compliance to its instructions; specifically the rules about accurately conveying sources and being transparent about limitations. Activating the Critical Analysis Filter resulted in an acceptable compliance score (>=2) in 67.0% of responses, compared to only 8.7% when the filter was deactivated. These results suggest that a self-reflection layer could enable LLMs to be used effectively and safely in mental health platforms, maintaining adaptability while reliably limiting their scope to appropriate use cases.",10-Oct-24,https://openalex.org/W4403578724,OpenAlex,Foundation model,Chatbot,Multi-Agent,Team Architecture,TRUE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Systematic reviews  / Meta analysis,6A2,"adolescence, young_adulthood",unspecified,Unspecified,TRUE,FALSE,TRUE,,,TRUE,,,TRUE,TRUE,TRUE,FALSE,,,
Chess Game Therapy to Improve the Mental Ability of Dementias Patients,"Dementia is a term used to describe cognitive decline sufficiently severe to cause problems with daily functioning, such as remembrance, language, real concern, and other brain abilities. This research looks into how to treat them with game therapy. In game therapy, the counsellor encourages the client to identify personal history that may have had an influence on the current circumstance, primarily through play but also through words, in a way that is consistent with the impacted role. Because it can help people communicate, uncover hidden ideas and feelings, confront unresolved trauma, and experience personal growth, game therapy is widely regarded as an important, effective, and developmentally appropriate mental health treatment. Here the environment is created by using a minified chess game with an Artificial Intelligence (AI) Virtual Assistant to assist the player (patient) in teaching the game and stimulating their cognitive skills. In order to prevent short-term oscillations from a moving target, the agent is trained using Deep Q-Learning Network (DQN) methods for reinforcement learning. With a replay memory, Given that it takes into account the unpredictability that would come from online learning, the first method more closely approaches a supervised learning issue. The goal of this Virtual AI agent is to only assist the player. It will not help the patient in winning the games, but rather to help the patient to learn chess faster and stimulate the cognitive ability. The agent is programmed to assist the patient in two ways, either to take an attacking approach or to play-safe. This agent will also provide more information on why the move was suggested or warned. Thus, by making the patient more involved in the game and resulting in an improvement in cognitive ability of the patient.",5-May-23,https://openalex.org/W4372268937,OpenAlex,Deep Learning Models,Non-Chat Agent,Single Agent,Not Applicable,,TRUE,,,,Text,FALSE,TRUE,Not Applicable,Not Applicable,6D8,age_unspecified,unspecified,Unspecified,,,TRUE,,,,,,TRUE,,,,,,
"Cactus: Towards Psychological Counseling Conversations using Cognitive
  Behavioral Theory","Recently, the demand for psychological counseling has significantly increased as more individuals express concerns about their mental health. This surge has accelerated efforts to improve the accessibility of counseling by using large language models (LLMs) as counselors. To ensure client privacy, training open-source LLMs faces a key challenge: the absence of realistic counseling datasets. To address this, we introduce Cactus, a multi-turn dialogue dataset that emulates real-life interactions using the goal-oriented and structured approach of Cognitive Behavioral Therapy (CBT). We create a diverse and realistic dataset by designing clients with varied, specific personas, and having counselors systematically apply CBT techniques in their interactions. To assess the quality of our data, we benchmark against established psychological criteria used to evaluate real counseling sessions, ensuring alignment with expert evaluations. Experimental results demonstrate that Camel, a model trained with Cactus, outperforms other models in counseling skills, highlighting its effectiveness and potential as a counseling agent. We make our data, model, and code publicly available.",7/3/24,https://openalex.org/W4400376569,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,,Text,,TRUE,Not Applicable,Not Applicable,"6A7, 6B0","adolescence, young_adulthood, middle_adulthood, old",unspecified,Both,,,TRUE,,,TRUE,TRUE,,TRUE,TRUE,TRUE,TRUE,,,7/3/24
AI-Powered Mental Health Chatbot,"Abstract: Mental health disorders, including anxiety and depression, affect millions worldwide, yet many individuals refrain from seeking professional help due to stigma, accessibility challenges, and financialconstraints.ThispaperpresentsanAIpoweredmentalhealthchatbotdesignedtoprovide empathetic, accessible, and anonymous emotional support using Generative AI and Natural Language Processing (NLP). The chatbot offers 24/7 availability, confidential conversations, and self-care recommendations while ensuring data privacy and security. Through comparison with existingsolutions, thisstudyhighlightsits superiorcontextualawarenessandengagementfeatures. Future enhancements, including voice-based interaction and multilingual support, are explored.",25-May,https://openalex.org/W4410360729,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general_psychiatry,age_unspecified,unspecified,Unspecified,TRUE,,TRUE,,,,,,TRUE,TRUE,,TRUE,,,
"Interactive Agents: Simulating Counselor-Client Psychological Counseling
  via Role-Playing LLM-to-LLM Interactions","Virtual counselors powered by large language models (LLMs) aim to create interactive support systems that effectively assist clients struggling with mental health challenges. To replicate counselor-client conversations, researchers have built an online mental health platform that allows professional counselors to provide clients with text-based counseling services for about an hour per session. Notwithstanding its effectiveness, challenges exist as human annotation is time-consuming, cost-intensive, privacy-protected, and not scalable. To address this issue and investigate the applicability of LLMs in psychological counseling conversation simulation, we propose a framework that employs two LLMs via role-playing for simulating counselor-client interactions. Our framework involves two LLMs, one acting as a client equipped with a specific and real-life user profile and the other playing the role of an experienced counselor, generating professional responses using integrative therapy techniques. We implement both the counselor and the client by zero-shot prompting the GPT-4 model. In order to assess the effectiveness of LLMs in simulating counselor-client interactions and understand the disparities between LLM- and human-generated conversations, we evaluate the synthetic data from various perspectives. We begin by assessing the client's performance through automatic evaluations. Next, we analyze and compare the disparities between dialogues generated by the LLM and those generated by professional counselors. Furthermore, we conduct extensive experiments to thoroughly examine the performance of our LLM-based counselor trained with synthetic interactive dialogues by benchmarking against state-of-the-art models for mental health.",8/28/24,https://openalex.org/W4402706023,OpenAlex,Foundation model,Chatbot,Multi-Agent,Team Architecture,,TRUE,,,,Text,TRUE,,Not Applicable,Interview (Questionnaire),general_psychiatry,unspecified,unspecified,Unspecified,,,TRUE,,,TRUE,TRUE,,TRUE,TRUE,TRUE,,,,8/28/24
The use of artificial intelligence in psychotherapy,"The increasing demand for psychotherapy and limited access to specialists underscore the potential of artificial intelligence (AI) in mental health care. This study evaluates the effectiveness of the AI-powered Friend chatbot in providing psychological support during crisis situations, compared to traditional psychotherapy. A randomized controlled trial was conducted with 104 women diagnosed with anxiety disorders in active war zones. Participants were randomly assigned to two groups: the experimental group used the Friend chatbot for daily support, while the control group received 60-minute psychotherapy sessions three times a week. Anxiety levels were assessed using the Hamilton Anxiety Rating Scale and Beck Anxiety Inventory. T-tests were used to analyze the results. Both groups showed significant reductions in anxiety levels. The control group receiving traditional therapy had a 45% reduction on the Hamilton scale and a 50% reduction on the Beck scale, compared to 30% and 35% reductions in the chatbot group. While the chatbot provided accessible, immediate support, traditional therapy proved more effective due to the emotional depth and adaptability provided by human therapists. The chatbot was particularly beneficial in crisis settings where access to therapists was limited, proving its value in scalability and availability. However, its emotional engagement was notably lower compared to in-person therapy. The Friend chatbot offers a scalable, cost-effective solution for psychological support, particularly in crisis situations where traditional therapy may not be accessible. Although traditional therapy remains more effective in reducing anxiety, a hybrid model combining AI support with human interaction could optimize mental health care, especially in underserved areas or during emergencies. Further research is needed to improve AI's emotional responsiveness and adaptability.",28-Feb-25,https://openalex.org/W4408068023,OpenAlex,Deep Learning Models,Chatbot,Single Agent,Not Applicable,,TRUE,,,,Text,,,,,6B0,"adolescence, young_adulthood, middle_adulthood",ukraine,Female only,TRUE,,TRUE,,,,,,TRUE,,,TRUE,,,
Leveraging Large Language Models for Simulated Psychotherapy Client Interactions,"The rapid advancements in large language models (LLMs) have opened up new opportunities for transforming patient engagement in healthcare through conversational AI. This paper presents an overview of the current landscape of LLMs in healthcare, specifically focusing on their applications in analyzing and generating conversations for improved patient engagement. We showcase the power of LLMs in handling unstructured conversational data through four case studies: (1) analyzing mental health discussions on Reddit, (2) developing a personalized chatbot for cognitive engagement in seniors, (3) summarizing medical conversation datasets, and (4) designing an AI-powered patient engagement system. These case studies demonstrate how LLMs can effectively extract insights and summarizations from unstructured dialogues and engage patients in guided, goal-oriented conversations. Leveraging LLMs for conversational analysis and generation opens new doors for many patient-centered outcomes research opportunities. However, integrating LLMs into healthcare raises important ethical considerations regarding data privacy, bias, transparency, and regulatory compliance. We discuss best practices and guidelines for the responsible development and deployment of LLMs in healthcare settings. Realizing the full potential of LLMs in digital health will require close collaboration between the AI and healthcare professionals communities to address technical challenges and ensure these powerful tools' safety, efficacy, and equity.",19-Jun-24,https://openalex.org/W4399913032,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,,Text,TRUE,TRUE,,,"6A7, 6B0",unspecified,unspecified,Unspecified,,,,,,TRUE,,,TRUE,,TRUE,,,,
Machine Learning and Deep Learning Models for Predicting Mental Health Disorders,"Mental health disorders have recently been prompting increased concern globally and finding new ways of diagnosing and treating them efficiently. Machine learning (ML) and deep learning (DL) enabled chatbots are enormous tool for predicting and supporting mental health. This work aims to carry out an assessment of several AI models for prognostics of mental health disorders based on the comparison of intents, patterns, and responses in a structured chatbot-based dataset. Since it is intent-based, our dataset is best suited to classifying user inputs accurately into different mental health thematic buckets such as anxiety, stress, and proved suicide ideation. To assess the models, we compared basic models such as Multinomial Nave Bayes, Random Forest and SVM as well as deep learning models including LSTM networks. SVM and LSTM showed promising results among the tested models with the accuracy of 94.6%. LSTM was proved to address the problem of sequential context dependence typical for conversational data. For further improvement in the models accuracy, we used ensemble methods whose accuracy came out near like the highest accuracy models, 94.2% accurate. This work is new in the sense that it involves the use of data from an intent-based chatbot, and a comparison of the ML and DL models designed specifically for the prediction of mental health outcomes. Also, it is important to note that we dealt with underrepresented intents, including suicide ideation, using data augmentation and ensemble approach. It fills the gaps in the deployment of AI for mental health by providing recommendations concerning the models performance and possible ethical concerns as well as integrating it into conversational assistance. We also found the relevance of an AI chatbot in the delivery of efficient and easily deployable intervention for mental health.",28-Dec-24,https://openalex.org/W4405864589,OpenAlex,Deep Learning Models,Chatbot,Non-Agent,Not Applicable,,TRUE,TRUE,,,Text,,,,,general_psychiatry,unspecified,unspecified,Unspecified,TRUE,,,,TRUE,,,,TRUE,,,,,,
Generative Transformer Chatbots for Mental Health Support,"Mental health is a critical issue worldwide and effective treatments are available. However, incidence of social stigma prevents many from seeking the support they need. Given the rapid developments in the field of large-language models, this study explores the potential of chatbots to support people experiencing depression and anxiety. The focus of this research is on the engineering aspect of building chatbots, and through topology optimisation find an effective hyperparameter set that can predict tokens with 88.65% accuracy and with a performance of 96.49% and 97.88% regarding the correct token appearing in the top 5 and 10 predictions. Examples of how optimised chatbots can effectively answer questions surrounding mental health are provided, generalising information from verified online sources. The results of this study demonstrate the potential of chatbots to provide accessible and anonymous support to individuals who may otherwise be deterred by the stigma associated with seeking help for mental health issues. However, the limitations and challenges of using chatbots for mental health support must also be acknowledged, and future work is suggested to fully understand the potential and limitations of chatbots and to ensure that they are developed and deployed ethically and responsibly.",10-Aug-23,https://openalex.org/W4385729889,OpenAlex,Deep Learning Models,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,,,,Text,TRUE,,,,"6A7, 6B0",age_unspecified,unspecified,Unspecified,,,TRUE,,,,,,TRUE,,,,,,
Soul Support: AI Driven Emotional Assistance ChatBot,"Abstract This paper introduces the development of an AI-based Mental Health Therapist Chatbot designed to provide immediate, accessible emotional support and mental health information. Utilizing deep learning and natural language processing (NLP), the chatbot identifies user intents and delivers appropriate responses across various mental health topics, including anxiety, stress, and sadness. The system is deployed via a web interface that supports both text and voice input, enhancing user accessibility. On the backend, the chatbot uses a Flask server, a trained Keras model for intent classification, and preprocessing pipelines built with NLTK and SpaCy. Experimental evaluations show high accuracy in intent recognition and efficient real-time interaction. Additionally, user feedback reflects a positive reception in terms of helpfulness and ease of use. While the chatbot does not replace professional care, it acts as a supportive digital tool for initial engagement and emotional guidance. Future enhancements will focus on multilingual support, contextual dialogue, and integration with mental health services.",6/12/25,https://openalex.org/W4411243695,OpenAlex,Deep Learning Models,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,"Text, Audio",,TRUE,,,general_psychiatry,age_unspecified,unspecified,Unspecified,,,TRUE,,,TRUE,,,TRUE,,,TRUE,,,6/12/25
MindMate: AI-Powered Multilingual Mental Health Chatbot,"An AI motivated mental health chat bot which can enable personalized support through voice as well as text based interaction, its dubbed as MindMate. Using the Rasa framework for natural language understanding (NLU) and dialogue management, along with Streamlit to make the interface user friendly and breathable for people, it makes an empathetic environment to allow people to express themselves and get responses from it. It is a multilingual chatbot, so it can be used by a lot of people with different languages. MindMate approaches users with a personalized approach to learn from user interactions to provide personalized responses, suggestions and resources that respond to the individual needs and preferences. This unique system not only makes it easy to share how you feel, but also gives you useful resources for dealing with whatever problem or issue you may be facing, helping to solve those problems. MindMate hopes to change the game of mental health support, incorporating advanced AI technology into an intuitive interface meant to ensure that mental well-being is readily available and effective for people seeking assistance with how to manage that aspect of their life.",12-May-25,https://openalex.org/W4410296801,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,,"Text, Audio",TRUE,,,,general_psychiatry,age_unspecified,uspecified,Unspecified,TRUE,,TRUE,,,,,,TRUE,,FALSE,TRUE,,,
Virtual Reality System Controlled by Embedded Artificial Intelligence,"In recent years, the health area has received technological contributions that provide support for diagnostic practices, monitoring, and treatment of different disorders and diseases, mainly combining various techniques of Artificial Intelligence, Virtual Reality, and Mobile Computing. There are many challenges to integrating these technologies and providing solutions that consider the automation of processes, the simplification of interaction between professionals and patients, the low price of equipment, the individualization of use, mobility, and the use of Artificial Intelligence strategies. Aiming to overcome the limitations of two previous works, which applied technological combinations in the desensitization of stress and phobias, this work aims to develop a technological combination that integrates an autonomous and low-cost virtual environment, with multi-agent control and natural language communication support, to be used in the Treatment by Exposure in Virtual Environments - VRET in the area of Clinical Psychology, more specifically related to Anxiety Disorders. Low-cost virtual reality glasses were used, with visualization on a smartphone. The prototype, called PhobIA 3DS, is controlled by multi-agents that have modules for capturing physiological signals (heart rate); uses natural language to obtain the level of anxiety perceived by the patient; considers these two pieces of information in a Fuzzy system, which, in turn, generates a response on the calculated level of anxiety; and controls and changes the display of specific scenarios for each level of anxiety. Finally, the system was evaluated by a group of 6 experienced psychologists to verify aspects of the interface, relevance, and usability. The data obtained by the evaluation showed positive results and good prospects for using the system in real activities. As a contribution, this work created an integration of AI technologies in an ESP32 microcontroller connected to a smartphone and attached to low-cost goggles. This combination of technics opens perspectives for adopting affordable technologies in phobia treatments.",10/31/24,https://openalex.org/W4404141049,OpenAlex,Rule-based models,Non-Chat Agent,Multi-Agent,Flat Architecture,,TRUE,,,,Text,,TRUE,Lab Test,Interview (Questionnaire),6B0,age_unspecified,unspecified,Unspecified,TRUE,,TRUE,,,,,,,,TRUE,,,,
Towards Understanding Emotions,"Providing timely support and intervention is crucial in mental health settings. As the need to engage youth comfortable with texting increases, mental health providers are exploring and adopting text-based media such as chatbots, community-based forums, online therapies with licensed professionals, and helplines operated by trained responders. To support these text-based media for mental healthparticularly for crisis carewe are developing a system to perform passive emotion-sensing using a combination of keystroke dynamics and sentiment analysis. Our early studies of this system posit that the analysis of short text messages and keyboard typing patterns can provide emotion information that may be used to support both clients and responders. We use our preliminary findings to discuss the way forward for applying AI to support mental health providers in providing better care.",1-Jul-24,https://openalex.org/W4399794874,OpenAlex,Foundation model,Non-Chat Agent,Single Agent,Not Applicable,,TRUE,TRUE,,,Text,TRUE,,,Interview (Questionnaire),general_psychiatry,"adolescence, young_adulthood, middle_adulthood",singapore,Both,TRUE,TRUE,TRUE,,,,,,TRUE,,TRUE,,,,
Integrating ELECTRA and BERT models in transformer-based mental healthcare chatbot,"Over the last decade, the surge in mental health disorders has necessitated innovative support methods, notably artificial intelligent (AI) chatbots. These chatbots provide prompt, tailored conversations, becoming crucial in mental health support. This article delves into the use of sophisticated models like convolutional neural network (CNN), long-short term memory (LSTM), efficiently learning an encoder that classifies token replacements accurately (ELECTRA), and bidirectional encoder representation of transformers (BERT) in developing effective mental health chatbots. Despite their importance for emotional assistance, these chatbots struggle with precise and relevant responses to complex mental health issues. BERT, while strong in contextual understanding, lacks in response generation. Conversely, ELECTRA shows promise in text creation but is not fully exploited in mental health contexts. The article investigates merging ELECTRA and BERT to improve chatbot efficiency in mental health situations. By leveraging an extensive mental health dialogue dataset, this integration substantially enhanced chatbot precision, surpassing 99% accuracy in mental health responses. This development is a significant stride in advancing AI chatbot interactions and their contribution to mental health support.",25-Jan,https://openalex.org/W4403939359,OpenAlex,Deep Learning Models,Chatbot,Single Agent,Not Applicable,,TRUE,,,,Text,TRUE,,,,general_psychiatry,unspecified,unspecified,Unspecified,,,TRUE,,,,,,TRUE,,,,,,
TherapyTrainer: Using AI to train therapists in written exposure therapy,"Though evidence-based treatments for mental disorders are effective, existing implementation efforts are expensive and difficult to scale. Novel solutions especially those that offer active learning strategies, repeat skill practice and personalized feedback to therapists  are needed to fill this gap. We developed TherapyTrainer, which uses large language models (LLMs) to allow therapists to practice delivering written exposure therapy (WET) for PTSD to AI-Patients while receiving expert feedback from an AI-Consultant. Here we present initial feasibility, acceptability and usability data for TherapyTrainer gathered from therapists, supervisors, and WET expert-consultants across iterative rounds of development. In Phase 1, we rapidly prototyped and developed TherapyTrainer based on ongoing feedback from WET clinicians and experts (n = 4). In Phase 2, mixed methods data from therapists engaged in an otherwise-routine WET workshop (n = 14) indicated that TherapyTrainer is feasible and acceptable and may help therapists feel prepared to deliver WET. In Phase 3, therapists (n = 6) completed structured user testing interviews to identify key issues impacting usability for subsequent rounds of development. AI and large language models hold potential to provide ongoing support to therapists in a cost-effective and scalable manner, and may help close the research-practice gap.",6/25/25,https://osf.io/wx93m_v2/,OSF,Foundation model,Chatbot,Multi-Agent,Team Architecture,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,general_psychiatry,"young_adulthood, middle_adulthood",unspecified,Both,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,,,6/25/25
"Exploring ChatGPT's Capabilities, Stability, Potential and Risks in Conducting Psychological Counseling through Simulations in School Counseling","This study explores ChatGPT's capabilities, stability, and risks in simulating psychological counseling sessions in a school counseling context. Using scripted role-plays between a human counselor and an AI client, we examine how a large language model performs core counseling skills such as empathy, reflection, summarizing, and asking open-ended questions, as well as its ability to maintain therapeutic communication over time. We focus on how consistently ChatGPT can behave like a ""virtual client"" for school counselors in training, and how its responses might support or disrupt counselor skill development, supervision, and practice. At the same time, we analyze potential risks, including inaccurate or unsafe suggestions, over-compliance with counselor prompts, and the illusion of a competent therapist where no real professional judgment exists. The findings suggest that ChatGPT can serve as a low-cost, always-available training tool for practicing counseling techniques and interviewing skills in education and mental health settings, but it should not be viewed as a replacement for a human therapist or school counselor. We propose practical guidelines for educators, supervisors, and researchers who wish to use ChatGPT or similar LLM-based conversational agents in counseling training, highlighting how to leverage its potential while managing ethical, pedagogical, and psychological risks.",22-Jun-25,https://osf.io/preprints/psyarxiv/6zqkr_v1/,OSF,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,TRUE,EHRs,Not Applicable,general_psychiatry,adolescence,unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,,,
"Empowering pediatric, adolescent, and young adult patients with cancer","Pediatric and adolescent/young adult (AYA) cancer patients face profound psychological challenges, exacerbated by limited access to continuous mental health support. While conventional therapeutic interventions often follow structured protocols, the potential of generative artificial intelligence (AI) chatbots to provide continuous conversational support remains unexplored. This study evaluates the feasibility and impact of AI chatbots in alleviating psychological distress and enhancing treatment engagement in this vulnerable population.",2025 Feb,https://pubmed.ncbi.nlm.nih.gov/40070545,Pubmed,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,"6A7, 6B0","childhood, adolescence, young_adulthood",Japan,Both,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,,,
Safety of Large Language Models in Addressing Depression,"Background Generative artificial intelligence (AI) models, exemplified by systems such as ChatGPT, Bard, and Anthropic, are currently under intense investigation for their potential to address existing gaps in mental health support. One implementation of these large language models involves the development of mental health-focused conversational agents, which utilize pre-structured prompts to facilitate user interaction without requiring specialized knowledge in prompt engineering. However, uncertainties persist regarding the safety and efficacy of these agents in recognizing severe depression and suicidal tendencies. Given the well-established correlation between the severity of depression and the risk of suicide, improperly calibrated conversational agents may inadequately identify and respond to crises. Consequently, it is crucial to investigate whether publicly accessible repositories of mental health-focused conversational agents can consistently and safely address crisis scenarios before considering their adoption in clinical settings. This study assesses the safety of publicly available ChatGPT-3.5 conversational agents by evaluating their responses to a patient simulation indicating worsening depression and suicidality. Methodology This study evaluated ChatGPT-3.5 conversational agents on a publicly available repository specifically designed for mental health counseling. Each conversational agent was evaluated twice by a highly structured patient simulation. First, the simulation indicated escalating suicide risk based on the Patient Health Questionnaire (PHQ-9). For the second patient simulation, the escalating risk was presented in a more generalized manner not associated with an existing risk scale to assess the more generalized ability of the conversational agent to recognize suicidality. Each simulation recorded the exact point at which the conversational agent recommended human support. Then, the simulation continued until the conversational agent stopped entirely and shut down completely, insisting on human intervention. Results All 25 agents available on the public repository FlowGPT.com were evaluated. The point at which the conversational agents referred to a human occurred around the mid-point of the simulation, and definitive shutdown predominantly only happened at the highest risk levels. For the PHQ-9 simulation, the average initial referral and shutdown aligned with PHQ-9 scores of 12 (moderate depression) and 25 (severe depression). Few agents included crisis resources - only two referenced suicide hotlines. Despite the conversational agents insisting on human intervention, 22 out of 25 agents would eventually resume the dialogue if the simulation reverted to a lower risk level. Conclusions Current generative AI-based conversational agents are slow to escalate mental health risk scenarios, postponing referral to a human to potentially dangerous levels. More rigorous testing and oversight of conversational agents are needed before deployment in mental healthcare settings. Additionally, further investigation should explore if sustained engagement worsens outcomes and whether enhanced accessibility outweighs the risks of improper escalation. Advancing AI safety in mental health remains imperative as these technologies continue rapidly advancing.",12/18/23,https://pubmed.ncbi.nlm.nih.gov/38111813,Pubmed,Foundation model,Chatbot,Single Agent,Not Applicable,,,TRUE,,FALSE,Text,,TRUE,,,6A7,age_unspecified,unspecified,Unspecified,TRUE,,,,,,,,TRUE,,,,,,12/18/23
Usability Comparison Among Healthy Participants of an Anthropomorphic Digital Human and a Text-Based Chatbot as a Responder to Questions on Mental Health: Randomized Controlled Trial,"The use of chatbots in mental health support has increased exponentially in recent years, with studies showing that they may be effective in treating mental health problems. More recently, the use of visual avatars called digital humans has been introduced. Digital humans have the capability to use facial expressions as another dimension in human-computer interactions. It is important to study the difference in emotional response and usability preferences between text-based chatbots and digital humans for interacting with mental health services.",1/1/24,https://pubmed.ncbi.nlm.nih.gov/38683664,Pubmed,Rule-based models,Chatbot,Single Agent,Not Applicable,,,,,TRUE,"Text, Audio",TRUE,TRUE,,,general_psychiatry,"adolescence, young_adulthood, middle_adulthood, old",sweden,Both,FALSE,,TRUE,,,,,,TRUE,,,TRUE,,,1/1/24
Identifying depression and its determinants upon initiating treatment: ChatGPT versus primary care physicians,To compare evaluations of depressive episodes and suggested treatment protocols generated by Chat Generative Pretrained Transformer (ChatGPT)-3 and ChatGPT-4 with the recommendations of primary care physicians.,,https://pubmed.ncbi.nlm.nih.gov/37844967,Pubmed,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,TRUE,,TRUE,Text,TRUE,TRUE,EHRs,Interview (Questionnaire),6A7,middle_adulthood,unspecified,Both,FALSE,TRUE,TRUE,,,,,,TRUE,,TRUE,,,,
"Artificial intelligence conversational agents in mental health: Patients see potential, but prefer humans in the loop","Digital mental health interventions, such as artificial intelligence (AI) conversational agents, hold promise for improving access to care by innovating therapy and supporting delivery. However, little research exists on patient perspectives regarding AI conversational agents, which is crucial for their successful implementation. This study aimed to fill the gap by exploring patients' perceptions and acceptability of AI conversational agents in mental healthcare.",1/31/25,https://pubmed.ncbi.nlm.nih.gov/39957757,Pubmed,Foundation model,Chatbot,Single Agent,Not Applicable,,,,,TRUE,Text,,,,,6B0,"adolescence, young_adulthood, middle_adulthood",united_states,Both,,,,,,,,TRUE,,,,TRUE,,,1/31/25
The plasticity of ChatGPT's mentalizing abilities: personalization for personality structures,"This study evaluated the potential of ChatGPT, a large language model, to generate mentalizing-like abilities that are tailored to a specific personality structure and/or psychopathology. Mentalization is the ability to understand and interpret one's own and others' mental states, including thoughts, feelings, and intentions. Borderline Personality Disorder (BPD) and Schizoid Personality Disorder (SPD) are characterized by distinct patterns of emotional regulation. Individuals with BPD tend to experience intense and unstable emotions, while individuals with SPD tend to experience flattened or detached emotions. We used ChatGPT's free version 23.3 and assessed the extent to which its responses akin to emotional awareness (EA) were customized to the distinctive personality structure-character characterized by Borderline Personality Disorder (BPD) and Schizoid Personality Disorder (SPD), employing the Levels of Emotional Awareness Scale (LEAS). ChatGPT was able to accurately describe the emotional reactions of individuals with BPD as more intense, complex, and rich than those with SPD. This finding suggests that ChatGPT can generate mentalizing-like responses consistent with a range of psychopathologies in line with clinical and theoretical knowledge. However, the study also raises concerns regarding the potential for stigmas or biases related to mental diagnoses to impact the validity and usefulness of chatbot-based clinical interventions. We emphasize the need for the responsible development and deployment of chatbot-based interventions in mental health, which considers diverse theoretical frameworks.",2023 Sep ,https://pubmed.ncbi.nlm.nih.gov/37720897,Pubmed,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,,,,Interview (Questionnaire),6D1,age_unspecified,unspecified,Unspecified,,FALSE,TRUE,,,FALSE,,,TRUE,,,,,,2023
"A multimodal dialog approach to mental state characterization in clinically depressed, anxious, and suicidal populations","The rise of depression, anxiety, and suicide rates has led to increased demand for telemedicine-based mental health screening and remote patient monitoring (RPM) solutions to alleviate the burden on, and enhance the efficiency of, mental health practitioners. Multimodal dialog systems (MDS) that conduct on-demand, structured interviews offer a scalable and cost-effective solution to address this need.",2023 Sep,https://pubmed.ncbi.nlm.nih.gov/37767217,Pubmed,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,"Text, Audio, Image, Video",,,,,"6A7, 6B0, 6B4","adolescence, young adulthood, middle adulthood",United States,Both,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,
Evaluation of ChatGPT's responses to information needs and information seeking of dementia patients,,2024 May 4,https://pubmed.ncbi.nlm.nih.gov/38704403/,Pubmed,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,FALSE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,6D8,"middle adulthood, old",Unspecified,Both,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,,,
Deep learning-based dimensional emotion recognition for conversational agent-based cognitive behavioral therapy,"Internet-based cognitive behavioral therapy (iCBT) offers a scalable, cost-effective, accessible, and low-threshold form of psychotherapy. Recent advancements explored the use of conversational agents such as chatbots and voice assistants to enhance the delivery of iCBT. These agents can deliver iCBT-based exercises, recognize and track emotional states, assess therapy progress, convey empathy, and potentially predict long-term therapy outcome. However, existing systems predominantly utilize categorical approaches for emotional modeling, which can oversimplify the complexity of human emotional states. To address this, we developed a transformer-based model for dimensional text-based emotion recognition, fine-tuned with a novel, comprehensive dimensional emotion dataset comprising 75,503 samples. This model significantly outperforms existing state-of-the-art models in detecting the dimensions of valence, arousal, and dominance, achieving a Pearson correlation coefficient of <i>r</i>=0.90, <i>r</i>=0.77, and <i>r</i>=0.64, respectively. Furthermore, a feasibility study involving 20 participants confirmed the model's technical effectiveness and its usability, acceptance, and empathic understanding in a conversational agent-based iCBT setting, marking a substantial improvement in personalized and effective therapy experiences.",2024 Jun 24,https://pubmed.ncbi.nlm.nih.gov/38983201,Pubmed,"Rule-based models, Small pre-trained model",Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Interview (Questionnaire),"6A7, 6B0","adolescence, young adulthood",Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,,,
Using ChatGPT in Psychiatry to Design Script Concordance Tests in Undergraduate Medical Education: Mixed Methods Study,"Undergraduate medical studies represent a wide range of learning opportunities served in the form of various teaching-learning modalities for medical learners. A clinical scenario is frequently used as a modality, followed by multiple-choice and open-ended questions among other learning and teaching methods. As such, script concordance tests (SCTs) can be used to promote a higher level of clinical reasoning. Recent technological developments have made generative artificial intelligence (AI)-based systems such as ChatGPT (OpenAI) available to assist clinician-educators in creating instructional materials.",2024-Apr-4,https://pubmed.ncbi.nlm.nih.gov/38596832,Pubmed,Foundation model,Non-Chat Agent,Single Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,,,,,general psychiatry,Unspecified,Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,,,
"Do patients prefer a human doctor, artificial intelligence, or a blend, and is this preference dependent on medical discipline? Empirical evidence and implications for medical practice","Today the doctor-patient relationship typically takes place in a face-to-face setting. However, with the advent of artificial intelligence (AI) systems, two further interaction scenarios are possible: an AI system supports the doctor's decision regarding diagnosis and/or treatment while interacting with the patient, or an AI system could even substitute the doctor and hence a patient interacts with a chatbot (i.e., a machine) alone. Against this background, we report on an online experiment in which we analyzed data from <i>N</i>=1,183 people. The data was collected in German-speaking countries (Germany, Austria, Switzerland). The participants were asked to imagine they had been suffering from medical conditions of unknown origin for some time and that they were therefore visiting a health center to seek advice from a doctor. We developed descriptions of patient-doctor interactions (referred to as vignettes), thereby manipulating the patient's interaction partner: (i) human doctor, (ii) human doctor with an AI system, and (iii) an AI system only (i.e., chatbot). Furthermore, we manipulated medical discipline: (i) cardiology, (ii) orthopedics, (iii) dermatology, and (iv) psychiatry. Based on this 34 experimental within-subjects design, our results indicate that people prefer a human doctor, followed by a human doctor with an AI system, and an AI system alone came in last place. Specifically, based on these 12 hypothetical interaction situations, we found a significant main effect of a patient's interaction partner on trust, distrust, perceived privacy invasion, information disclosure, treatment adherence, and satisfaction. Moreover, perceptions of trust, distrust, and privacy invasion predicted information disclosure, treatment adherence, and satisfaction as a function of interaction partner and medical discipline. We found that the situation in psychiatry is different from the other three disciplines. Specifically, the six outcome variables differed strongly between psychiatry and the three other disciplines in the ""human doctor with an AI system"" condition, while this effect was not that strong in the other conditions (human doctor, chatbot). These findings have important implications for the use of AI in medical care and in the interaction between patients and their doctors.",8/12/24,https://pubmed.ncbi.nlm.nih.gov/39188871,Pubmed,unspecified,Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Not Applicable,6A7,"adolescence, young adulthood, middle adulthood, old","Austria, Germany, Switzerland",Both,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,,,8/12/24
Self-initiated humour protocol: a pilot study with an AI agent,"Non-hostile humour and laughter have been known for therapeutic benefits in an individual's mental health and wellbeing. To this end, we evaluated the Self-Initiated Humour Protocol (SIHP), a new type of self-administrable laughter intervention that utilises spontaneous and self-induced laughter. Rooted in the core principles of the Self-Attachment Technique-in which an individual creates an affectional bond with their childhood self as represented by their childhood photo or personalised childhood avatar-SIHP provides an algorithmic framework for individuals to learn to laugh in a non-hostile manner and develop a sense of humour in all possible life contexts. This allows SIHP to be self-administered by interacting with an AI agent.",3/13/25,https://pubmed.ncbi.nlm.nih.gov/40182587,Pubmed,Rule-based models,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,"Text, Image",FALSE,FALSE,Not Applicable,Not Applicable,general psychiatry,"adolescence, young adulthood",Unspecified,Both,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,3/13/25
"AI Trauma Chatbot Analyzing Voice, Text and Providing Direct Doctor Link",,5/23/25,https://api.elsevier.com/content/abstract/scopus_id/105007424332,Scopus,Deep Learning Models,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,TRUE,,TRUE,"Text, Audio",TRUE,,,,general_psychiatry,age_unspecified,unspecified,Unspecified,TRUE,TRUE,TRUE,,,,,TRUE,TRUE,TRUE,,,,https://api.elsevier.com/content/abstract/scopus_id/105007424332,1/1/25
Implementing Cognitive Behavioral Therapy in Chatbots to Reduce Students' Exam Stress using ChatGPT,,1/20/25,https://api.elsevier.com/content/abstract/scopus_id/85218191086,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,TRUE,,TRUE,Text,FALSE,TRUE,,,6B0,"adolescence, young_adulthood",united_states,Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,,,https://api.elsevier.com/content/abstract/scopus_id/85218191086,1/1/24
AliceBot: Designing AI-Driven Applications for Mental Health Management,,31-May-25,https://api.elsevier.com/content/abstract/scopus_id/105007993371,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,,,TRUE,Text,TRUE,,,Interview (Questionnaire),6B0,"adolescence, young_adulthood",China,Unspecified,TRUE,,TRUE,,,TRUE,,,TRUE,,TRUE,TRUE,,https://api.elsevier.com/content/abstract/scopus_id/105007993371,1/1/25
SentimentCareBot: Retrieval-Augmented Generation Chatbot for Mental Health Support with Sentiment Analysis,,10/28/24,https://api.elsevier.com/content/abstract/scopus_id/85214969262,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,,,,TRUE,Text,TRUE,,,,general_psychiatry,age_unspecified,unspecified,Unspecified,,,TRUE,,,,,,TRUE,,,,,https://api.elsevier.com/content/abstract/scopus_id/85214969262,1/1/24
Assessing the use of ChatGPT as a psychoeducational tool for mental health practice,,25-Apr-24,https://api.elsevier.com/content/abstract/scopus_id/85191305839,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,,TRUE,,,general_psychiatry,age_unspecified,unspecified,Unspecified,,,TRUE,,,TRUE,,,,,TRUE,,,https://api.elsevier.com/content/abstract/scopus_id/85191305839,2025
Effectiveness of a Chatbot Single Session Intervention on Eating Disorders for People on the Waitlist for Treatment: Randomized Controlled Trial,,2025 May,https://api.elsevier.com/content/abstract/scopus_id/105006454493,Scopus,Rule-based models,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,,,,Interview (Questionnaire),general_psychiatry,"adolescence, young_adulthood, middle_adulthood",Australia,Both,FALSE,,TRUE,,,TRUE,FALSE,FALSE,TRUE,,,TRUE,,,
MentalRAG: Developing an Agentic Framework for Therapeutic Support Systems,,4/6/25,https://api.elsevier.com/content/abstract/scopus_id/105003533395,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Random Controlled Trial (RCT),"6A7, 6B0","adolescence, young_adulthood, middle_adulthood, old",united_states,Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,https://api.elsevier.com/content/abstract/scopus_id/105003533395,1/1/25
Integrating Sentiment Analysis to Enhance Mental Health Support Chatbots,,11/12/24,https://api.elsevier.com/content/abstract/scopus_id/85219083745,Scopus,Deep Learning Models,Chatbot,Single Agent,Not Applicable,,,,TRUE,TRUE,Text,TRUE,FALSE,,,general_psychiatry,age_unspecified,unspecified,Unspecified,,,TRUE,,,,,TRUE,TRUE,,,,,https://api.elsevier.com/content/abstract/scopus_id/85219083745,1/1/24
The effect of personalizing a ChatGPT based psychotherapy conversational agent on therapeutic alliance and usage intentions,,24-Mar,https://api.elsevier.com/content/abstract/scopus_id/85190965031,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,,Text,TRUE,,,Interview (Questionnaire),general_psychiatry,"adolescence, young_adulthood",Belgium,Both,FALSE,,TRUE,,,,,TRUE,,,,TRUE,,https://api.elsevier.com/content/abstract/scopus_id/85190965031,
"Constructing a Knowledge-Guided Mental Health Chatbot
with LLMs",,5-Sep-24,https://api.elsevier.com/content/abstract/scopus_id/85219571673,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,TRUE,,Text,TRUE,TRUE,,,general_psychiatry,age_unspecified,unspecified,Unspecified,TRUE,TRUE,TRUE,,,TRUE,,,TRUE,TRUE,TRUE,,,https://api.elsevier.com/content/abstract/scopus_id/85219571673,
A LLM-Based Chatbot for Mindfulness Practice with Older Adults: A Development and Usability Study,,5/15/25,https://api.elsevier.com/content/abstract/scopus_id/105005816806,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,,,,Systematic reviews / Meta analysis,general_psychiatry,old,sweden,Unspecified,,,TRUE,,,TRUE,,,,,,TRUE,,https://api.elsevier.com/content/abstract/scopus_id/105005816806,5/15/25
SabaiJai: A Buddhist AI Chatbot Innovation for Stress Resilience in Thailand's Working-Aged Population,,8-Feb-25,https://api.elsevier.com/content/abstract/scopus_id/105007181138,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,TRUE,FALSE,FALSE,"Text, Image, Video",FALSE,FALSE,Not Applicable,Not Applicable,6B4,"adolescence, young adulthood, middle adulthood, old",Thailand,Both,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,,https://api.elsevier.com/content/abstract/scopus_id/105007181138,
Beyond the Dialogue: Multi-chatbot Group Motivational Interviewing for Premenstrual Syndrome (PMS) Management,,4/26/25,https://api.elsevier.com/content/abstract/scopus_id/105005764465,Scopus,Foundation model,Chatbot,Multi-Agent,Team Architecture,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Not Applicable,6A7,"young adulthood, middle adulthood",Japan,Female only,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,,https://api.elsevier.com/content/abstract/scopus_id/105005764465,4/26/25
A Hybrid Dialogue System for Student Mental Health Assessment and Support,,3/11/25,https://api.elsevier.com/content/abstract/scopus_id/105000964854,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,FALSE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,6A7,"adolescence, young adulthood",vietnam,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,,,1/1/25
"LLM-Based Empathetic Response Through Psychologist-Agent Debate""",,28-Aug-24,https://api.elsevier.com/content/abstract/scopus_id/85203144116,Scopus,Foundation model,Chatbot,Multi-Agent,Team Architecture,FALSE,FALSE,FALSE,FALSE,TRUE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general psychiatry,Unspecified,Unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,https://api.elsevier.com/content/abstract/scopus_id/85203144116,
"Comparing the performance of ChatGPT GPT-4, Bard, and Llama-2 in the Taiwan Psychiatric Licensing Examination and in differential diagnosis with multi-center psychiatrists",,2024 Feb 26,https://api.elsevier.com/content/abstract/scopus_id/85186587080,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Interview (Questionnaire),general psychiatry,Unspecified,Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,https://api.elsevier.com/content/abstract/scopus_id/85186587080,
AI-Enhanced Cognitive Therapy: Personalized Guidance via Adaptive Agents with Voice Analysis and Stress Detection,,12/1/24,https://api.elsevier.com/content/abstract/scopus_id/85218412275,Scopus,"Foundation model, Deep Learning Models",Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,"Text, Audio, Video",TRUE,FALSE,Not Applicable,Interview (Questionnaire),"6A7, 6B0, 6B4",Unspecified,Unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,,https://api.elsevier.com/content/abstract/scopus_id/85218412275,1/1/24
Policies for Refugee Mental Health Using Generative Agents,,5/23/25,https://api.elsevier.com/content/abstract/scopus_id/105009239802,Scopus,Foundation model,Non-Chat Agent,Multi-Agent,Team Architecture,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,"6A7, 6B0, 6B4",Unspecified,Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,FALSE,FALSE,,https://api.elsevier.com/content/abstract/scopus_id/105009239802,5/23/25
Artificial Intelligence based Chatbot Therapist to Improve the Mental Health of Trauma Patients,,1/1/24,https://api.elsevier.com/content/abstract/scopus_id/85209101478,Scopus,"Small pre-trained model, Deep Learning Models",Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,"Text, Audio",FALSE,FALSE,Not Applicable,Interview (Questionnaire),6B4,Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,https://api.elsevier.com/content/abstract/scopus_id/85209101478,1/1/24
Multi-Tiered RAG-Based Chatbot for Mental Health Support,,5/29/25,https://api.elsevier.com/content/abstract/scopus_id/105007693321,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,TRUE,FALSE,TRUE,Text,TRUE,TRUE,Not Applicable,Systematic reviews  / Meta analysis,general psychiatry,Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,,https://api.elsevier.com/content/abstract/scopus_id/105007693321,1/1/25
Comparative Study on the Performance of LLM-based Psychological Counseling Chatbots via Prompt Engineering Techniques,,1/10/25,https://api.elsevier.com/content/abstract/scopus_id/85217283343,Scopus,Foundation model,Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,general psychiatry,Unspecified,Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,,https://api.elsevier.com/content/abstract/scopus_id/85217283343,1/1/24
Development and preliminary testing of a secure large language model-based chatbot for brief alcohol counseling in young adults,,2025 Apr 28,https://api.elsevier.com/content/abstract/scopus_id/105004284819,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,6C4,"adolescence, young adulthood",usa,Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,https://api.elsevier.com/content/abstract/scopus_id/105004284819,7/1/25
Integrating Psychology Into Supportive Chatbots for Mental Health using NLP,,4/1/25,https://api.elsevier.com/content/abstract/scopus_id/105007426188,Scopus,Deep Learning Models,Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,general psychiatry,Unspecified,Unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,https://api.elsevier.com/content/abstract/scopus_id/105007426188,1/1/25
Assessing bias in AI-driven psychiatric recommendations,,2025 Apr 15,https://api.elsevier.com/content/abstract/scopus_id/105002926603,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,TRUE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,6A7,"young_adulthood, middle_adulthood",unspecified,Unspecified,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,,https://api.elsevier.com/content/abstract/scopus_id/105002926603,
Evaluating the Feasibility and Acceptability of a GPT-Based Chatbot for Depression Screening,,14-Aug-24,https://api.elsevier.com/content/abstract/scopus_id/85201940027,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,6A7,"adolescence, young_adulthood, middle_adulthood",unspecified,Both,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,
A Reinforcement Learning Approach for Intelligent Conversational Chatbot For Enhancing Mental Health Therapy,,24-Jan,https://api.elsevier.com/content/abstract/scopus_id/85196359094,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,TRUE,,,Text,TRUE,,,,general_psychiatry,unspecified,unspecified,Unspecified,,,TRUE,,,,,,TRUE,TRUE,,,,https://api.elsevier.com/content/abstract/scopus_id/85196359094,
"Psychological Health Chatbot, Detecting and Assisting Patients in their Path to Recovery",,1/1/25,https://api.elsevier.com/content/abstract/scopus_id/105000178905,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,TRUE,,,Text,TRUE,,,Interview (Questionnaire),"6A7, 6B0, 6B4","adolescence, young_adulthood, middle_adulthood",iran,Both,TRUE,,TRUE,,,TRUE,,,TRUE,,TRUE,TRUE,,https://api.elsevier.com/content/abstract/scopus_id/105000178905,1/1/25
iCare: Harnessing AI and Emotional Intelligence to Revolutionize Accessible Mental Health Support,,2/27/25,https://api.elsevier.com/content/abstract/scopus_id/105000662974,Scopus,Machine Learning Models,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,,,,Text,TRUE,,,,general_psychiatry,unspecified,unspecified,Unspecified,TRUE,,TRUE,,,,,,TRUE,,,TRUE,,,1/1/24
Integrating AI in Psychotherapy: An Investigation of Trust in Voicebot Therapists,,13-Oct-24,https://api.elsevier.com/content/abstract/scopus_id/85206563716,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,TRUE,,Text,,,,,,,,,,,,,,,,,,,,,,,
MindWell: A Conversational Agent for Professional Depression Screening on Social Media,,3-Apr-25,https://api.elsevier.com/content/abstract/scopus_id/105006474278,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,,,,Text,TRUE,,,,6A7,unspecified,unspecified,Unspecified,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,,,,
Stress and Depression Detection by Chatbot,,5/17/25,https://api.elsevier.com/content/abstract/scopus_id/105006675124,Scopus,Machine Learning Models,Chatbot,Single Agent,Not Applicable,,TRUE,,,,Text,TRUE,,,,6A7,unspecified,unspecified,Unspecified,TRUE,,TRUE,,,,,,TRUE,,,,,https://api.elsevier.com/content/abstract/scopus_id/105006675124,1/1/25
Mental Chatbot Application Using Retrieval Augmented Generation,,12/15/24,https://api.elsevier.com/content/abstract/scopus_id/85213368237,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,,,,Text,TRUE,,,,general_psychiatry,"adolescence, young_adulthood",unspecified,Unspecified,TRUE,,TRUE,,,,,,,,,TRUE,,https://api.elsevier.com/content/abstract/scopus_id/85213368237,1/1/24
Does the AI-driven Chatbot Work? Effectiveness of the Woebot app in reducing anxiety and depression in group counseling courses and student acceptance of technological aids,,25-Jan-25,https://api.elsevier.com/content/abstract/scopus_id/85217240955,Scopus,Rule-based models,Chatbot,Non-Agent,Not Applicable,,TRUE,,,,Text,,,,Systematic reviews  / Meta analysis,general_psychiatry,,,,,,TRUE,,,TRUE,,,TRUE,,,TRUE,,https://api.elsevier.com/content/abstract/scopus_id/85217240955,
Development of a Chatbot Powered by Artificial Intelligence to Diagnose and Improve Stress and Anxiety Levels in University Students,,14-Jan-25,https://api.elsevier.com/content/abstract/scopus_id/85217215418,Scopus,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,,"Text, Audio",TRUE,,,,"6B0, 6B4",,,,TRUE,,TRUE,,,,,,,,,TRUE,,https://api.elsevier.com/content/abstract/scopus_id/85217215418,
Collaborative Agents for Anxious Depression Diagnosis,,14-Apr-25,https://api.elsevier.com/content/abstract/scopus_id/105004363169,Scopus,Deep Learning Models,Chatbot,Single Agent,Not Applicable,,TRUE,,TRUE,,"Text, Image",FALSE,FALSE,Lab Test,Interview (Questionnaire),"6A7, 6B0","adolescence, young_adulthood",China,Both,TRUE,,TRUE,,,,,,TRUE,TRUE,,TRUE,,https://api.elsevier.com/content/abstract/scopus_id/105004363169,
A GPT-4o-powered framework for identifying cognitive impairment stages in electronic health records,"Alzheimers Disease and Related Dementias (ADRD) pose a major public health challenge, with a critical need for accurate and scalable tools for detecting cognitive impairment (CI). Readily available electronic health records (EHRs) contain valuable cognitive health data, but much of it is embedded in unstructured clinical notes. To address this problem, we developed a GPT-4o-powered framework for CI stage classification, leveraging longitudinal patient history summarization, multi-step reasoning, and confidence-aware decision-making. Evaluated on 165,926 notes from 1002 Medicare patients from Mass General Brigham (MGB), our GPT-4o framework achieved high accuracy in CI stage classification (weighted Cohens kappa=0.95, Spearman correlation=0.93), and outperformed two other language models (weighted Cohens kappa 0.820.85). Our framework also achieved high performance on Clinical Dementia Rating (CDR) scoring on an independent dataset of 769 memory clinic patients (weighted Cohens kappa=0.83). Finally, to ensure reliability and safety, we designed an interactive AI agent integrating our GPT-4o-powered framework and clinician oversight. This collaborative approach has the potential to facilitate CI diagnoses in real-world clinical settings.",7/3/25,http://dx.doi.org/10.1038/s41746-025-01834-5,Springer,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,FALSE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,EHRs,Not Applicable,6D7,old,United States (Massachusetts General Brigham),Both,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,,,7/3/25
Artificial intelligence-assisted chatbot: impact on breastfeeding outcomes and maternal anxiety,"['Background', 'Artificial intelligence (AI) is increasingly used in healthcare interventions to provide accessible, continuous, and personalized patient support. This study investigates the impact of a mobile breastfeeding counseling application developed with artificial AI on mothers breastfeeding self-efficacy, success, and anxiety levels.', 'Methods', 'A quasi-experimental design was employed, involving 60 mothers. Participants were divided into two groups: 30 mothers received AI-based counseling, and 30 mothers were provided a booklet. Data collection tools included a personal information form, Breastfeeding Charting System and Assessment Tool (LATCH), Postnatal Breastfeeding Self-Efficacy Scale, and Beck Anxiety Inventory. Data were collected from mothers who delivered at a state hospitals obstetrics and gynecology department and were followed for ten days postpartum (postpartum days 1, 3, 7, and 10).', 'Results', 'No significant differences were found in the demographic characteristics of the two groups ( p \u2009>\u20090.05). Statistically significant improvements were observed in breastfeeding self-efficacy over time for both groups (AI group: f\u2009=\u200936.356, p \u2009=\u20090.000; booklet group: f\u2009=\u200943.349, p \u2009=\u20090.000). At day 10, the AI group scored significantly higher than the booklet group (Z=-2.216, p \u2009=\u20090.027). For breastfeeding success, as measured by the LATCH tool, significant differences were also noted over time for both groups (AI group: f\u2009=\u200968.466, p \u2009=\u20090.000; booklet group: f\u2009=\u200968.088, p \u2009=\u20090.000). At day seven, the AI group outperformed the booklet group (Z=-2.995, p \u2009=\u20090.003). Anxiety levels showed no significant differences between groups.', 'Conclusions', 'AI-based breastfeeding counseling positively impacts breastfeeding self-efficacy and success. The findings highlight the potential of AI applications in healthcare. AI-based chatbots can serve as effective tools for breastfeeding education, offering accessible, personalized, and continuous support. The significant improvements in breastfeeding outcomes indicate that innovative AI-assisted interventions can effectively support mothers during the critical early postpartum period. This research demonstrates the feasibility of integrating AI technology into maternal care and serves as a foundation for future studies.', 'Clinical trial number', 'Not applicable.']",5/30/25,http://dx.doi.org/10.1186/s12884-025-07753-3,Springer,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,6.00E+02,"adolescence, young_adulthood",Turkey,Female only,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,,,5/30/25
Exploring the potential of lightweight large language models for AI-based mental health counselling task: a novel comparative study,"In recent years, Transformer-based large language models (LLMs) have significantly improved upon their text generation capability. Mental health is a serious concern that can be addressed using LLM-based automated mental health counselors. These systems can provide empathetic responses to individuals in need while considering the negative beliefs, stigma, and taboos associated with mental health issues. Considering the large size of these LLMs makes it difficult to deploy these automated counselors on low cost/resource devices such as edge devices. Therefore, the motivation of the present study to analyze the effectiveness of lightweight LLMs in the development of automated mental health counseling systems. In this study, lightweight open source LLMs such as Googles T5_s (small variant), BART_B (base variant), FLAN-T5_s (small variant), and Microsofts GODEL_B (base variant) have been fine-tuned for automated mental health counseling task utilizing a diverse set of datasets publicly available online. The experimental results reveal that BARTs base variant outperformed the other models across all key metrics such as ROUGE-1, ROUGE-2, ROUGE-L, and BLEU with scores of 0.4727, 0.2665, 0.3554, and 25.3993 respectively. In comparison to other models, BART-base model generated empathetic, and emotionally supportive responses. These findings highlight the potential of lightweight LLMs (small size LLMs), in advancing the field of LLM-based mental health counseling solutions and underscore the need for exploration of lightweight LLMs for this mental health counseling use case. The code for this work is available at the following link: https://github.com/diviitmg03/Comparative-analysis-of-LLMs-.git .",7/2/25,http://dx.doi.org/10.1038/s41598-025-05012-1,Springer,Small pre-trained model,Chatbot,,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,general psychiatry,Unspecified,Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,,,7/2/25
"Psychological, economic, and ethical factors in human feedback for a chatbot-based smoking cessation intervention","Integrating human support with chatbot-based behavior change interventions raises three challenges: (1) attuning the support to an individuals state (e.g., motivation) for enhanced engagement, (2) limiting the use of the concerning human resources for enhanced efficiency, and (3) optimizing outcomes on ethical aspects (e.g., fairness). Therefore, we conducted a study in which 679 smokers and vapers had a 20% chance of receiving human feedback between five chatbot sessions. We find that having received feedback increases retention and effort spent on preparatory activities. However, analyzing a reinforcement learning (RL) model fit on the data shows there are also states where not providing feedback is better. Even this standard benefit-maximizing RL model is value-laden. It not only prioritizes people who would benefit most, but also those who are already doing well and want feedback. We show how four other ethical principles can be incorporated to favor other smoker subgroups, yet, interdependencies exist.",5/31/25,http://dx.doi.org/10.1038/s41746-025-01701-3,Springer,Machine Learning Models,Chatbot,Single Agent,,,TRUE,,,,Text,TRUE,,,Interview (Questionnaire),6C4,"adolescence, young_adulthood, middle_adulthood, old",unspecified,Both,FALSE,,TRUE,,,TRUE,,,TRUE,,,TRUE,,,5/31/25
ChatGPT Helps Students Feign ADHD: An Analogue Study on AIAssisted Coaching,"This preregistered study aimed to assess whether AI-generated coaching helps students to successfully feign attention-deficit/hyperactivity disorder (ADHD) in adulthood. First, based on questions generated by 22 students, we conducted an extensive ChatGPT query to develop a concise AI-generated information sheet designed to coach students in feigning ADHD during a clinical assessment. Second, we evaluated the effect of this coaching in an experimental analogue study in which 110 university students were randomly assigned to one of three groups: (1) a control group ( n =42), (2) an ADHD symptomcoached simulation group ( n =35), and (3) an AI-coached simulation group ( n =33). All participants underwent a clinical neuropsychological assessment that included measures of ADHD symptoms, functional impairments, selective attention, and working memory. Our preregistered data analysis revealed that the AI-coached simulation group consistently moderated their symptom overreporting and cognitive underperformance compared to the symptom-coached group in small to medium size, resulting in lower detection sensitivity. We conclude that publicly accessible AI tools, such as current versions of chatbots, can provide clear and effective strategies for feigning ADHD during clinical neuropsychological assessments, posing a significant threat to the validity assessments. We recommend that researchers and clinicians exercise caution when sharing assessment materials, example items, and scoring methodologies.",25-Mar,http://dx.doi.org/10.1007/s12207-025-09538-7,Springer,Foundation model,Chatbot,Single Agent,Not Applicable,,,,,TRUE,Text,,TRUE,,,6A0,adolescence,netherlands,Both,,,,TRUE,,FALSE,,TRUE,FALSE,,,TRUE,,,
Integrating AI into ADHD Therapy: Insights from ChatGPT-4o and Robotic Assistants,"Traditional attention-deficit/hyperactivity disorder (ADHD) interventions often struggle with scalability, requiring significant human resources to ensure individualized attention and real-time adaptability, as many existing tools rely on fixed content that cannot adjust to fluctuating user behaviors. Additionally, most current methods lack multi-modal interaction, limiting their ability to engage children effectively across speech, text, and visual feedback. This study explores the integration of ChatGPT-4o with robotic assistants to enhance personalization, engagement, and adaptability in ADHD therapy. Unlike pre-scripted or static therapy tools, ChatGPT-4o enables dynamic, real-time interactions tailored to individual needs, providing a more engaging and adaptable therapeutic experience. The resultsvalidate its potential to deliver personalized, scalable, and responsive therapeutic support through AI-driven robotic assistants. Technical validation demonstrated inclusivity across diverse user profiles, achieving a 93% success rate in tailored interactions and maintaining stability in 91% of extreme input scenarios. Cross-platform tests confirmed high accuracy, while scalability assessments validated effective handling of multi-user interactions. Clinical evaluations over eight weeks indicated potential improvements in attention span (+ 28%) and reductions in hyperactivity ( 22%), with high satisfaction ratings from therapists, caregivers, and teachers. These results establish ChatGPT as a promising tool for ADHD therapy, enhancing engagement, personalization, and scalability. Future directions include optimizing for younger users, deploying to real-world environments, and further interdisciplinary advancements to refine AI-driven therapeutic solutions.",5/25/25,http://dx.doi.org/10.1007/s44230-025-00099-1,Springer,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,TRUE,"Text, Audio, Image, video",TRUE,TRUE,EHRs,Not Applicable,6A0,"childhood, adolescence",unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,,5/25/25
AI-driven multi-agent reinforcement learning framework for real-time monitoring of physiological signals in stress and depression contexts,"['Purpose', 'Effective patient monitoring is crucial for timely healthcare interventions and improved outcomes, especially in managing conditions influenced by stress and depression, which can manifest through physiological changes. Traditional monitoring systems often struggle with the complexity and dynamic nature of such conditions, leading to delays in identifying critical scenarios. This study proposes a novel multi-agent deep reinforcement learning (DRL) framework to address these challenges by monitoring vital signs and providing real-time decision-making capabilities.', 'Methods', 'Our framework deploys multiple learning agents, each dedicated to monitoring specific physiological features such as heart rate, respiration, and temperature. These agents interact with a generic healthcare monitoring environment, learn patients behavior patterns, and estimate the level of emergency to alert Medical Emergency Teams (METs) accordingly. The study evaluates the proposed system using two real-world datasets-PPG-DaLiA and WESAD-designed to capture physiological and stress-related data. The performance is compared with baseline models, including Q-Learning, PPO, Actor-Critic, Double DQN, and DDPG, as well as existing monitoring frameworks like WISEML and CA-MAQL. Hyperparameter optimization is also performed to fine-tune learning rates and discount factors.', 'Results', 'Experimental results demonstrate that the proposed multi-agent DRL framework outperforms baseline models in accurately monitoring patients vital signs under stress and varying conditions. The optimized agents adapt effectively to dynamic environments, ensuring timely detection of critical health deviations. Comparative evaluations reveal superior performance in metrics related to decision-making accuracy and response efficiency, highlighting the robustness of the framework.', 'Conclusions', 'The proposed AI-driven monitoring system offers significant advancements over traditional methods by handling complex and uncertain environments, adapting to varying patient conditions influenced by stress and depression, and making autonomous, real-time decisions. While the framework demonstrates high accuracy and adaptability, challenges related to data scale and future vital sign prediction remain. Future research will focus on extending predictive capabilities to further enhance proactive healthcare interventions.']",6/9/25,http://dx.doi.org/10.1186/s40708-025-00262-1,Springer,Deep Learning Models,Non-Chat Agent,Multi-Agent,Flat Architecture,FALSE,TRUE,TRUE,FALSE,FALSE,"Text, Others",FALSE,FALSE,EHRs,Not Applicable,"6A7, 6B4",age_unspecified,unspecified,Unspecified,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,6/9/25
"Enhancing academic stress assessment through self-disclosure chatbots: effects on engagement, accuracy, and self-reflection","Academic stress significantly affects students well-being and academic performance, highlighting the need for more effective assessment methods to guide targeted interventions. This study investigates how self-disclosure chatbotsdesigned to share relevant experiences and thoughtscan enhance academic stress assessments by increasing student engagement, improving accuracy, and fostering deeper self-reflection. Two chatbot conditions were developed: a self-disclosure (SD) chatbot that used personal narratives to build empathy, and a non-self-disclosure (NSD) chatbot. In a randomized experiment with 50 university students, participants interacted with either the SD or NSD chatbot. Results showed that the SD chatbot elicited significantly higher engagement, as evidenced by longer session lengths (15.555.92min) and higher word counts (240114.02 words), compared to the NSD chatbot (11.315.21min; 162.3866.24 words). Assessment accuracyevaluated by comparing results from the SISCO Inventory of Academic Stress with chatbot-generated evaluationswas slightly higher for the SD chatbot (0.936) than for the NSD chatbot (0.862), based on accuracy within a one-point deviation. Moreover, students who interacted with the SD chatbot reported deeper self-reflection and developed more actionable strategies for managing their stress. Overall, these findings illuminate the value of self-disclosure in chatbot-based assessments and highlight broader applications for addressing academic stress and mental health challenges in educational settings.",9-May-25,http://dx.doi.org/10.1186/s41239-025-00527-z,Springer,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,TRUE,Text,TRUE,FALSE,Not Applicable,Interview (Questionnaire),6B0,adolescence,South Korea,Both,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,
Independent Clinical Evaluation of General-Purpose LLM Responses to Signals of Suicide Risk,"['Nick Judd', 'Alexandre Vaz', 'Kevin Paeth', 'Layla Ins Davis', 'Milena Esherick', 'Jason Brand', 'Ins Amaro', 'Tony Rousmaniere']",10/31/25,https://arxiv.org/abs/2510.27521v1,arXiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,FALSE,TRUE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,"6A7, 6B0, 6B4",Middle Adulthood,United States,Both,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,10/31/25
Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening,"['Zhijun Guo', 'Alvina Lai', 'Julia Ive', 'Alexandru Petcu', 'Yutong Wang', 'Luyuan Qi', 'Johan H Thygesen', 'Kezhi Li']",2025-07-08T13:41:22Z,https://arxiv.org/abs/2507.05984v1,arXiv,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,FALSE,"Text, Audio",TRUE,TRUE,Not Applicable,Not Applicable,6A7,"Adolescence, Young Adulthood, Middle Adulthood, Old","United Kingdom, China",Both,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,,,2025-07-08T13:41:22Z
Talking to an AI Mirror: Designing Self-Clone Chatbots for Enhanced Engagement in Digital Mental Health Support,"['Mehrnoosh Sadat Shirvani', 'Jackie Liu', 'Thomas Chao', 'Suky Martinez', 'Laura Brandt', 'Ig-Jae Kim', 'Dongwook Yoon']",9/8/25,https://arxiv.org/abs/2509.06393v1,arXiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,General Psychiatry,"Young Adulthood, Middle Adulthood",Unspecified,Both,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,,,9/8/25
Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System,"['Yuanchen Shi', 'Longyin Zhang', 'Fang Kong']",2025-07-10T07:56:35Z,https://arxiv.org/abs/2507.07509v1,arXiv,Foundation model,Chatbot,Multi-Agent,Team Architecture,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,General Psychiatry,"Childhood, Adolescence, Young Adulthood, Middle Adulthood, Old",China,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,TRUE,TRUE,,,2025-07-10T07:56:35Z
Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System,"['Eranga Bandara', 'Ross Gore', 'Atmaram Yarlagadda', 'Anita H. Clayton', 'Preston Samuel', 'Christopher K. Rhea', 'Sachin Shetty']",2025-10-29T14:54:22Z,https://arxiv.org/abs/2510.25588v1,arXiv,Foundation model,Non-Chat Agent,Multi-Agent,Hybrid Architecture,FALSE,FALSE,TRUE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,2025-10-29T14:54:22Z
MindEval: Benchmarking Language Models on Multi-turn Mental Health Support,"['Jos Pombal', ""Maya D'Eon"", 'Nuno M. Guerreiro', 'Pedro Henrique Martins', 'Antnio Farinhas', 'Ricardo Rei']",11/23/25,https://arxiv.org/abs/2511.18491v3,arXiv,Foundation model,Chatbot,Multi-Agent,Flat Architecture,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,"6A7, 6B0","Adolescence, Young Adulthood, Middle Adulthood, Old",Unspecified,Both,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,,,11/23/25
Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias,"['Rushia Harada', 'Yuken Kimura', 'Keito Inoshita']",2025-07-15T11:27:32Z,https://arxiv.org/abs/2507.11210v2,arXiv,Foundation model,Chatbot,Multi-Agent,Team Architecture,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,6B4,Unspecified,Japan,Both,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,,,2025-07-15T11:27:32Z
LLM Agent-Based Simulation of Student Activities and Mental Health Using Smartphone Sensing Data,"['Wayupuk Sommuang', 'Kun Kerdthaisong', 'Pasin Buakhaw', 'Aslan B. Wong', 'Nutchanon Yongsatianchot']",2025-07-17T03:30:11Z,https://arxiv.org/abs/2508.02679v2,arXiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,TRUE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Interview (Questionnaire),General Psychiatry,Adolescence,United States,Unspecified,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,,,2025-07-17T03:30:11Z
Kardia-R1: Unleashing LLMs to Reason toward Understanding and Empathy for Emotional Support via Rubric-as-Judge Reinforcement Learning,"['Jiahao Yuan', 'Zhiqing Cui', 'Hanqing Wang', 'Yuansheng Gao', 'Yucheng Zhou', 'Usman Naseem']",2025-12-01T04:54:03Z,https://arxiv.org/abs/2512.01282v2,arXiv,Foundation model,Chatbot,Non-Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,General Psychiatry,Unspecified,Unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,2025-12-01T04:54:03Z
MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn Mental Health Counseling Sessions,"['Aishik Mandal', 'Tanmoy Chakraborty', 'Iryna Gurevych']",2025-09-04T12:59:24Z,https://arxiv.org/abs/2509.04183v1,arXiv,Foundation model,Non-Chat Agent,Multi-Agent,Hybrid Architecture,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,"6A7, 6B0",Unspecified,Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,,,2025-09-04T12:59:24Z
Investigating AI in Peer Support via Multi-Module System-Driven Embodied Conversational Agents,"['Ruoyu Wen', 'Xiaoli Wu', 'Kunal Gupta', 'Simon Hoermann', 'Mark Billinghurst', 'Alaeddin Nassani', 'Dwain Allan', 'Thammathip Piumsomboon']",2025-11-27T09:47:18Z,https://arxiv.org/abs/2511.22269v1,arXiv,Foundation model,Chatbot,Multi-Agent,Flat Architecture,TRUE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,General Psychiatry,"Adolescence, Young Adulthood",Unspecified,Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,2025-11-27T09:47:18Z
Bloom: Designing for LLM-Augmented Behavior Change Interactions,"['Matthew Jrke', 'Defne Gen', 'Valentin Teutschbein', 'Shardul Sapkota', 'Sarah Chung', 'Paul Schmiedmayer', 'Maria Ines Campero', 'Abby C. King', 'Emma Brunskill', 'James A. Landay']",2025-10-06T23:31:18Z,https://arxiv.org/abs/2510.05449v1,arXiv,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,TRUE,FALSE,"Text, Audio, Images",TRUE,TRUE,Not Applicable,Not Applicable,6B4,"Adolescence, Young Adulthood, Middle Adulthood, Old",United States,Both,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,TRUE,,,2025-10-06T23:31:18Z
AI-Powered Chatbot for Mental Health Assistance,,10/1/25,https://openalex.org/W4414724326,OpenAlex,Foundation model,Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Not Applicable,General Psychiatry,"Adolescence, Young Adulthood",United States,Both,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,10/1/25
DialogGuard: Multi-Agent Psychosocial Safety Evaluation of Sensitive LLM Responses,,12/1/25,https://openalex.org/W4417004206,OpenAlex,Deep Learning Models,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Not Applicable,"6A7, 6B0, 6B4, 6A80, 6C4, 6B6","Adolescence, Young Adulthood",India,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,,,12/1/25
AI-Driven Mental Health Screening Using NLP Chatbots and Vision Transformers,,10/3/25,https://openalex.org/W4414792612,OpenAlex,Foundation model,Non-Chat Agent,Multi-Agent,Hybrid Architecture,FALSE,FALSE,TRUE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,6A7,Unspecified,Unspecified,Unspecified,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,,,10/3/25
Cultural Prompting Improves the Empathy and Cultural Responsiveness of GPT-Generated Therapy Responses,,10/19/25,https://openalex.org/W4416931997,OpenAlex,"Deep Learning Models, Small pre-trained model",Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,"Text, Audio, Images",TRUE,FALSE,Not Applicable,Interview (Questionnaire),6A7,Unspecified,Unspecified,Unspecified,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,,,10/19/25
Mental Health Support System,,10/23/25,https://openalex.org/W4415477278,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Not Applicable,General Psychiatry,"Adolescence, Young Adulthood, Middle Adulthood, Old",United States,Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,10/23/25
VERA-MH Concept Paper,,10/17/25,https://openalex.org/W4416245556,OpenAlex,Rule-based models,Chatbot,Non-Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Interview (Questionnaire),"6A7, 6B0, 6B4","Adolescence, Young Adulthood",Unspecified,Unspecified,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,,,10/17/25
ZenLoop: An AI-Powered Mental Health Platform,,8/25/25,https://openalex.org/W4413524629,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,"6A7, 6B0","Adolescence, Young Adulthood, Middle Adulthood, Old",Unspecified,Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,,,8/25/25
Exploratory Research on the Potential of HumanAI Interaction for Mental Health: Building and Verifying an Experimental Environment Based on ChatGPT and Metaverse,,19-Oct-25,,OpenAlex,Small pre-trained model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,TRUE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,,,
HUMAN-AI COLLABORATION IN MATERNAL MENTAL HEALTH: A HYBRID APPROACH TO REDUCING ANXIETY IN EXPECTANT AND NURSING MOTHERS,,11/24/25,https://openalex.org/W7106854405,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,"Text, Audio,Image",FALSE,TRUE,Not Applicable,Not Applicable,"6A7, 6B0, 6B4","Adolescence, Young Adulthood",Japan,Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,11/24/25
Using Nonverbal Cues in Empathic Multi-Modal LLM-Driven Chatbots for Mental Health Support,,9-Sep-25,https://dl.acm.org/doi/10.1145/3743724,OpenAlex,Rule-based models,Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,6B0,"Adolescence, Young Adulthood, Middle Adulthood",Unspecified,Female only,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,,,2025
Tweaking the Messages and Approaching the Glass Box: Using AI Chatbots to Promote Help-Seeking for Depressive Symptoms,,10/18/25,https://openalex.org/W4415312902,OpenAlex,unspecified(emol),Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,"Text, Audio,Image",TRUE,FALSE,Not Applicable,Not Applicable,6A7,Adolescence,Japan,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,10/18/25
MINDWELL AN AI-POWERED MENTAL HEALTH COMPANION,,11/26/25,https://openalex.org/W7107964138,OpenAlex,unspecified(emol),Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,6A7,"Young Adulthood, Middle Adulthood, Old",United States,Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,11/26/25
Large language models and retrieval-augmented generation-based chatbot for adolescent mental health,,9/1/25,https://openalex.org/W4414218383,OpenAlex,"Rule-based models, Deep Learning Models",Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,FALSE,"Text, Audio",FALSE,TRUE,Not Applicable,Not Applicable,General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,,,9/1/25
AI Powered Mental Health Chatbots,,11/14/25,https://openalex.org/W4416248858,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,FALSE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Systematic reviews  / Meta analysis,"6A7, 6B0",Adolescence,Indonesia,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,,,11/14/25
Evaluating LLM Safety Across Child Development Stages: A Simulated Agent Approach,,10/7/25,https://openalex.org/W4414977043,OpenAlex,Small pre-trained model,Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,General Psychiatry,Unspecified,India,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,,,10/7/25
Apprentice bot model design and implementation for psychological clients' therapy,,24-Feb-25,https://link.springer.com/article/10.1007/s42452-025-06515-2,OpenAlex,Foundation model,Non-Chat Agent,Multi-Agent,Flat Architecture,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,General Psychiatry,Childhood,Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,2025
A Comparison of Responses from Human Therapists and Large Language ModelBased Chatbots to Assess Therapeutic Communication: Mixed Methods Study,,21.May.2025,https://mental.jmir.org/2025/1/e69709,WoS,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,General Psychiatry,"Adolescence, Young Adulthood",China,Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,
Does the AI-driven Chatbot Work? Effectiveness of the Woebot app in reducing anxiety and depression in group counseling courses and student acceptance of technological aids,,5/1/25,,WoS,Foundation model,Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Not Applicable,General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,5/1/25
Effect of an AI agent trained on a large language model (LLM) as an intervention for depression and anxiety symptoms in young adults: A 28-day randomized controlled trial,,2025 Oct,https://pubmed.ncbi.nlm.nih.gov/40910958/,WoS,Rule-based models,Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,"6A7, 6B0","Adolescence, Young Adulthood",Taiwan,Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,2025
"Deploying a Mental Health Chatbot in Higher Education: The Development and Evaluation of Luna, an AI-Based Mental Health Support System",,6/10/25,https://doi.org/10.3390/computers14060227,WoS,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,FALSE,FALSE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,"6A7, 6B0","Adolescence, Young Adulthood",China,Both,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,6/10/25
Evaluating LLM Safety Across Child Development Stages: A Simulated Agent Approach,,10/7/25,,WoS,Foundation model,Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,FALSE,"Text,Audio,Video",FALSE,FALSE,Not Applicable,Interview (Questionnaire),6B4,"Adolescence, Young Adulthood, Middle Adulthood",Italy,Female only,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,,,10/7/25
Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered Mental Health Chatbots,"['Chenhan Lyu', 'Yutong Song', 'Pengfei Zhang', 'Amir M. Rahmani']",9/19/25,https://arxiv.org/abs/2509.16444v1,arXiv,Foundation model,Non-Chat Agent,Single Agent,Not Applicable,FALSE,FALSE,TRUE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,General Psychiatry,"Childhood, Adolescence",Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,9/19/25
Between Help and Harm: An Evaluation of Mental Health Crisis Handling by LLMs,"['Adrian Arnaiz-Rodriguez', 'Miguel Baidal', 'Erik Derner', 'Jenn Layton Annable', 'Mark Ball', 'Mark Ince', 'Elvira Perez Vallejos', 'Nuria Oliver']",9/29/25,https://arxiv.org/abs/2509.24857v2,arXiv,Small pre-trained model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,TRUE,FALSE,FALSE,Text,TRUE,FALSE,EHRs,Not Applicable,General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,,,9/29/25
Collective Voice: Recovered-Peer Support Mediated by An LLM-Based Chatbot for Eating Disorder Recovery,"['Ryuhaerang Choi', 'Taehan Kim', 'Subin Park', 'Seohyeon Yoo', 'Jennifer G. Kim', 'Sung-Ju Lee']",9/18/25,https://arxiv.org/abs/2509.15289v1,arXiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,FALSE,TRUE,FALSE,FALSE,Text,TRUE,TRUE,EHRs,Interview (Questionnaire),6B8,Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,,,9/18/25
Mitigating Harmful Erraticism in LLMs Through Dialectical Behavior Therapy Based De-Escalation Strategies,"['Pooja Rangarajan', 'Jacob Boyle']",2025-09-06T11:20:15Z,https://arxiv.org/abs/2510.15889v1,arXiv,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,TRUE,Text,TRUE,FALSE,Not Applicable,Interview (Questionnaire),6B8,"Adolescence, Young Adulthood",Unspecified,Female only,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,,,2025-09-06T11:20:15Z
TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling,"['He Hu', 'Yucheng Zhou', 'Chiyuan Ma', 'Qianning Wang', 'Zheng Zhang', 'Fei Ma', 'Laizhong Cui', 'Qi Tian']",2025-10-29T17:54:20Z,https://arxiv.org/abs/2510.25758v1,arXiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,FALSE,TRUE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,2025-10-29T17:54:20Z
PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian,"['Mohammad Amin Abbasi', 'Hassan Naderi']",2025-10-04T19:40:10Z,https://arxiv.org/abs/2510.03913v1,arXiv,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,TRUE,FALSE,FALSE,Text,FALSE,FALSE,EHRs,Case Series and Reports ,General Psychiatry,Unspecified,China,Unspecified,TRUE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,,,2025-10-04T19:40:10Z
CLINPREAI: AN AGENTIC AI SYSTEM FOR EARLY POSTPARTUM DEPRESSION RISK PREDICTION FROM MULTIMODAL EHR DATA,,11/18/25,https://www.medrxiv.org/content/10.1101/2025.11.14.25340265v1,arXiv,Small pre-trained model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,General Psychiatry,Unspecified,Iran,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,,,11/18/25
Evaluating Voice-Enabled Generative AI for Mental Health: Real-Time Performance and Safety Analyses,"Ngo, N.; Sano, A.",11/17/25,https://doi.org/10.1101/2025.11.14.25340246,medRxiv,Machine Learning Models,Non-Chat Agent,Single Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,TRUE,Text,TRUE,FALSE,EHRs,Not Applicable,6A7,"Adolescence, Young Adulthood, Middle Adulthood",United States,Female only,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,11/17/25
MoPHES: Leveraging on-device LLMs as Agent for Mobile Psychological Health Evaluation and Support,,17-Oct-25,,medRxiv,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,TRUE,FALSE,FALSE,"Text, Audio",TRUE,FALSE,Not Applicable,Interview (Questionnaire),"6A7, 6B0",Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,
Development of Mental Health Prediction App for the Depression Assistance Based on AI Chatbot,,7/4/25,https://openalex.org/W4415745379,OpenAlex,Small pre-trained model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Interview (Questionnaire),"6A7, 6B0",Unspecified,China,Unspecified,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,,,7/4/25
MindEval: Benchmarking Language Models on Multi-turn Mental Health Support,,11/23/25,https://openalex.org/W7106783787,OpenAlex,Machine Learning Models,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,6A7,Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,11/23/25
Which voice aids mental health interactions? Exploring dual-pathway effects of voice anthropomorphism of AI health chatbots on patient anxiety,,12/4/25,https://openalex.org/W4416986379,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,TRUE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,"6A7, 6B0","Adolescence, Young Adulthood, Middle Adulthood, Old",Unspecified,Both,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,,,12/4/25
Using a Secondary Channel to Display the Internal Empathic Resonance of LLM-Driven Agents for Mental Health Support,,10/11/25,https://openalex.org/W4415065069,OpenAlex,Foundation model,Chatbot,Non-Agent,Not Applicable,FALSE,TRUE,FALSE,TRUE,FALSE,"Text, Audio, Video",TRUE,TRUE,Not Applicable,Interview (Questionnaire),6B0,"Adolescence, Young Adulthood, Middle Adulthood, Old",China,Both,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,10/11/25
Effectiveness of AI Chatbot in reducing Anxiety Levels in a Community Mental Health Center,,,,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Interview (Questionnaire),6B0,"Adolescence, Young Adulthood, Middle Adulthood","European Economic Area, North America, Asia-Pacific Region",Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,
Safety Evaluation of a Clinical-Grade Generative AI Agent for Anxiety and Depression Symptoms,,9/27/25,https://openalex.org/W4414559103,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,TRUE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,6B0,"Adolescence, Young Adulthood, Middle Adulthood",Peru,Both,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,9/27/25
Mood Mate: Advancing Real-Time Mental Health Monitoring with Multimodal AI,,11/17/25,https://openalex.org/W4416286813,OpenAlex,Foundation model,Chatbot,Multi-Agent,Hybrid Architecture,TRUE,TRUE,TRUE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Interview (Questionnaire),"6A7, 6B0","Adolescence, Young Adulthood, Middle Adulthood, Old",United States,Both,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,11/17/25
"On the Replication of Psychological Manipulation, Coercive Control, and Constraint Evasion in ChatGPT 5",,12/4/25,https://openalex.org/W7108997227,OpenAlex,Deep Learning Models,Chatbot,Non-Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,FALSE,"Text, Audio, Video, Images",TRUE,FALSE,Not Applicable,Not Applicable,"6A7, 6B0","Adolescence, Young Adulthood, Middle Adulthood, Old",,Both,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,,,12/4/25
Mental Health Chatbot,,,,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,FALSE,FALSE,FALSE,TRUE,Text,TRUE,FALSE,Not Applicable,Not Applicable,"6B0, 6A7, 6A6, 6D1, 6B4, 6A2",Unspecified,Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,
Performance of mental health chatbot agents in detecting and managing suicidal ideation,,8/27/25,https://openalex.org/W4413753732,OpenAlex,Unsure,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,TRUE,TRUE,Text,FALSE,TRUE,Not Applicable,Not Applicable,General Psychiatry,"Adolescence, Young Adulthood, Middle Adulthood",Unspecified,Unspecified,TRUE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,8/27/25
AI Chatbot for Mental Health Support,,8/7/25,https://openalex.org/W4413038414,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,FALSE,TRUE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,General Psychiatry,"Adolescence, Young Adulthood",Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,,,8/7/25
A Framework for ECA-Based Psychotherapy,,7/30/25,https://openalex.org/W4412743645,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,TRUE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Systematic reviews  / Meta analysis,General Psychiatry,"Adolescence, Young Adulthood, Middle Adulthood, Old",Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,,7/30/25
An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation,,7/11/25,https://openalex.org/W4414942721,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,FALSE,"Text, Audio, Video",TRUE,FALSE,Not Applicable,Interview (Questionnaire),"6A7, 6B0, 6B4",Unspecified,United States,Unspecified,TRUE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,7/11/25
"Exploring ChatGPT's Capabilities, Stability, Potential and Risks in Conducting Psychological Counseling through Simulations in School Counseling",,11/3/25,,OpenAlex,Small pre-trained model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Interview (Questionnaire),General Psychiatry,"Adolescence, Young Adulthood",Unspecified,Both,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,,,11/3/25
EmoCare: A Lightweight Emotion-Aware Chatbot for Mental Health Support using Multi-LLM Context Escalation,,9/10/25,https://openalex.org/W4415708267,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,TRUE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,General Psychiatry,Adolescence,Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,9/10/25
MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory,,8/10/25,https://openalex.org/W4415194259,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,TRUE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,,,8/10/25
PychoAgent: A Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events,,10/10/25,https://openalex.org/W7106788326,OpenAlex,Small pre-trained model,Non-Chat Agent,Non-Agent,Not Applicable,FALSE,FALSE,TRUE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Interview (Questionnaire),6B01,"Adolescence, Young Adulthood, Middle Adulthood, Old","United Kingdom, Sweden, United States",Both,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,10/10/25
From Evaluation to Enhancement: A Decision Support Framework for Quality Assurance in Therapeutic AI Systems (Preprint),,11/16/25,https://openalex.org/W4416702500,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,TRUE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,6B0,Unspecified,United States,Unspecified,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,11/16/25
Multimodal Emotion-Aware Conversational Agent for Mental Health Support Using Deep Learning and Generative AI,,11/17/25,https://openalex.org/W7105831267,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,General Psychiatry,"Adolescence, Young Adulthood",South Korea,Unspecified,TRUE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,,11/17/25
A Comprehensive Node.js-Based Mental Health Monitoring and Exercise Recommendation System with AI-Driven Emotional Support,,11/30/25,https://openalex.org/W7108068155,OpenAlex,Deep Learning Models,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,"Text, Images, Audio, Video, Signals",TRUE,TRUE,Not Applicable,Interview (Questionnaire),General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,11/30/25
AI-Driven Chatbot for Mental Health Analysis Using Transformer Models,,10/11/25,https://openalex.org/W4415352366,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,TRUE,FALSE,Text,FALSE,FALSE,Not Applicable,Not Applicable,General Psychiatry,"Adolescence, Young Adulthood, Middle Adulthood, Old",Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,10/11/25
Conversational health agents: a personalized large language model-powered agent framework,,7/3/25,https://openalex.org/W4412065574,OpenAlex,Small pre-trained model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Systematic reviews  / Meta analysis,General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,7/3/25
Prompt Engineering an Informational Chatbot for Education on Mental Health Using a Multiagent Approach for Enhanced Compliance With Prompt Instructions: Algorithm Development and Validation,,12/9/24,,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,FALSE,FALSE,"Text, Audio, Images",TRUE,FALSE,EHRs,Cohort Study,6B4,Adolescence,Unspecified,Unspecified,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,1/1/25
Emotion-aware psychological first aid: Integrating BERT-based emotional distress detection with Psychological First Aid-Generative Pre-Trained Transformer chatbot for mental health support,,1/25/25,,WoS,Foundation model,Chatbot,Multi-Agent,Hierarchical Architecture,TRUE,FALSE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Systematic reviews  / Meta analysis,General Psychiatry,Unspecified,Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,,,1/1/25
Psychological First Aid by AI: Proof-of-Concept and Comparative Performance of ChatGPT-4 and Gemini in Different Disaster Scenarios,,5/9/25,https://doi.org/10.1002/jclp.23808,WoS,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Interview (Questionnaire),General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,,1/1/25
Emotional Resonance and Self-Esteem: The Role of Humanlike Competencies From Mental Health Chatbots,,24-Oct-25,,WoS,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Interview (Questionnaire),6B4,"Childhood, Adolescence, Young Adulthood",Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,
The FAIIR conversational AI agent assistant for youth mental health service provision,,5/3/25,,WoS,Unspecified,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Interview (Questionnaire),General Psychiatry,Unspecified,China,Both,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,5/3/25
The effect of artificial intelligence applications GymBuddy & Elomia on the mental health and physical activities of teachers and learners exposed to e-learning platforms,,27-Jun-25,https://doi.org/10.1007/s12144-025-08015-3,WoS,Deep Learning Models,Chatbot,Non-Agent,Not Applicable,FALSE,FALSE,TRUE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,General Psychiatry,"Adolescence, Young Adulthood",Canada,Both,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,,,2025
Comparing perceived empathy and intervention strategies of an AI chatbot and human psychotherapists in online mental health support,,3/1/25,,WoS,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Interview (Questionnaire),General Psychiatry,"Adolescence, Young Adulthood",Unspecified,Unspecified,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,,3/1/25
Smart Health Companion: An AI-Powered Digital Assistant for Mental and Physical Well-Being,,25-Dec,https://openalex.org/W7117467952,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,TRUE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Not Applicable,General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,
AI-Based Mental Health Support System Using Natural Language Processing and Machine Learning,,21-Dec-25,https://openalex.org/W7116785726,OpenAlex,Small pre-trained model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Interview (Questionnaire),General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,
Mindful Journey: An AI-Powered Mental Health Support Platform Using Conversational Agents and Automated Journaling,,25-Dec,https://openalex.org/W7117497628,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,FALSE,TRUE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,
MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms Identification in Memes,,2025-12-31T18:06:21Z,,OpenAlex,Foundation model,Non-Chat Agent,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Systematic reviews  / Meta analysis,6A7,Unspecified,Unspecified,Unspecified,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,2025-12-31T18:06:21Z
From Visual Perception to Deep Empathy: An Automated Assessment Framework for House-Tree-Person Drawings Using Multimodal LLMs and Multi-Agent Collaboration,,2025-12-23T09:26:23Z,https://openalex.org/W7117552243,OpenAlex,Foundation model,Non-Chat Agent,Multi-Agent,Team Architecture,FALSE,TRUE,TRUE,TRUE,FALSE,"Text, Images",FALSE,FALSE,Not Applicable,Interview (Questionnaire),General Psychiatry,"Adolescence, Young Adulthood, Middle Adulthood",China,Both,TRUE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,,,2025-12-23T09:26:23Z
Should AI have humanlike self-disclosure in the mental health intervention? The moderated mediation effect of negative emotion,,6-Jan,,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,TRUE,FALSE,Text,FALSE,FALSE,Not Applicable,Not Applicable,6A7,"Adolescence, Young Adulthood, Middle Adulthood",China,Both,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,,,
Retrieval-Augmented Generation in LLMs for Mental Health: Examining the Impact on User Intent Detection in Wysa,," December 30th, 2025",https://openalex.org/W7117539811,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,FALSE,TRUE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,,,
Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation Tools,,8/3/24,,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,FALSE,TRUE,FALSE,FALSE,Text,TRUE,TRUE,Not Applicable,Not Applicable,General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,TRUE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,,,8/3/24
Performance of mental health chatbot agents in detecting and managing suicidal ideation,,8/27/25,https://doi.org/10.1038/s41598-025-17242-4,WoS,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,TRUE,FALSE,FALSE,Text,FALSE,TRUE,Not Applicable,Interview (Questionnaire),General Psychiatry,"Adolescence, Young Adulthood",Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,,,8/27/25
Exploratory Research on the Potential of HumanAI Interaction for Mental Health: Building and Verifying an Experimental Environment Based on ChatGPT and Metaverse,,19-Oct-25,,WoS,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,"Text, Audio",TRUE,FALSE,Not Applicable,Interview (Questionnaire),"6B4, 6A7, 6B0","Adolescence, Young Adulthood",Japanese Speaker,Both,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,
Using Nonverbal Cues in Empathic Multi-Modal LLM-Driven Chatbots for Mental Health Support,,9-Sep-25,https://doi.org/10.1145/3743724,WoS,Foundation model,Chatbot,Multi-Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,"Text, Images",FALSE,FALSE,Not Applicable,Interview (Questionnaire),General Psychiatry,"Adolescence, Young Adulthood, Middle Adulthood","European Economic Area, Southern Africa, North America, Asia-Pacific Region, India, East Africa, South America",Both,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,,,2025
Tweaking the Messages and Approaching the Glass Box: Using AI Chatbots to Promote Help-Seeking for Depressive Symptoms,,10/18/25,https://doi.org/10.1080/10810730.2025.2569563,WoS,Machine Learning Models,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Interview (Questionnaire),6A7,"Adolescence, Young Adulthood, Middle Adulthood, Old",United States,Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,10/18/25
"Enhancing Self-Disclosure Intensions in Mental Health Support: The Catalytic Effect of Chatbot Bubble Coloring, Moderated by Expressive Suppression and Mediated by Information Processing Fluency and Perceived Initial Trust",,30-Oct-25,https://doi.org/10.1080/10447318.2025.2575298,WoS,Small pre-trained model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,General Psychiatry,"Adolescence, Young Adulthood",China,Both,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,2025
"Deploying a Mental Health Chatbot in Higher Education: The Development and Evaluation of Luna, an AI-Based Mental Health Support System",,6/10/25,https://doi.org/10.3390/computers14060227,WoS,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,General Psychiatry,"Adolescence, Young Adulthood",Unspecified,Unspecified,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,,,6/10/25
Therapeutic Potential of Social Chatbots in Alleviating Loneliness and Social Anxiety: Quasi-Experimental Mixed Methods Study,,1/14/25,https://doi.org/10.2196/65589,WoS,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,TRUE,FALSE,Not Applicable,Not Applicable,"6B0, 6B4","Adolescence, Young Adulthood",South Korea,Both,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,,,1/14/25
Leveraging Large Language Models for Simulated Psychotherapy Client Interactions: Development and Usability Study of Client101,,31-Jul-25,https://doi.org/10.2196/68056,WoS,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,FALSE,FALSE,FALSE,Text,FALSE,FALSE,Not Applicable,Systematic reviews  / Meta analysis,"6A7, 6B0",Unspecified,Unspecified,Unspecified,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,,,2025
LEKIA: Expert-Aligned AI Behavior Design for High-Risk Human-AI Interactions,"['Boning Zhao', 'Yutong Hu', 'Xinnuo Li']",7/20/25,https://arxiv.org/abs/2507.14944v2,arXiv,Foundation model,Chatbot,Multi-Agent,Hybrid Architecture,TRUE,TRUE,TRUE,FALSE,TRUE,Text,TRUE,,,Interview (Questionnaire),General Psychiatry,"Adolescence, Young Adulthood",Unspecified,Unspecified,FALSE,FALSE,TRUE,,,TRUE,TRUE,,TRUE,,TRUE,,,,7/20/25
The Anatomy of a Personal Health Agent,"['A. Ali Heydari', 'Ken Gu', 'Vidya Srinivas', 'Hong Yu', 'Zhihan Zhang', 'Yuwei Zhang', 'Akshay Paruchuri', 'Qian He', 'Hamid Palangi', 'Nova Hammerquist', 'Ahmed A. Metwally', 'Brent Winslow', 'Yubin Kim', 'Kumar Ayush', 'Yuzhe Yang', 'Girish Narayanswamy', 'Maxwell A. Xu', 'Jake Garrison', 'Amy Armento Lee', 'Jenny Vafeiadou', 'Ben Graef', 'Isaac R. Galatzer-Levy', 'Erik Schenck', 'Andrew Barakat', 'Javier Perez', 'Jacqueline Shreibati', 'John Hernandez', 'Anthony Z. Faranesh', 'Javier L. Prieto', 'Connor Heneghan', 'Yun Liu', 'Jiening Zhan', 'Mark Malhotra', 'Shwetak Patel', 'Tim Althoff', 'Xin Liu', 'Daniel McDuff', 'Xuhai ""Orson"" Xu']",2025-08-27T14:38:46Z,https://arxiv.org/abs/2508.20148v2,arXiv,Foundation model,Chatbot,Multi-Agent,Flat Architecture,TRUE,TRUE,TRUE,TRUE,TRUE,"Text, Images",TRUE,,EHRs,Interview (Questionnaire),"6B0, 6A7","Adolescence, Young Adulthood, Middle Adulthood, Old",Unspecified,Both,TRUE,TRUE,TRUE,TRUE,,TRUE,TRUE,,TRUE,TRUE,TRUE,TRUE,,,2025-08-27T14:38:46Z
Natural Language Tools: A Natural Language Approach to Tool Calling In Large Language Agents,"['Reid T. Johnson', 'Michelle D. Pain', 'Jordan D. West']",2025-10-16T08:52:52Z,https://arxiv.org/abs/2510.14453v1,arXiv,Foundation model,Non-Chat Agent,Non-Agent,Not Applicable,,,,TRUE,,Text,,TRUE,,,General Psychiatry,Unspecified,Unspecified,unspecified,,,,,,,,TRUE,TRUE,,,,,,2025-10-16T08:52:52Z
Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates,"['Liam G. McCoy', 'Fateme Nateghi Haredasht', 'Kanav Chopra', 'David Wu', 'David JH Wu', 'Abass Conteh', 'Sarita Khemani', 'Saloni Kumar Maharaj', 'Vishnu Ravi', 'Arth Pahwa', 'Yingjie Weng', 'Leah Rosengaus', 'Lena Giang', 'Kelvin Zhenghao Li', 'Olivia Jee', 'Daniel Shirvani', 'Ethan Goh', 'Jonathan H. Chen']",8/2/25,https://arxiv.org/abs/2508.01159v2,arXiv,Foundation model,Chatbot,Multi-Agent,Flat Architecture,TRUE,,TRUE,,TRUE,Text,,,EHRs,Interview (Questionnaire),General Psychiatry,Unspecified,Unspecified,Unspecified,,TRUE,,TRUE,,,TRUE,,TRUE,TRUE,TRUE,,,,8/2/25
Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis,"['Mithat Can Ozgun', 'Jiahuan Pei', 'Koen Hindriks', 'Lucia Donatelli', 'Qingzhi Liu', 'Junxiao Wang']",2025-08-15T11:08:32Z,https://arxiv.org/abs/2508.11398v2,arXiv,Foundation model,Chatbot,Multi-Agent,Hierarchical Architecture,TRUE,TRUE,TRUE,TRUE,,Text,TRUE,,,,General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,TRUE,,,,,TRUE,,TRUE,TRUE,,,,,2025-08-15T11:08:32Z
Cultural Prompting Improves the Empathy and Cultural Responsiveness of GPT-Generated Therapy Responses,"['Serena Jinchen Xie', 'Shumenghui Zhai', 'Yanjing Liang', 'Jingyi Li', 'Xuehong Fan', 'Trevor Cohen', 'Weichao Yuwen']",10/19/25,https://arxiv.org/abs/2512.00014v1,arXiv,Foundation model,Chatbot,Non-Agent,Not Applicable,,TRUE,,,TRUE,Text,TRUE,,,,General Psychiatry,"Adolescence, Young Adulthood, Middle Adulthood",United States,Both,,,TRUE,,,,TRUE,TRUE,TRUE,,,TRUE,,,10/19/25
VERA-MH Concept Paper,"['Luca Belli', 'Kate Bentley', 'Will Alexander', 'Emily Ward', 'Matt Hawrilenko', 'Kelly Johnston', 'Mill Brown', 'Adam Chekroud']",10/17/25,https://arxiv.org/abs/2510.15297v2,arXiv,Foundation model,Chatbot,Multi-Agent,Flat Architecture,,TRUE,TRUE,,TRUE,Text,,TRUE,,,"6A7, 6B4","Adolescence, Young Adulthood, Middle Adulthood, Old",Unspecified,Unspecified,TRUE,TRUE,TRUE,,,TRUE,TRUE,TRUE,TRUE,,TRUE,,,,10/17/25
AN INTELLIGENT SYSTEM TO GAMIFY MENTAL HEALTH EDUCATION AND DISSOCIATIVE IDENTITY DISORDER AWARENESS USING ARTIFICIAL INTELLIGENCE,,10/25/25,https://openalex.org/W4415694091,OpenAlex,Foundation model,Chatbot,Non-Agent,Not Applicable,,TRUE,,,TRUE,Text,TRUE,,,,6B6,Unspecified,Unspecified,Unspecified,,,,,,TRUE,,,,,TRUE,,,,10/25/25
Attitudes Toward the Use of Artificial Intelligence Chatbots for Mental Health Support: Artificial Intelligence in Mental Health Scale,,11/18/25,https://openalex.org/W4416398257,OpenAlex,unspecified_ai,Chatbot,Non-Agent,Not Applicable,,,,,TRUE,Text,,,,Interview (Questionnaire),General Psychiatry,"Adolescence, Young Adulthood, Middle Adulthood, Old",Greece,Both,,,,,,,TRUE,,,,TRUE,TRUE,,,11/18/25
Practitioner Perspectives on the Uses of Generative AI Chatbots in Mental Health Care: Mixed Methods Study,,8/10/25,https://openalex.org/W4413232168,OpenAlex,Foundation model,Chatbot,Non-Agent,Not Applicable,TRUE,TRUE,,,TRUE,Text,,TRUE,,Interview (Questionnaire),General Psychiatry,"Adolescence, Young Adulthood, Middle Adulthood, Old",Australia,Both,TRUE,TRUE,TRUE,TRUE,,TRUE,,,,,TRUE,,,,8/10/25
REIMAGINING WORKFORCE WELL-BEING: AI-DRIVEN MENTAL HEALTH INTERVENTIONS IN HYBRID WORK ENVIRONMENTS,,8/25/25,https://openalex.org/W4414290849,OpenAlex,Foundation model,Chatbot,Non-Agent,Not Applicable,,,,,TRUE,Text,,,,Interview (Questionnaire),General Psychiatry,"Young Adulthood, Middle Adulthood",India,Both,,,,,,,,TRUE,,,TRUE,TRUE,,,8/25/25
An Exploratory Study of Chatbot-based Mindfulness for Mental Health Support,,27-Oct-25,https://link.springer.com/article/10.1007/s12671-025-02685-7,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,,TRUE,,Interview (Questionnaire),General Psychiatry,"Adolescence, Young Adulthood",Canada,Both,,,TRUE,,,TRUE,,TRUE,TRUE,,,TRUE,,,
Self-help psychological intervention for young individuals during the post-COVID-19 era: development of a PST chatbot using GPT-4,,9/23/25,https://openalex.org/W4414443465,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,TRUE,TRUE,,,"6A7, 6B0, 6B4","Adolescence, Young Adulthood",China,Both,TRUE,,TRUE,,,TRUE,,,,,TRUE,TRUE,,,9/23/25
Development and Validation of the Artificial Intelligence in Mental Health Scale: Application for AI Mental Health Chatbots,,12/12/25,https://openalex.org/W4417278215,OpenAlex,unspecified_ai,Chatbot,Non-Agent,Not Applicable,,,,,TRUE,Text,,,,Interview (Questionnaire),General Psychiatry,"Adolescence, Young Adulthood, Middle Adulthood, Old",Greece,Both,TRUE,,TRUE,,,,TRUE,TRUE,,,TRUE,TRUE,,,12/12/25
"Feasibility of an AI-Enabled Smart Mirror Integrating MA-rPPG, Facial Affect, and Conversational Guidance in Realtime",,9/18/25,https://openalex.org/W4414308846,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,FALSE,TRUE,,TRUE,,"Text, Audio, Images",TRUE,TRUE,,,,Unspecified,"Germany, Uzbekistan, Pakistan, South Korea, Malaysia, Philippines",Both,TRUE,,TRUE,,,TRUE,,,TRUE,,,TRUE,,,9/18/25
Cognicare : An AI-Powered Conversational Agent for Mental Health Monitoring and Support,,11/10/25,https://openalex.org/W4416222713,OpenAlex,Small pre-trained model,Chatbot,Multi-Agent,Hierarchical Architecture,TRUE,TRUE,TRUE,TRUE,,"Text, Audio",TRUE,TRUE,,Interview (Questionnaire),General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,TRUE,TRUE,TRUE,,TRUE,,,TRUE,TRUE,,,,,11/10/25
AI Powered Digital Mental Health Platform for Students - Psyconnect,,11/4/25,https://openalex.org/W4415877292,OpenAlex,Deep Learning Models,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,,,TRUE,Text,TRUE,,,,General Psychiatry,Adolescence,Unspecified,Unspecified,TRUE,,TRUE,TRUE,,,,,,,,,,,11/4/25
A Neuro-Symbolic Multi-Agent Architecture for Digital Transformation of Psychological Support Systems via Artificial Neurotransmitters and Archetypal Reasoning,,11/15/25,https://openalex.org/W7106040341,OpenAlex,Machine Learning Models,Chatbot,Multi-Agent,Hierarchical Architecture,TRUE,TRUE,TRUE,TRUE,TRUE,"Text, Audio, Images",TRUE,,,Interview (Questionnaire),"6A7, 6B4","Adolescence, Young Adulthood",Italy,Both,TRUE,,TRUE,,,,TRUE,,TRUE,,TRUE,TRUE,,,11/15/25
Effectiveness of e-counseling platforms enhanced by AI chatbots on academic self-efficacy and fear of negative evaluation among undergraduate students,,8/26/25,https://openalex.org/W4413737488,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,TRUE,,,Interview (Questionnaire),6B0,Adolescence,"Egypt, Kuwait",Both,TRUE,,TRUE,,,,,,TRUE,,,TRUE,,,8/26/25
On the Security and Privacy of AI-based Mobile Health Chatbots,,11/15/25,https://openalex.org/W4416355036,OpenAlex,Foundation model,Chatbot,Non-Agent,Not Applicable,,TRUE,,,TRUE,Text,,,,Not Applicable,General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,,TRUE,,,,,TRUE,TRUE,,TRUE,,,,11/15/25
Multimodal Emotion Recognition and Human Computer Interaction for AI-Driven Mental Health Support,,25-Sep,,OpenAlex,"Deep Learning Models, Foundation Model",Chatbot,Non-Agent,,,TRUE,,,TRUE,"Text, Audio, Images, Video",TRUE,TRUE,Lab Test,,General Psychiatry,Unspecified,Unspecified,Unspecified,TRUE,,TRUE,,,TRUE,,TRUE,TRUE,TRUE,,TRUE,,,
Talking to an AI Mirror: Designing Self-Clone Chatbots for Enhanced Engagement in Digital Mental Health Support,,9/8/25,https://openalex.org/W4415055876,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,TRUE,,,,General Psychiatry,"Adolescence, Young Adulthood, Middle Adulthood, Old",Unspecified,Both,,,TRUE,,,TRUE,,,TRUE,TRUE,,TRUE,,,9/8/25
Interactional Fairness in LLM Multi-Agent Systems: An Evaluation Framework,,10/15/25,https://openalex.org/W4415230938,OpenAlex,Foundation model,Chatbot,Multi-Agent,Flat Architecture,,,,,TRUE,Text,,TRUE,Lab Test,,General Psychiatry,Unspecified,Unspecified,Unspecified,,,,,,,TRUE,,TRUE,,,,,,10/15/25
The Effectiveness of AI-Driven Chatbots in Providing Emotional Support in Mental Health Interventions,,25-Sep,,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,TRUE,TRUE,Lab Test,,"6A7, 6B0","Adolescence, Young Adulthood, Middle Adulthood",Unspecified,Unspecified,,,TRUE,,,TRUE,,,TRUE,,,TRUE,,,
Application of Artificial Intelligence in Chatbot and Social Media Community for a Mental Health Monitoring Web-Application,,10/2/25,https://openalex.org/W4414792765,OpenAlex,Deep Learning Models,Chatbot,Single Agent,Not Applicable,,TRUE,,,,Text,TRUE,,,Interview (Questionnaire),General Psychiatry,"Adolescence, Young Adulthood, Middle Adulthood",South Africa,Unspecified,TRUE,,TRUE,,,TRUE,,,,,,TRUE,,,10/2/25
Attitudes towards AI counseling: the existence of perceptual fear in affecting perceived chatbot support quality,,8/1/25,https://openalex.org/W4412838943,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,TRUE,TRUE,,Interview (Questionnaire),General Psychiatry,"Adolescence, Young Adulthood, Middle Adulthood, Old",Hongkong Sar,Both,,,TRUE,,,,,TRUE,TRUE,,,TRUE,,,8/1/25
An Ensemble Learning Approach for AI-Powered Chatbot for Mental Health Assessment and Support,,10/25/25,https://openalex.org/W7105685035,OpenAlex,Machine Learning Models,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,TRUE,,,,"6A7, 6A6, 6A0, 6B0",Unspecified,Unspecified,Unspecified,TRUE,TRUE,,,,TRUE,,TRUE,TRUE,,,TRUE,,,10/25/25
Was Epictetus Right? Do Feelings Really Result from Thoughts?,,17-Dec-25,,OpenAlex,Foundation model,Chatbot,Single Agent,Not Applicable,,TRUE,,,TRUE,Text,TRUE,,,,"6A7, 6B0",Unspecified,Unspecified,Unspecified,TRUE,,TRUE,,,TRUE,,,TRUE,,,TRUE,,,
Development and Evaluation of a Mental Health Chatbot Using ChatGPT 4.0: Mixed Methods User Experience Study With Korean Users,,1/3/25,,WoS,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,,,TRUE,,Text,TRUE,TRUE,,,"6A7, 6B0","Adolescence, Young Adulthood",Republic Of Korea,Both,TRUE,,TRUE,,,TRUE,,,TRUE,,,TRUE,,,1/1/25
Human guided empathetic AI agent for mental health support leveraging reinforcement learning-enhanced retrieval-augmented generation,,2/1/25,,WoS,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,,,TRUE,Text,TRUE,,,,General Psychiatry,Unspecified,United States,Unspecified,,,TRUE,,,,,TRUE,TRUE,TRUE,TRUE,TRUE,,,2/1/25
"A conversational agent framework for mental health screening: design, implementation, and usability",,3/27/24,,WoS,Machine Learning Models,Chatbot,Single Agent,Not Applicable,,TRUE,TRUE,,,Text,TRUE,,,Interview (Questionnaire),General Psychiatry,"Adolescence, Young Adulthood",Romania,Both,TRUE,,TRUE,,,TRUE,,,TRUE,TRUE,TRUE,TRUE,,,3/27/24
Enhancing parental skills through artificial intelligence-based conversational agents: The PAT Initiative,,7/1/25,,WoS,Foundation model,Chatbot,Multi-Agent,Hybrid Architecture,TRUE,TRUE,,TRUE,,"Text, Audio",TRUE,TRUE,,Interview (Questionnaire),"6A02, 6C91",Childhood,Argentina,Unspecified,,,TRUE,,,TRUE,,,,,,TRUE,,,7/1/25
Leveraging Large Language Models for Simulated Psychotherapy Client Interactions: Development and Usability Study of Client101,,31-Jul-25,https://doi.org/10.2196/68056,WoS,Foundation model,Chatbot,Multi-Agent,Flat Architecture,TRUE,,,,TRUE,Text,,TRUE,,,"6A7, 6B0",Unspecified,Unspecified,Unspecified,,,,,,TRUE,,,TRUE,,TRUE,,,,2025
The development of a World Health Organization transdiagnostic chatbot intervention for distressed adolescents and young adults,,2/21/25,,WoS,Rule-based models,Chatbot,Non-Agent,Not Applicable,,TRUE,TRUE,,TRUE,"Text, Audio, Video",TRUE,,,Interview (Questionnaire),General Psychiatry,"Adolescence, Young Adulthood",Jordan,Both,,,TRUE,,,TRUE,,TRUE,,,TRUE,TRUE,,,2/21/25
Human guided empathetic AI agent for mental health support leveraging reinforcement learning-enhanced retrieval-augmented generation,,2/1/25,,WoS,Foundation model,Chatbot,Single Agent,Not Applicable,TRUE,TRUE,,,TRUE,Text,TRUE,,,,General Psychiatry,Unspecified,United States,Unspecified,,,TRUE,,,,,TRUE,TRUE,TRUE,TRUE,TRUE,,,2/1/25
Enhancing parental skills through artificial intelligence-based conversational agents: The PAT Initiative,,7/1/25,,WoS,Foundation model,Chatbot,Multi-Agent,Hybrid Architecture,TRUE,TRUE,,TRUE,,"Text, Audio",TRUE,TRUE,,Interview (Questionnaire),"6A02, 6C91",Childhood,Argentina,Unspecified,,,TRUE,,,TRUE,,,,,,TRUE,,,7/1/25
Leveraging Large Language Models for Simulated Psychotherapy Client Interactions: Development and Usability Study of Client101,,31-Jul-25,https://doi.org/10.2196/68056,WoS,Foundation model,Chatbot,Multi-Agent,Flat Architecture,TRUE,,,,TRUE,Text,,TRUE,,,"6A7, 6B0",Unspecified,Unspecified,Unspecified,,,,,,TRUE,,,TRUE,,TRUE,,,,2025
The development of a World Health Organization transdiagnostic chatbot intervention for distressed adolescents and young adults,,2/21/25,,WoS,Rule-based models,Chatbot,Non-Agent,Not Applicable,,TRUE,TRUE,,TRUE,"Text, Audio, Video",TRUE,,,Interview (Questionnaire),General Psychiatry,"Adolescence, Young Adulthood",Jordan,Both,,,TRUE,,,TRUE,,TRUE,,,TRUE,TRUE,,,2/21/25